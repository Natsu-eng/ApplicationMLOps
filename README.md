# DataLab Pro üß™

**DataLab Pro** est une plateforme d'analyse de donn√©es et de Machine Learning automatis√© construite avec Streamlit. Elle permet de charger, d'explorer, de pr√©traiter des donn√©es, ainsi que d'entra√Æner et d'√©valuer des mod√®les de classification, de r√©gression et de clustering.

> **Version Production-Ready** - Consolidation compl√®te du syst√®me de logging et des d√©corateurs pour une architecture robuste et maintenable.

## Architecture

Le projet suit une architecture modulaire pour une s√©paration claire des responsabilit√©s :

```
app-analyse/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ app/          # Interface utilisateur Streamlit
‚îÇ   ‚îú‚îÄ‚îÄ config/       # Configuration de l'application (Pydantic Settings)
‚îÇ   ‚îú‚îÄ‚îÄ data/         # Chargement et pr√©traitement des donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ models/       # Logique d'entra√Ænement et catalogue de mod√®les
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/   # Calcul des m√©triques et visualisations
‚îÇ   ‚îú‚îÄ‚îÄ monitoring/   # D√©tection de d√©rive et surveillance
‚îÇ   ‚îî‚îÄ‚îÄ shared/       # Modules partag√©s (√©tat, logging centralis√©)
‚îú‚îÄ‚îÄ orchestrators/    # Orchestrateurs m√©tier (ML et Computer Vision)
‚îú‚îÄ‚îÄ helpers/          # Helpers r√©utilisables
‚îú‚îÄ‚îÄ utils/            # Utilitaires g√©n√©riques
‚îú‚îÄ‚îÄ monitoring/       # Monitoring et d√©corateurs (point d'entr√©e unique)
‚îú‚îÄ‚îÄ .env              # Fichier pour les variables d'environnement
‚îú‚îÄ‚îÄ requirements.txt  # D√©pendances Python
‚îú‚îÄ‚îÄ Dockerfile        # Fichier de build Docker
‚îî‚îÄ‚îÄ docker-compose.yml # Orchestration des services
```

### üîß Bonnes Pratiques de D√©veloppement

#### Logging Centralis√©

Le syst√®me de logging est **centralis√©** dans `src/shared/logging.py`. Tous les modules doivent utiliser `get_logger()` :

```python
from src.shared.logging import get_logger

logger = get_logger(__name__)
logger.info("Message d'information")
```

**‚ö†Ô∏è Ne pas** configurer `logging.basicConfig()` ou cr√©er des handlers manuellement. Le syst√®me de logging est initialis√© automatiquement via `setup_logging()` appel√© dans `main.py`.

#### D√©corateurs Standardis√©s

Tous les d√©corateurs de gestion d'erreurs et de monitoring sont **centralis√©s** dans `monitoring/decorators.py` :

```python
from monitoring.decorators import safe_execute, monitor_performance, handle_mlflow_errors

@safe_execute(fallback_value=None, max_retries=2)
def ma_fonction():
    # Code avec gestion d'erreurs automatique
    pass

@monitor_performance
def operation_longue():
    # Monitoring automatique des performances
    pass

@handle_mlflow_errors
def log_mlflow():
    # Gestion gracieuse des erreurs MLflow
    pass
```

**‚ö†Ô∏è Ne pas** cr√©er de nouveaux d√©corateurs de gestion d'erreurs. Utiliser ceux de `monitoring/decorators.py`.

#### Structure des Imports

Pour maintenir la coh√©rence, suivre cette structure d'imports :

```python
# 1. Imports standards
import os
import pandas as pd

# 2. Imports de logging (TOUJOURS utiliser get_logger)
from src.shared.logging import get_logger

# 3. Imports de configuration
from src.config.constants import ...
from src.config.settings import ...

# 4. Imports de d√©corateurs (monitoring/decorators.py)
from monitoring.decorators import safe_execute, monitor_performance

# 5. Imports internes (src/)
from src.models.training import ...
from src.data.data_loader import ...

# 6. Imports helpers/utils
from helpers.data_validators import ...
from utils.system_utils import ...
```

## üöÄ D√©marrage Rapide

### 1. Pr√©requis

- Python 3.11+
- Docker & Docker Compose
- Un client PostgreSQL (optionnel, pour MLflow)

### 2. Installation Locale

1.  **Clonez le projet :**
    ```bash
    git clone <repository_url>
    cd app-analyse
    ```

2.  **Cr√©ez un environnement virtuel et installez les d√©pendances :**
    ```bash
    python -m venv env
    source env/bin/activate  # sur Windows: env\Scripts\activate
    pip install -r requirements.txt
    ```

3.  **Configurez l'environnement :**
    - Cr√©ez un fichier `.env` √† la racine du projet.
    - Ajoutez vos configurations, notamment pour MLflow si vous l'utilisez :
      ```env
      MLFLOW_TRACKING_URI=postgresql+psycopg2://user:password@host:port/dbname
      ```

4.  **Lancez l'application :**
    ```bash
    streamlit run src/app/main.py
    ```

### 3. D√©marrage avec Docker

Cette m√©thode est recommand√©e pour un environnement de production reproductible.

1.  **Assurez-vous que votre fichier `.env` est configur√©.** Le `docker-compose.yml` l'utilisera.

2.  **Lancez les services :**
    - Pour lancer l'application, la base de donn√©es et MLflow :
      ```bash
      docker-compose up --build
      ```
    - L'application sera disponible sur `http://localhost:8501`.
    - L'interface MLflow sera sur `http://localhost:5000`.

## Utilisation de l'Application

1.  **Accueil** : Chargez votre jeu de donn√©es (CSV, Parquet, Excel).
2.  **Dashboard** : Explorez les donn√©es via les onglets (qualit√©, analyse univari√©e, corr√©lations, etc.).
3.  **Entra√Ænement** : Configurez votre exp√©rimentation ML (cible, features, mod√®les) et lancez l'entra√Ænement.
4.  **√âvaluation** : Comparez les mod√®les, analysez les m√©triques et visualisez les r√©sultats d√©taill√©s.

## üìö Documentation Technique

### Syst√®me de Logging

Le syst√®me de logging est **centralis√© et idempotent**. Il est configur√© automatiquement au d√©marrage de l'application via `setup_logging()` dans `main.py`.

**Configuration** :
- Fichiers de logs avec rotation automatique
- Support MLflow int√©gr√©
- Niveaux configurables via variables d'environnement (`LOG_LEVEL`)
- Format standardis√© pour tous les logs

**Utilisation** :
```python
from src.shared.logging import get_logger

logger = get_logger(__name__)
logger.info("Message")
logger.error("Erreur", exc_info=True)
```

### D√©corateurs de Production

Tous les d√©corateurs sont dans `monitoring/decorators.py` :

- **`@safe_execute`** : Ex√©cution s√©curis√©e avec fallback et retry
- **`@monitor_performance`** : Monitoring automatique des performances
- **`@monitor_operation`** : Monitoring avec logs structur√©s
- **`@handle_mlflow_errors`** : Gestion gracieuse des erreurs MLflow
- **`@safe_metric_calculation`** : Calculs de m√©triques avec retry
- **`@timeout`** : Timeout automatique sur op√©rations longues

### Configuration

La configuration utilise **Pydantic Settings** pour validation et chargement depuis `.env` :

```python
from src.config.settings import app_settings, training_settings, mlflow_settings

# Utilisation
max_size = app_settings.MAX_FILE_SIZE_MB
threshold = training_settings.HIGH_MEMORY_THRESHOLD
mlflow_uri = mlflow_settings.MLFLOW_TRACKING_URI
```

### State Management

Le state management est **thread-safe** et utilise un pattern singleton :

```python
from monitoring.state_managers import STATE

# Acc√®s aux donn√©es
if STATE.loaded:
    df = STATE.data.df
    # ...
```

## üöÄ D√©ploiement Production

### Pr√©requis

- Python 3.11+
- PostgreSQL (pour MLflow tracking)
- Variables d'environnement configur√©es (voir `.env.example`)

### Checklist Production

- [ ] Variables d'environnement configur√©es (`.env`)
- [ ] MLflow tracking URI configur√© (PostgreSQL recommand√©)
- [ ] Logs configur√©s avec rotation
- [ ] Niveau de logging adapt√© (INFO ou WARNING en production)
- [ ] Monitoring syst√®me activ√©
- [ ] Health checks configur√©s

### Variables d'Environnement Critiques

```env
# Logging
LOG_LEVEL=INFO  # ou WARNING en production

# MLflow
MLFLOW_TRACKING_URI=postgresql+psycopg2://user:password@host:port/dbname
MLFLOW_EXPERIMENT_NAME=production_experiments

# Performance
MAX_MEMORY_USAGE=85
MEMORY_CHECK_INTERVAL=180
```

## üîç Troubleshooting

### Probl√®mes de Logging

Si les logs ne s'affichent pas, v√©rifier :
1. Que `setup_logging()` est appel√© dans `main.py`
2. Le niveau de log dans `.env` (`LOG_LEVEL`)
3. Les permissions d'√©criture dans le dossier `logs/`

### Probl√®mes MLflow

Si MLflow ne fonctionne pas :
1. V√©rifier la connexion √† PostgreSQL
2. V√©rifier les variables `MLFLOW_TRACKING_URI` et `MLFLOW_EXPERIMENT_NAME`
3. L'application continue sans MLflow si non disponible (graceful degradation)

## üìù Contribution

Lors de l'ajout de code :

1. **Utiliser `get_logger()`** de `src/shared/logging.py`
2. **Utiliser les d√©corateurs** de `monitoring/decorators.py`
3. **Suivre la structure d'imports** document√©e ci-dessus
4. **Documenter** les fonctions publiques avec docstrings
5. **Tester** les modifications avec les tests existants

## üìÑ Licence

[Votre licence ici]
