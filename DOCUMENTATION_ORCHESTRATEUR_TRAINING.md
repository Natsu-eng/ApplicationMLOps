# ğŸ“š Documentation ComplÃ¨te - Orchestrateur, Training Bas Niveau et PrÃ©dictions

## ğŸ“‹ Table des MatiÃ¨res

1. [Architecture Globale](#architecture-globale)
2. [Orchestrateur (Niveau Haut)](#orchestrateur-niveau-haut)
3. [Trainer Bas Niveau](#trainer-bas-niveau)
4. [Pipeline de PrÃ©diction](#pipeline-de-prÃ©diction)
5. [Pipeline d'Ã‰valuation](#pipeline-dÃ©valuation)
6. [Flux Complet de Bout en Bout](#flux-complet-de-bout-en-bout)

---

## ğŸ—ï¸ Architecture Globale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         STREAMLIT UI (Pages)                            â”‚
â”‚  â€¢ 4_training_computer.py  â†’  Lancement training                       â”‚
â”‚  â€¢ 5_anomaly_evaluation.py â†’  Visualisation rÃ©sultats                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STATE MANAGER (Global State)                         â”‚
â”‚  â€¢ STATE.data           â†’ DonnÃ©es chargÃ©es                             â”‚
â”‚  â€¢ STATE.training_results â†’ RÃ©sultats training                         â”‚
â”‚  â€¢ STATE.model_config   â†’ Configuration modÃ¨le                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ORCHESTRATEUR (visio_training_orchestrator.py)             â”‚
â”‚  â€¢ ComputerVisionTrainingOrchestrator                                   â”‚
â”‚  â€¢ Coordination complÃ¨te du workflow                                    â”‚
â”‚  â€¢ IntÃ©gration MLflow                                                   â”‚
â”‚  â€¢ Gestion preprocessing                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              TRAINER BAS NIVEAU (computer_vision_training.py)           â”‚
â”‚  â€¢ ComputerVisionTrainer (SupervisÃ©)                                    â”‚
â”‚  â€¢ AnomalyAwareTrainer (Anomalies)                                      â”‚
â”‚  â€¢ MÃ©thodes: fit(), predict(), evaluate()                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MODÃˆLES & UTILITAIRES                                â”‚
â”‚  â€¢ ModelBuilder          â†’ Construction modÃ¨les                        â”‚
â”‚  â€¢ DataPreprocessor      â†’ Preprocessing                               â”‚
â”‚  â€¢ DataLoaderFactory     â†’ CrÃ©ation DataLoaders                        â”‚
â”‚  â€¢ OptimizerFactory      â†’ CrÃ©ation optimizers                         â”‚
â”‚  â€¢ SchedulerFactory      â†’ CrÃ©ation schedulers                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Orchestrateur (Niveau Haut)

### **Fichier**: `orchestrators/visio_training_orchestrator.py`

### **Classe Principale**: `ComputerVisionTrainingOrchestrator`

L'orchestrateur est le **coordinateur central** qui orchestre tout le workflow d'entraÃ®nement. Il ne fait **PAS** l'entraÃ®nement lui-mÃªme, mais :
- âœ… Valide les configurations
- âœ… DÃ©marre le run MLflow
- âœ… CrÃ©e le trainer appropriÃ©
- âœ… Lance l'entraÃ®nement
- âœ… Log les mÃ©triques et artifacts dans MLflow
- âœ… Retourne un `TrainingResult` standardisÃ©

---

### **MÃ©thode Principale**: `train(context: TrainingContext) -> TrainingResult`

#### **Flux d'ExÃ©cution**:

```python
def train(self, context: TrainingContext) -> TrainingResult:
    """
    Pipeline complet orchestrÃ©:
    
    1. VALIDATION DES CONFIGS
    2. VALIDATION DES DONNÃ‰ES
    3. DÃ‰MARRAGE MLFLOW RUN
    4. LOG CONFIGURATION
    5. CRÃ‰ATION TRAINER
    6. EXÃ‰CUTION ENTRAÃNEMENT
    7. RÃ‰CUPÃ‰RATION PREPROCESSOR
    8. LOG MÃ‰TRIQUES & ARTIFACTS
    9. FINALISATION
    """
```

#### **Ã‰tape 1: Conversion et Validation des Configs**

```python
# Ligne 83
context.training_config = self._ensure_training_config_object(context.training_config)
```

**FonctionnalitÃ©s**:
- âœ… Convertit `dict` â†’ `TrainingConfig` object
- âœ… Valide les Enums (`OptimizerType`, `SchedulerType`)
- âœ… Validation stricte des valeurs (epochs > 0, lr dans ]0,1[, etc.)
- âœ… Fallback sÃ©curisÃ© sur valeurs par dÃ©faut

**Exemple**:
```python
# Input: dict
training_config = {
    "epochs": 100,
    "batch_size": 32,
    "optimizer": "adamw",  # String â†’ Converti en OptimizerType.ADAMW
    "scheduler": "reduce_on_plateau"  # String â†’ Converti en SchedulerType.REDUCE_ON_PLATEAU
}

# Output: TrainingConfig object avec Enums validÃ©s
```

---

#### **Ã‰tape 2: Validation des DonnÃ©es**

```python
# Ligne 88
self._validate_training_context(context)
```

**VÃ©rifications**:
- âœ… `X_train` et `X_val` non None
- âœ… Datasets non vides
- âœ… Logging des shapes pour observabilitÃ©

---

#### **Ã‰tape 3: DÃ©marrage MLflow Run**

```python
# Ligne 93
run_id = self._start_mlflow_run(context)
```

**Actions**:
- âœ… CrÃ©ation run MLflow avec tags enrichis
- âœ… Tags: `anomaly_type`, `dataset_size`, `train_val_split`, `model_type`
- âœ… Retourne `run_id` pour tracking

---

#### **Ã‰tape 4: Log Configuration dans MLflow**

```python
# Ligne 98
self._log_configuration_to_mlflow(context)
```

**Logs**:
- âœ… `model_config`: type, params du modÃ¨le
- âœ… `training_config`: epochs, batch_size, lr, optimizer, scheduler
- âœ… `preprocessing_config`: stratÃ©gie, augmentation

---

#### **Ã‰tape 5: CrÃ©ation du Trainer**

```python
# Ligne 103
trainer = self._create_trainer(context)
```

**Logique**:
```python
if context.anomaly_type:
    # CrÃ©ation AnomalyAwareTrainer (wrapper autour de ComputerVisionTrainer)
    return AnomalyAwareTrainer(
        anomaly_type=context.anomaly_type,
        model_config=model_config,
        training_config=context.training_config,
        callbacks=callbacks
    )
else:
    # CrÃ©ation ComputerVisionTrainer standard
    return ComputerVisionTrainer(
        model_config=model_config,
        training_config=context.training_config,
        callbacks=callbacks
    )
```

---

#### **Ã‰tape 6: ExÃ©cution de l'EntraÃ®nement**

```python
# Ligne 108
result = self._execute_training(trainer, context)
```

**FonctionnalitÃ©s ClÃ©s** (`_execute_training`):

1. **Appel du Trainer**:
   ```python
   if context.anomaly_type:
       raw_result = trainer.train(X_train, y_train, X_val, y_val)  # AnomalyAwareTrainer
   else:
       raw_result = trainer.fit(X_train, y_train, X_val, y_val)  # ComputerVisionTrainer
   ```

2. **Normalisation Robuste du RÃ©sultat**:
   - âœ… GÃ¨re 4 formats possibles:
     - `dict` direct
     - Objet `Result` avec `.data`
     - Objet avec `.success` et `.history`
     - Format inconnu (erreur)
   - âœ… **Garantit** un dict normalisÃ© avec clÃ©s: `success`, `data`, `error`, `metadata`
   - âœ… VÃ©rifie que `data.history` existe (fallback si manquant)

3. **Validation Finale**:
   ```python
   if result['success'] and not result.get('data', {}).get('history'):
       result['success'] = False
       result['error'] = "Historique d'entraÃ®nement manquant"
   ```

---

#### **Ã‰tape 7: RÃ©cupÃ©ration Preprocessor**

```python
# Ligne 122
preprocessor = self._get_preprocessor(trainer, context)
```

**Actions**:
- âœ… RÃ©cupÃ¨re `trainer.preprocessor`
- âœ… Si None â†’ CrÃ©ation fallback avec config par dÃ©faut
- âœ… Fit le fallback sur `X_train`

---

#### **Ã‰tape 8: Log MÃ©triques et Artifacts**

```python
# Lignes 128-129
self._log_training_metrics(history)
self._log_training_artifacts(trainer.model, preprocessor, context, run_id)
```

**Log MÃ©triques**:
- âœ… MÃ©triques par epoch: `train_loss`, `val_loss`, `val_accuracy`, `val_f1`, `learning_rate`
- âœ… MÃ©triques finales: `best_val_loss`, `training_time`, `total_epochs`, `best_epoch`
- âœ… Log courbes d'entraÃ®nement

**Log Artifacts**:
- âœ… ModÃ¨le PyTorch (`.pt` file)
- âœ… Preprocessor (pickle ou config JSON si non-picklable)
- âœ… `model_config.json` (sÃ©rialisation safe)
- âœ… Gestion robuste des erreurs (try/catch sur chaque artifact)

---

#### **Ã‰tape 9: Finalisation**

```python
# Ligne 134
cv_mlflow_tracker.end_run("FINISHED")

# Ligne 137
final_history = self._build_final_history(history, context, run_id, preprocessor)
```

**Construction Historique Final** (`_build_final_history`):
- âœ… Extraction safe des mÃ©triques (gestion None, NaN, Inf)
- âœ… Conversion types (float, int, list)
- âœ… Ajout mÃ©tadonnÃ©es: `mlflow_run_id`, `preprocessor_config`, `anomaly_type`
- âœ… Fallback historique minimal si erreur

**Retour `TrainingResult`**:
```python
return TrainingResult(
    success=True,
    model=trainer.model,  # ModÃ¨le PyTorch entraÃ®nÃ©
    history=final_history,  # Dict normalisÃ© avec mÃ©triques
    preprocessor=preprocessor,  # Preprocessor fittÃ©
    mlflow_run_id=run_id,  # ID du run MLflow
    metadata={
        "model_type": context.model_config["model_type"],
        "total_epochs": history.get('total_epochs_trained', 0),
        "best_epoch": history.get('best_epoch', 0)
    }
)
```

---

### **Gestion des Erreurs**

```python
except Exception as e:
    # Log erreur
    logger.error(f"Erreur orchestration entraÃ®nement: {e}", exc_info=True)
    
    # End MLflow run en FAILED
    if run_id:
        cv_mlflow_tracker.log_metrics({"training_failed": 1.0})
        cv_mlflow_tracker.end_run("FAILED")
    
    # Retour TrainingResult avec success=False
    return TrainingResult(
        success=False,
        error=str(e),
        mlflow_run_id=run_id,
        metadata={"error_type": type(e).__name__, ...}
    )
```

---

## ğŸ”§ Trainer Bas Niveau

### **Fichier**: `src/models/computer_vision_training.py`

### **Classe Principale**: `ComputerVisionTrainer`

Le trainer est responsable de l'**entraÃ®nement rÃ©el** du modÃ¨le. Il gÃ¨re:
- âœ… Preprocessing (fit sur train, transform sur val)
- âœ… Construction du modÃ¨le
- âœ… Setup optimizer/scheduler/criterion
- âœ… Boucle d'entraÃ®nement avec early stopping
- âœ… PrÃ©dictions et Ã©valuations

---

### **MÃ©thode Principale**: `fit(X_train, y_train, X_val, y_val) -> Result`

#### **Pipeline Complet**:

```python
def fit(self, X_train, y_train, X_val, y_val) -> Result:
    """
    Pipeline d'entraÃ®nement:
    
    1. VALIDATION DES DONNÃ‰ES
    2. SETUP PREPROCESSING (fit sur train, transform sur val)
    3. CONSTRUCTION MODÃˆLE
    4. SETUP TRAINING (optimizer, scheduler, criterion)
    5. CRÃ‰ATION DATALOADERS
    6. BOUCLE D'ENTRAÃNEMENT
    7. RETOUR RÃ‰SULTATS STRUCTURÃ‰S
    """
```

---

#### **Ã‰tape 1: Validation des DonnÃ©es**

```python
# Ligne 368
val_result = self._validate_data(X_train, y_train, X_val, y_val)
```

**VÃ©rifications** (`_validate_data`):
- âœ… Validation `X_train`, `y_train` (shapes, types, non-vides)
- âœ… Validation `X_val`, `y_val`
- âœ… CohÃ©rence shapes entre train et val
- âœ… Analyse dÃ©sÃ©quilibre classes (warning si `ratio < 0.1`)

**Retour**: `Result.err()` si validation Ã©choue, sinon `Result.ok(None, imbalance=...)`

---

#### **Ã‰tape 2: Setup Preprocessing**

```python
# Ligne 373
prep_result = self._setup_preprocessing(X_train, y_train, X_val, y_val)
```

**Actions Critiques** (`_setup_preprocessing`):

1. **CrÃ©ation Preprocessor**:
   ```python
   self.preprocessor = DataPreprocessor(
       strategy="standardize",
       auto_detect_format=True  # DÃ©tection automatique channels_first/last
   )
   ```

2. **FIT sur TRAIN UNIQUEMENT** (âš ï¸ Pas de fuite de donnÃ©es):
   ```python
   X_train_norm = self.preprocessor.fit_transform(
       X_train,
       output_format="channels_first"  # Format PyTorch
   )
   ```

3. **TRANSFORM sur VALIDATION** (mÃªme format):
   ```python
   X_val_norm = self.preprocessor.transform(
       X_val,
       output_format="channels_first"
   )
   ```

4. **Validations Post-Processing**:
   - âœ… VÃ©rification non-None
   - âœ… Format 4D (N, C, H, W)
   - âœ… Canaux valides (1 ou 3)
   - âœ… VÃ©rification NaN/Inf

**Retour**: `Result.ok((X_train_norm, y_train, X_val_norm, y_val))` ou `Result.err(...)`

---

#### **Ã‰tape 3: Construction du ModÃ¨le**

```python
# Ligne 380
model_result = self._build_model()
```

**Actions** (`_build_model`):
- âœ… Utilise `ModelBuilder` pour construire le modÃ¨le selon `model_config`
- âœ… Log nombre de paramÃ¨tres (total et trainable)
- âœ… Stocke dans `self.model`

**Types de ModÃ¨les SupportÃ©s**:
- `CNN` (Classification)
- `RESNET_TRANSFER`, `EFFICIENTNET_TRANSFER` (Transfer Learning)
- `CONV_AUTOENCODER`, `VAE`, `DENOISING_AE` (Anomaly Detection)

---

#### **Ã‰tape 4: Setup Training**

```python
# Ligne 385
setup_result = self._setup_training(y_train)
```

**Actions** (`_setup_training`):

1. **Optimizer**:
   ```python
   self.optimizer = OptimizerFactory.create(self.model, self.training_config)
   # Support: ADAM, ADAMW, SGD, RMSPROP
   ```

2. **Scheduler**:
   ```python
   self.scheduler = SchedulerFactory.create(self.optimizer, self.training_config)
   # Support: REDUCE_ON_PLATEAU, COSINE_ANNEALING, STEP_LR
   ```

3. **Criterion (Loss)**:
   ```python
   # DÃ©tection type de modÃ¨le
   is_autoencoder = model_type in [CONV_AUTOENCODER, VAE, DENOISING_AE]
   
   if is_autoencoder:
       self.train_criterion = nn.MSELoss()  # Reconstruction loss
       self.val_criterion = nn.MSELoss()
   else:
       # Classification
       if use_class_weights:
           weights = compute_class_weight('balanced', classes, y_train)
           self.train_criterion = nn.CrossEntropyLoss(weight=weights_tensor)
       else:
           self.train_criterion = nn.CrossEntropyLoss()
       
       self.val_criterion = nn.CrossEntropyLoss()  # Validation sans weights
   ```

---

#### **Ã‰tape 5: CrÃ©ation DataLoaders**

```python
# Lignes 390-404
train_loader = DataLoaderFactory.create(
    X_train_norm, y_train,
    batch_size=self.training_config.batch_size,
    shuffle=True,  # âœ… Shuffle pour training
    num_workers=0,
    pin_memory=False
)

val_loader = DataLoaderFactory.create(
    X_val_norm, y_val,
    batch_size=self.training_config.batch_size,
    shuffle=False,  # âœ… Pas de shuffle pour validation
    num_workers=0,
    pin_memory=False
)
```

---

#### **Ã‰tape 6: Boucle d'EntraÃ®nement**

```python
# Ligne 407
train_result = self._training_loop(train_loader, val_loader, y_val)
```

**Pipeline de `_training_loop`**:

```python
def _training_loop(self, train_loader, val_loader, y_val) -> Result:
    # Initialisation
    best_val_metric = float('inf') if is_autoencoder else 0.0
    best_model_state = None
    best_epoch = 0
    patience_counter = 0
    
    # Callbacks: on_train_begin()
    
    for epoch in range(self.training_config.epochs):
        # Callbacks: on_epoch_begin(epoch)
        
        # === PHASE TRAIN ===
        train_loss = self._train_epoch(train_loader, is_autoencoder)
        
        # === PHASE VALIDATION ===
        if is_autoencoder:
            val_loss = self._validate_epoch_autoencoder(val_loader)
            val_metrics = {'loss': val_loss}
        else:
            val_loss, val_metrics = self._validate_epoch(val_loader, y_val)
            # val_metrics = {'accuracy', 'f1', 'loss'}
        
        # === MISE Ã€ JOUR HISTORIQUE ===
        self.history['train_loss'].append(float(train_loss))
        self.history['val_loss'].append(float(val_loss))
        if not is_autoencoder:
            self.history['val_accuracy'].append(float(val_metrics['accuracy']))
            self.history['val_f1'].append(float(val_metrics['f1']))
        self.history['learning_rates'].append(current_lr)
        
        # === SCHEDULER STEP ===
        if self.scheduler is not None:
            if isinstance(self.scheduler, ReduceLROnPlateau):
                self.scheduler.step(val_loss)  # Step avec mÃ©trique
            else:
                self.scheduler.step()  # Step simple
        
        # === EARLY STOPPING ===
        if is_autoencoder:
            improved = val_loss < best_val_metric  # Minimiser loss
        else:
            improved = val_metrics['f1'] > best_val_metric  # Maximiser F1
        
        if improved:
            best_model_state = copy.deepcopy(self.model.state_dict())
            best_epoch = epoch + 1
            patience_counter = 0
            # Checkpoint si configurÃ©
        else:
            patience_counter += 1
        
        # Check early stopping
        if patience_counter >= self.training_config.early_stopping_patience:
            logger.info(f"Early stopping dÃ©clenchÃ© Ã  l'epoch {epoch+1}")
            break
        
        # Callbacks: on_epoch_end(epoch, logs)
    
    # === RESTAURATION MEILLEUR MODÃˆLE ===
    if best_model_state is not None:
        self.model.load_state_dict(best_model_state)
    
    # Callbacks: on_train_end({'training_time': ...})
    
    return Result.ok({...}, training_time=..., best_epoch=...)
```

**DÃ©tails `_train_epoch`** (Phase Train):
```python
def _train_epoch(self, train_loader, is_autoencoder):
    self.model.train()  # Mode training
    total_loss = 0.0
    
    for data, target in train_loader:
        data = data.to(device)
        target = target.to(device)
        
        # Forward pass
        self.optimizer.zero_grad()
        output = self.model(data)
        
        if is_autoencoder:
            loss = self.train_criterion(output, data)  # Reconstruction
        else:
            loss = self.train_criterion(output, target)  # Classification
        
        # Backward pass
        loss.backward()
        
        # Gradient clipping
        if self.training_config.gradient_clip > 0:
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(),
                self.training_config.gradient_clip
            )
        
        self.optimizer.step()
        total_loss += loss.item()
    
    return total_loss / len(train_loader)
```

**DÃ©tails `_validate_epoch`** (Phase Validation):
```python
def _validate_epoch(self, val_loader, y_val):
    self.model.eval()  # Mode evaluation
    total_loss = 0.0
    all_preds = []
    all_targets = []
    
    with torch.no_grad():  # Pas de gradients
        for data, target in val_loader:
            data = data.to(device)
            target = target.to(device)
            
            output = self.model(data)
            loss = self.val_criterion(output, target)
            
            total_loss += loss.item()
            preds = output.argmax(dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
    
    # Calcul mÃ©triques
    accuracy = accuracy_score(all_targets, all_preds)
    f1 = f1_score(all_targets, all_preds, average='weighted')
    avg_loss = total_loss / len(val_loader)
    
    return avg_loss, {'accuracy': accuracy, 'f1': f1}
```

---

#### **Ã‰tape 7: Retour RÃ©sultats StructurÃ©s**

```python
# Ligne 421
return Result.ok(
    self._build_training_result(train_result),
    training_time=total_time
)
```

**Structure RetournÃ©e** (`_build_training_result`):
```python
{
    'model': self.model,  # ModÃ¨le PyTorch entraÃ®nÃ©
    'preprocessor': self.preprocessor,  # Preprocessor fittÃ©
    'history': {
        'success': True,  # Bool (exception)
        'model_type': 'conv_autoencoder',
        'is_autoencoder': True,
        
        # MÃ©triques (LISTES de float)
        'train_loss': [0.5, 0.4, 0.3, ...],
        'val_loss': [0.6, 0.5, 0.4, ...],
        'val_accuracy': [...],  # Vide si autoencoder
        'val_f1': [...],  # Vide si autoencoder
        'learning_rates': [1e-4, 1e-4, 9e-5, ...],
        
        # RÃ©sumÃ©
        'best_epoch': 45,
        'best_val_loss': 0.35,
        'final_train_loss': 0.32,
        'training_time': 1200.5,
        'total_epochs_trained': 50,
        'early_stopping_triggered': False,
        
        # Config
        'input_shape': (3, 256, 256),
        'output_format': 'channels_first',
        'training_config': {...},
        
        # MÃ©tadonnÃ©es
        'metadata': {...}
    }
}
```

---

### **Classe**: `AnomalyAwareTrainer`

**Wrapper** autour de `ComputerVisionTrainer` pour les tÃ¢ches d'anomalie avec taxonomy.

**MÃ©thode**: `train(X_train, y_train, X_val, y_val) -> Result`

```python
def train(self, X_train, y_train, X_val, y_val, callbacks=None) -> Result:
    # CrÃ©ation trainer standard
    trainer = ComputerVisionTrainer(
        model_config=self.model_config,
        training_config=self.training_config,
        callbacks=active_callbacks
    )
    
    # DÃ©lÃ©gation Ã  fit standard
    result = trainer.fit(X_train, y_train, X_val, y_val)
    
    # Copie attributs pour compatibilitÃ©
    if result.success:
        self.model = trainer.model
        self.preprocessor = trainer.preprocessor
        self.history = result.data['history']
    
    return result
```

**Note**: `AnomalyAwareTrainer` utilise le **mÃªme pipeline** que `ComputerVisionTrainer`. La diffÃ©rence est principalement au niveau de la **configuration** (taxonomy, anomaly_type).

---

## ğŸ”® Pipeline de PrÃ©diction

### **MÃ©thode**: `predict(X, return_reconstructed=False, batch_size=None) -> Result`

#### **Flux Complet**:

```python
def predict(self, X, return_reconstructed=False, batch_size=None) -> Result:
    """
    Pipeline de prÃ©diction:
    
    1. VALIDATION (modÃ¨le et preprocessor disponibles)
    2. PREPROCESSING (transform uniquement, pas fit!)
    3. DÃ‰TECTION TYPE MODÃˆLE
    4. CRÃ‰ATION DATALOADER
    5. PRÃ‰DICTION (classifier ou autoencoder)
    6. RETOUR RÃ‰SULTATS STRUCTURÃ‰S
    """
```

---

#### **Ã‰tape 1: Validation**

```python
# Lignes 1031-1034
if self.model is None:
    return Result.err("ModÃ¨le non entraÃ®nÃ©")
if self.preprocessor is None:
    return Result.err("Preprocessor non disponible")
```

---

#### **Ã‰tape 2: Preprocessing**

```python
# Ligne 1037
X_processed = self.preprocessor.transform(X, output_format="channels_first")
```

**âš ï¸ IMPORTANT**: `transform()` uniquement, **JAMAIS** `fit_transform()` sur donnÃ©es de test!

---

#### **Ã‰tape 3: DÃ©tection Type ModÃ¨le**

```python
# Lignes 1040-1042
is_autoencoder = self.model_config.model_type in [
    ModelType.CONV_AUTOENCODER, ModelType.VAE, ModelType.DENOISING_AE
]
```

---

#### **Ã‰tape 4: CrÃ©ation DataLoader**

```python
# Lignes 1045-1054
batch_size = batch_size or self.training_config.batch_size
dummy_labels = np.zeros(len(X_processed))  # Labels dummy (non utilisÃ©s)

test_loader = DataLoaderFactory.create(
    X_processed, dummy_labels,
    batch_size=batch_size,
    shuffle=False,  # âœ… Pas de shuffle pour prÃ©dictions
    num_workers=0,
    pin_memory=False
)
```

---

#### **Ã‰tape 5: PrÃ©diction**

```python
# Ligne 1057
self.model.eval()  # Mode evaluation

if is_autoencoder:
    return self._predict_autoencoder(test_loader, X_processed, return_reconstructed)
else:
    return self._predict_classifier(test_loader)
```

---

### **PrÃ©diction Autoencoder** (`_predict_autoencoder`)

```python
def _predict_autoencoder(self, test_loader, X_processed, return_reconstructed) -> Result:
    reconstruction_errors = []
    reconstructed_images = [] if return_reconstructed else None
    error_maps_list = []  # âœ… Cartes d'erreur spatiales
    
    with torch.no_grad():
        for data, _ in test_loader:
            data = data.to(device)
            reconstructed = self.model(data)
            
            # === ERREUR PAR Ã‰CHANTILLON ===
            errors = torch.mean(
                (data - reconstructed) ** 2,
                dim=tuple(range(1, data.ndim))  # Moyenne sur C, H, W
            ).cpu().numpy()
            reconstruction_errors.extend(errors)
            
            # === CARTES D'ERREUR SPATIALES ===
            if hasattr(self.model, 'get_reconstruction_error_map'):
                batch_error_maps = self.model.get_reconstruction_error_map(data)
                batch_error_maps_np = batch_error_maps[:, 0, :, :].cpu().numpy()  # (B, H, W)
            else:
                # Fallback: calcul manuel
                batch_error_maps = torch.mean((data - reconstructed) ** 2, dim=1, keepdim=True)
                batch_error_maps_np = batch_error_maps[:, 0, :, :].cpu().numpy()
            
            error_maps_list.append(batch_error_maps_np)
            
            if return_reconstructed:
                reconstructed_images.append(reconstructed.cpu().numpy())
    
    reconstruction_errors = np.array(reconstruction_errors)
    
    # === SEUIL AUTOMATIQUE (95Ã¨me percentile) ===
    threshold = np.percentile(reconstruction_errors, 95)
    predictions = (reconstruction_errors > threshold).astype(int)
    
    # === GÃ‰NÃ‰RATION HEATMAPS ===
    if error_maps_list:
        error_maps = np.concatenate(error_maps_list, axis=0)  # (N, H, W)
        
        # Normalisation pour heatmaps
        heatmaps = []
        for error_map in error_maps:
            if error_map.max() > error_map.min():
                normalized = (error_map - error_map.min()) / (error_map.max() - error_map.min() + 1e-8)
            else:
                normalized = np.zeros_like(error_map)
            heatmaps.append(normalized)
        heatmaps = np.array(heatmaps)  # (N, H, W)
    
    # === CONSTRUCTION RÃ‰SULTAT ===
    result_data = {
        'reconstruction_errors': reconstruction_errors,  # (N,)
        'predictions': predictions,  # (N,) binaires (0=normal, 1=anomaly)
        'threshold': float(threshold),
        'error_maps': error_maps,  # (N, H, W)
        'heatmaps': heatmaps,  # (N, H, W) normalisÃ©es [0, 1]
    }
    
    if return_reconstructed:
        result_data['reconstructed'] = np.concatenate(reconstructed_images, axis=0)
    
    return Result.ok(result_data)
```

**Sorties**:
- `reconstruction_errors`: Erreur moyenne par image (scalaire)
- `predictions`: PrÃ©dictions binaires (0=normal, 1=anomaly)
- `threshold`: Seuil utilisÃ© (95Ã¨me percentile)
- `error_maps`: Cartes d'erreur brutes (H, W)
- `heatmaps`: Cartes d'erreur normalisÃ©es [0, 1] pour visualisation

---

### **PrÃ©diction Classifier** (`_predict_classifier`)

```python
def _predict_classifier(self, test_loader) -> Result:
    all_probs = []
    all_preds = []
    
    with torch.no_grad():
        for data, _ in test_loader:
            data = data.to(device)
            output = self.model(data)
            
            # Softmax pour probabilitÃ©s
            probs = torch.softmax(output, dim=1).cpu().numpy()  # (B, num_classes)
            preds = output.argmax(dim=1).cpu().numpy()  # (B,) classes prÃ©dites
            
            all_probs.append(probs)
            all_preds.extend(preds)
    
    return Result.ok({
        'probabilities': np.concatenate(all_probs, axis=0),  # (N, num_classes)
        'predictions': np.array(all_preds)  # (N,) classes prÃ©dites
    })
```

**Sorties**:
- `probabilities`: ProbabilitÃ©s par classe (N, num_classes)
- `predictions`: Classes prÃ©dites (N,)

---

## ğŸ“Š Pipeline d'Ã‰valuation

### **MÃ©thode**: `evaluate(X_test, y_test) -> Result`

#### **Flux Complet**:

```python
def evaluate(self, X_test, y_test) -> Result:
    """
    Pipeline d'Ã©valuation:
    
    1. VALIDATION TEST SET
    2. PREPROCESSING (transform uniquement!)
    3. DÃ‰TECTION TYPE MODÃˆLE
    4. Ã‰VALUATION (classifier ou autoencoder)
    5. RETOUR MÃ‰TRIQUES COMPLÃˆTES
    """
```

---

#### **Ã‰tape 1: Validation Test Set**

```python
# Ligne 1189
test_val = DataValidator.validate_input_data(X_test, y_test, "test")
if not test_val.success:
    return test_val
```

---

#### **Ã‰tape 2: Preprocessing**

```python
# Ligne 1197
X_test_norm = self.preprocessor.transform(X_test, output_format="channels_first")
```

**âš ï¸ CRITIQUE**: `transform()` uniquement, **JAMAIS** `fit()` sur test set!

---

#### **Ã‰tape 3: DÃ©tection Type ModÃ¨le**

```python
# Lignes 1200-1202
is_autoencoder = self.model_config.model_type in [
    ModelType.CONV_AUTOENCODER, ModelType.VAE, ModelType.DENOISING_AE
]

if is_autoencoder:
    return self._evaluate_autoencoder(X_test_norm, y_test)
else:
    return self._evaluate_classifier(X_test_norm, y_test)
```

---

### **Ã‰valuation Classifier** (`_evaluate_classifier`)

```python
def _evaluate_classifier(self, X_test_norm, y_test) -> Result:
    # CrÃ©ation DataLoader
    test_loader = DataLoaderFactory.create(...)
    
    # PrÃ©dictions
    self.model.eval()
    all_preds = []
    all_probs = []
    all_targets = []
    
    with torch.no_grad():
        for data, target in test_loader:
            data = data.to(device)
            output = self.model(data)
            
            probs = torch.softmax(output, dim=1).cpu().numpy()
            preds = output.argmax(dim=1).cpu().numpy()
            
            all_probs.extend(probs)
            all_preds.extend(preds)
            all_targets.extend(target.numpy())
    
    # Calcul mÃ©triques
    metrics = {
        'accuracy': accuracy_score(all_targets, all_preds),
        'precision': precision_score(all_targets, all_preds, average='weighted'),
        'recall': recall_score(all_targets, all_preds, average='weighted'),
        'f1': f1_score(all_targets, all_preds, average='weighted'),
        'confusion_matrix': confusion_matrix(all_targets, all_preds).tolist(),
        'n_samples': len(X_test_norm),
        'n_classes': len(np.unique(y_test))
    }
    
    # AUC-ROC si binaire
    if self.model_config.num_classes == 2:
        metrics['auc_roc'] = roc_auc_score(all_targets, all_probs[:, 1])
    
    # Classification report
    metrics['classification_report'] = classification_report(
        all_targets, all_preds, output_dict=True
    )
    
    return Result.ok(metrics)
```

**MÃ©triques RetournÃ©es**:
- `accuracy`: PrÃ©cision globale
- `precision`: PrÃ©cision moyenne pondÃ©rÃ©e
- `recall`: Rappel moyen pondÃ©rÃ©
- `f1`: F1-score moyen pondÃ©rÃ©
- `confusion_matrix`: Matrice de confusion (list)
- `auc_roc`: AUC-ROC (si binaire)
- `classification_report`: Rapport dÃ©taillÃ© par classe
- `n_samples`, `n_classes`: MÃ©tadonnÃ©es

---

### **Ã‰valuation Autoencoder** (`_evaluate_autoencoder`)

```python
def _evaluate_autoencoder(self, X_test_norm, y_test) -> Result:
    # CrÃ©ation DataLoader
    test_loader = DataLoaderFactory.create(...)
    
    # Calcul erreurs de reconstruction
    self.model.eval()
    reconstruction_errors = []
    all_targets = []
    
    with torch.no_grad():
        for data, target in test_loader:
            data = data.to(device)
            reconstructed = self.model(data)
            
            errors = torch.mean(
                (data - reconstructed) ** 2,
                dim=tuple(range(1, data.ndim))
            ).cpu().numpy()
            
            reconstruction_errors.extend(errors)
            all_targets.extend(target.numpy())
    
    reconstruction_errors = np.array(reconstruction_errors)
    all_targets = np.array(all_targets)
    
    # === CALCUL SEUIL OPTIMAL (sur test set) ===
    # MÃ©thode 1: Percentile 95 des erreurs normales
    normal_errors = reconstruction_errors[all_targets == 0]
    if len(normal_errors) > 0:
        threshold = np.percentile(normal_errors, 95)
    else:
        threshold = np.percentile(reconstruction_errors, 95)
    
    # PrÃ©dictions binaires
    predictions = (reconstruction_errors > threshold).astype(int)
    
    # === MÃ‰TRIQUES ===
    metrics = {
        'accuracy': accuracy_score(all_targets, predictions),
        'precision': precision_score(all_targets, predictions, zero_division=0),
        'recall': recall_score(all_targets, predictions, zero_division=0),
        'f1': f1_score(all_targets, predictions, zero_division=0),
        'confusion_matrix': confusion_matrix(all_targets, predictions).tolist(),
        'auc_roc': roc_auc_score(all_targets, reconstruction_errors),
        'threshold': float(threshold),
        'reconstruction_errors': reconstruction_errors.tolist(),
        'n_samples': len(X_test_norm)
    }
    
    return Result.ok(metrics)
```

**MÃ©triques RetournÃ©es**:
- `accuracy`, `precision`, `recall`, `f1`: MÃ©triques binaires
- `confusion_matrix`: Matrice de confusion
- `auc_roc`: AUC-ROC (utilise `reconstruction_errors` comme scores)
- `threshold`: Seuil utilisÃ© pour binarisation
- `reconstruction_errors`: Liste des erreurs (pour analyse)
- `n_samples`: Nombre d'Ã©chantillons

---

## ğŸ”„ Flux Complet de Bout en Bout

### **1. Chargement des DonnÃ©es (Home Page)**

```
ui/home.py
  â””â”€> load_images_flexible(data_dir)
      â””â”€> Retourne: X, X_norm, y, y_train
      
  â””â”€> STATE.set_images(X, X_norm, y, dir_path, structure, info, y_train=y_train)
      â””â”€> DÃ©tection automatique tÃ¢che (supervised/unsupervised)
      â””â”€> Stockage dans STATE.data
```

---

### **2. Lancement Training (Page Training)**

```
src/app/pages/4_training_computer.py
  â””â”€> handle_training_success(training_result)
      â””â”€> CrÃ©ation TrainingContext:
          â€¢ X_train, y_train, X_val, y_val
          â€¢ model_config (dict)
          â€¢ training_config (dict)
          â€¢ preprocessing_config
      
      â””â”€> training_orchestrator.train(context)
          â”‚
          â”œâ”€> 1. Validation configs
          â”œâ”€> 2. DÃ©marrage MLflow run
          â”œâ”€> 3. CrÃ©ation trainer (ComputerVisionTrainer ou AnomalyAwareTrainer)
          â”œâ”€> 4. trainer.fit(X_train, y_train, X_val, y_val)
          â”‚     â”‚
          â”‚     â”œâ”€> 1. Validation donnÃ©es
          â”‚     â”œâ”€> 2. Setup preprocessing (fit sur train, transform sur val)
          â”‚     â”œâ”€> 3. Construction modÃ¨le
          â”‚     â”œâ”€> 4. Setup optimizer/scheduler/criterion
          â”‚     â”œâ”€> 5. Boucle entraÃ®nement (epochs)
          â”‚     â”‚     â”œâ”€> Phase train
          â”‚     â”‚     â”œâ”€> Phase validation
          â”‚     â”‚     â”œâ”€> Early stopping check
          â”‚     â”‚     â””â”€> Checkpointing
          â”‚     â””â”€> 6. Retour Result avec model, history, preprocessor
          â”‚
          â”œâ”€> 5. Log mÃ©triques MLflow
          â”œâ”€> 6. Log artifacts MLflow (modÃ¨le, preprocessor, config)
          â””â”€> 7. Retour TrainingResult
      
      â””â”€> Stockage dans STATE.training_results:
          â€¢ model
          â€¢ history
          â€¢ preprocessor
          â€¢ model_config
          â€¢ mlflow_run_id
```

---

### **3. Ã‰valuation (Page Evaluation)**

```
src/app/pages/5_anomaly_evaluation.py
  â””â”€> RÃ©cupÃ©ration depuis STATE.training_results:
      â€¢ model = STATE.training_results["model"]
      â€¢ history = STATE.training_results["history"]
      â€¢ preprocessor = STATE.training_results["preprocessor"]
      â€¢ model_config = STATE.training_results["model_config"]
  
  â””â”€> CrÃ©ation trainer (si nÃ©cessaire):
      trainer = ComputerVisionTrainer(model_config, training_config)
      trainer.model = model
      trainer.preprocessor = preprocessor
  
  â””â”€> PrÃ©dictions sur X_test:
      pred_result = trainer.predict(X_test, return_reconstructed=True, return_localization=True)
      
      Retourne:
      â€¢ Pour autoencoder:
        - reconstruction_errors
        - predictions (binaires)
        - error_maps (spatiales)
        - heatmaps (normalisÃ©es)
        - binary_masks (si return_localization=True)
      â€¢ Pour classifier:
        - probabilities
        - predictions (classes)
  
  â””â”€> Ã‰valuation complÃ¨te:
      eval_result = trainer.evaluate(X_test, y_test)
      
      Retourne:
      â€¢ MÃ©triques (accuracy, precision, recall, f1, auc_roc)
      â€¢ Confusion matrix
      â€¢ Classification report
  
  â””â”€> Visualisations:
      â€¢ Courbes d'entraÃ®nement (loss, accuracy)
      â€¢ Matrice de confusion
      â€¢ Heatmaps d'anomalies
      â€¢ ROC curve, Precision-Recall curve
      â€¢ Analyse erreurs (false positives, false negatives)
```

---

### **SchÃ©ma Visuel du Flux Complet**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           1. HOME PAGE                                  â”‚
â”‚  Upload images â†’ load_images_flexible() â†’ STATE.set_images()           â”‚
â”‚  DÃ©tection automatique: supervised/unsupervised                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        2. TRAINING PAGE                                 â”‚
â”‚  Configuration â†’ TrainingContext â†’ training_orchestrator.train()       â”‚
â”‚                                                                         â”‚
â”‚  ORCHESTRATEUR:                                                         â”‚
â”‚    â”œâ”€ Validation configs                                               â”‚
â”‚    â”œâ”€ MLflow run start                                                 â”‚
â”‚    â”œâ”€ CrÃ©ation trainer                                                 â”‚
â”‚    â”‚                                                                    â”‚
â”‚    â””â”€ TRAINER.fit():                                                   â”‚
â”‚         â”œâ”€ Preprocessing (fit sur train, transform sur val)            â”‚
â”‚         â”œâ”€ Construction modÃ¨le                                         â”‚
â”‚         â”œâ”€ Setup optimizer/scheduler/criterion                         â”‚
â”‚         â””â”€ Boucle entraÃ®nement (epochs)                                â”‚
â”‚              â”œâ”€ Train epoch                                            â”‚
â”‚              â”œâ”€ Validate epoch                                         â”‚
â”‚              â”œâ”€ Early stopping                                         â”‚
â”‚              â””â”€ Checkpointing                                          â”‚
â”‚                                                                         â”‚
â”‚    â”œâ”€ Log MLflow (mÃ©triques, artifacts)                                â”‚
â”‚    â””â”€ Retour TrainingResult â†’ STATE.training_results                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        3. EVALUATION PAGE                               â”‚
â”‚  RÃ©cupÃ©ration depuis STATE.training_results                            â”‚
â”‚                                                                         â”‚
â”‚  PRÃ‰DICTIONS:                                                           â”‚
â”‚    trainer.predict(X_test)                                             â”‚
â”‚      â”œâ”€ Preprocessing (transform uniquement!)                          â”‚
â”‚      â”œâ”€ Forward pass (model.eval())                                    â”‚
â”‚      â””â”€ Retour: predictions, probabilities, error_maps, heatmaps       â”‚
â”‚                                                                         â”‚
â”‚  Ã‰VALUATION:                                                            â”‚
â”‚    trainer.evaluate(X_test, y_test)                                    â”‚
â”‚      â”œâ”€ PrÃ©dictions                                                    â”‚
â”‚      â”œâ”€ Calcul mÃ©triques (accuracy, precision, recall, f1, auc_roc)   â”‚
â”‚      â””â”€ Retour: mÃ©triques complÃ¨tes                                    â”‚
â”‚                                                                         â”‚
â”‚  VISUALISATIONS:                                                        â”‚
â”‚    â€¢ Courbes d'entraÃ®nement                                            â”‚
â”‚    â€¢ Matrice de confusion                                              â”‚
â”‚    â€¢ Heatmaps d'anomalies                                              â”‚
â”‚    â€¢ ROC/PR curves                                                     â”‚
â”‚    â€¢ Analyse erreurs                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”‘ Points Critiques

### **1. Pas de Fuite de DonnÃ©es**

âœ… **Preprocessing**:
- `fit_transform()` sur `X_train` uniquement
- `transform()` sur `X_val` et `X_test`
- Le preprocessor est fittÃ© une seule fois sur le train

âœ… **Early Stopping**:
- BasÃ© sur `val_loss` ou `val_f1` (pas sur test)

âœ… **Checkpointing**:
- Meilleur modÃ¨le basÃ© sur validation uniquement

---

### **2. Gestion Formats**

âœ… **Channels First/Last**:
- Auto-dÃ©tection dans `DataPreprocessor`
- Conversion automatique vers `channels_first` pour PyTorch
- Gestion cohÃ©rente dans tout le pipeline

âœ… **Tensor â†” NumPy**:
- Conversion explicite avec `.cpu().numpy()`
- Gestion device (CPU/GPU) avec `DeviceManager`

---

### **3. Robustesse**

âœ… **Gestion Erreurs**:
- Try/catch Ã  chaque Ã©tape critique
- Retour `Result.err()` avec message explicite
- Logging dÃ©taillÃ© pour debugging

âœ… **Validation**:
- Validation donnÃ©es avant processing
- Validation configs (types, ranges)
- Validation rÃ©sultats (non-None, shapes cohÃ©rentes)

âœ… **Fallbacks**:
- Preprocessor fallback si manquant
- Historique minimal si erreur
- Default configs si invalides

---

## ğŸ“ RÃ©sumÃ© des MÃ©thodes ClÃ©s

| MÃ©thode | Classe | EntrÃ©e | Sortie | RÃ´le |
|---------|--------|--------|--------|------|
| `train()` | Orchestrateur | `TrainingContext` | `TrainingResult` | Orchestration complÃ¨te |
| `fit()` | `ComputerVisionTrainer` | `X_train, y_train, X_val, y_val` | `Result` | EntraÃ®nement modÃ¨le |
| `predict()` | `ComputerVisionTrainer` | `X` | `Result` | PrÃ©dictions |
| `evaluate()` | `ComputerVisionTrainer` | `X_test, y_test` | `Result` | MÃ©triques complÃ¨tes |
| `_training_loop()` | `ComputerVisionTrainer` | `train_loader, val_loader` | `Result` | Boucle epochs |
| `_predict_autoencoder()` | `ComputerVisionTrainer` | `test_loader` | `Result` | PrÃ©dictions autoencoder |
| `_predict_classifier()` | `ComputerVisionTrainer` | `test_loader` | `Result` | PrÃ©dictions classifier |
| `_evaluate_autoencoder()` | `ComputerVisionTrainer` | `X_test_norm, y_test` | `Result` | Ã‰valuation autoencoder |
| `_evaluate_classifier()` | `ComputerVisionTrainer` | `X_test_norm, y_test` | `Result` | Ã‰valuation classifier |

---

## âœ… Conclusion

Cette architecture garantit:
- âœ… **SÃ©paration des responsabilitÃ©s**: Orchestrateur â‰  Trainer
- âœ… **Pas de fuite de donnÃ©es**: Preprocessing strict
- âœ… **Robustesse**: Validation et gestion erreurs complÃ¨tes
- âœ… **ObservabilitÃ©**: Logging et MLflow intÃ©grÃ©s
- âœ… **FlexibilitÃ©**: Support classification et anomaly detection
- âœ… **Production-ready**: Gestion configs, checkpointing, early stopping

**Le systÃ¨me est prÃªt pour la production !** ğŸš€

