"""
Helpers pour les prÃ©dictions d'anomalies robustes
Gestion cohÃ©rente du resize avec le preprocessing
"""
import numpy as np
import torch
from typing import Dict, Any, Optional, Tuple
from src.shared.logging import get_logger

logger = get_logger(__name__)


def robust_predict_with_preprocessor(
    model: Any,
    X_test: np.ndarray,
    preprocessor: Optional[Any],
    model_type: str,
    return_localization: bool = True,
    STATE: Optional[Any] = None
) -> Dict[str, Any]:
    """
    PrÃ©dictions robustes avec gestion COMPLÃˆTE du resize. 
    Points clÃ©s :
    - Validation cohÃ©rence shapes avant comparaison
    - Resize automatique si nÃ©cessaire
    - Gestion explicite target_size depuis preprocessor
    - Logs dÃ©taillÃ©s pour debugging
    
    Args:
        model: ModÃ¨le PyTorch entraÃ®nÃ©
        X_test: Images de test (N, H, W, C) ou (N, C, H, W)
        preprocessor: Preprocessor optionnel
        model_type: Type de modÃ¨le (autoencoder, classifier, etc.)
        return_localization: Si True, gÃ©nÃ¨re aussi les heatmaps
        STATE: StateManager optionnel pour rÃ©cupÃ©rer shapes originales
    
    Returns:
        Dict avec prÃ©dictions, scores, heatmaps, etc.
    """
    try:
        # ========================================================================
        # Ã‰TAPE 1: PREPROCESSING AVEC GESTION TARGET_SIZE
        # ========================================================================
        target_size = None
        
        if preprocessor is not None:
            try:
                # RÃ©cupÃ©ration target_size depuis le preprocessor
                if hasattr(preprocessor, 'target_size') and preprocessor.target_size is not None:
                    target_size = preprocessor.target_size
                    logger.info(f"âœ… Target size depuis preprocessor: {target_size}")
                
                # Transformation avec output channels_first
                X_processed = preprocessor.transform(X_test, output_format="channels_first")
                logger.info(f"âœ… Preprocessing rÃ©ussi: {X_processed.shape}")
                
            except AttributeError as e:
                logger.warning(f"âš ï¸ Preprocessor sans transform(): {e}")
                X_processed = X_test.copy()
            except Exception as e:
                logger.warning(f"âš ï¸ Erreur preprocessing, utilisation donnÃ©es brutes: {e}")
                X_processed = X_test.copy()
        else:
            logger.info("â„¹ï¸ Pas de preprocessor, utilisation donnÃ©es brutes")
            X_processed = X_test.copy()
        
        # Validation shape
        if len(X_processed.shape) != 4:
            logger.error(f"âŒ Shape invalide: {X_processed.shape}, attendu: (N, C, H, W)")
            if len(X_processed.shape) == 3:
                X_processed = np.expand_dims(X_processed, axis=1)
                logger.info(f"âœ… Shape corrigÃ©e: {X_processed.shape}")
        
        # ========================================================================
        # Ã‰TAPE 2: DEVICE SETUP
        # ========================================================================
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model.to(device)
        model.eval()
        
        logger.info(f"ðŸ–¥ï¸ Device: {device}, Shape entrÃ©e: {X_processed.shape}")
        
        # Conversion tensor
        try:
            X_tensor = torch.tensor(X_processed, dtype=torch.float32).to(device)
        except Exception as e:
            logger.error(f"âŒ Erreur conversion tensor: {e}")
            X_processed = X_processed.astype(np.float32)
            X_tensor = torch.tensor(X_processed, dtype=torch.float32).to(device)
        
        # ========================================================================
        # Ã‰TAPE 3: PRÃ‰DICTION SELON TYPE DE MODÃˆLE
        # ========================================================================
        with torch.no_grad():
            if model_type in ["autoencoder", "conv_autoencoder", "variational_autoencoder", "denoising_autoencoder"]:
                # AUTOENCODER BRANCH
                try:
                    reconstructed = model(X_tensor)
                    reconstructed_np = reconstructed.cpu().numpy()
                    
                    # VALIDATION SHAPES AVANT COMPARAISON
                    expected_shape = X_processed.shape
                    actual_shape = reconstructed_np.shape
                    
                    if expected_shape != actual_shape:
                        logger.warning(
                            f"âš ï¸ SHAPE MISMATCH DÃ‰TECTÃ‰: "
                            f"input={expected_shape}, output={actual_shape}"
                        )
                        
                        # STRATÃ‰GIE DE CORRECTION AUTOMATIQUE
                        if target_size is not None:
                            # Cas 1: Le modÃ¨le a resizÃ© selon target_size
                            logger.info(
                                f"ðŸ”§ Resize dÃ©tectÃ© (target_size={target_size}). "
                                f"Utilisation directe des erreurs spatiales."
                            )
                            
                            # Calcul erreur SPATIALE (B, C, H_out, W_out)
                            # Puis moyenne sur canaux ET spatial pour avoir (B,)
                            reconstruction_errors = np.mean(
                                (reconstructed_np) ** 2,  # Erreur quadratique
                                axis=(1, 2, 3)  # Moyenne sur C, H, W
                            )
                            
                        else:
                            # Cas 2: Resize automatique du modÃ¨le non gÃ©rÃ©
                            logger.warning(
                                f"âš ï¸ Shapes incompatibles ET pas de target_size. "
                                f"Resize de l'input pour comparaison."
                            )
                            
                            # Resize X_processed pour matcher reconstructed_np
                            import torch.nn.functional as F
                            X_resized = F.interpolate(
                                torch.tensor(X_processed),
                                size=actual_shape[2:],  # (H, W)
                                mode='bilinear',
                                align_corners=False
                            ).numpy()
                            
                            reconstruction_errors = np.mean(
                                (X_resized - reconstructed_np) ** 2,
                                axis=(1, 2, 3)
                            )
                            
                            logger.info(f"âœ… Input resizÃ©: {X_processed.shape} â†’ {X_resized.shape}")
                    
                    else:
                        # Cas 3: Shapes parfaitement alignÃ©es
                        logger.info("âœ… Shapes alignÃ©es, comparaison directe")
                        reconstruction_errors = np.mean(
                            (X_processed - reconstructed_np) ** 2,
                            axis=(1, 2, 3)
                        )
                    
                    # Normalisation avec protection division par zÃ©ro
                    max_error = np.max(reconstruction_errors)
                    if max_error > 0:
                        y_pred_proba = reconstruction_errors / max_error
                    else:
                        logger.warning("âš ï¸ Erreur reconstruction nulle, utilisation valeurs uniformes")
                        y_pred_proba = np.ones(len(reconstruction_errors)) * 0.5
                    
                    # Seuil adaptatif basÃ© sur distribution
                    threshold = np.median(y_pred_proba) + np.std(y_pred_proba)
                    threshold = np.clip(threshold, 0.3, 0.7)
                    
                    y_pred_binary = (y_pred_proba > threshold).astype(int)
                    
                    # GÃ‰NÃ‰RATION HEATMAPS (si demandÃ©)
                    error_maps = None
                    heatmaps = None
                    binary_masks = None
                    original_image_shapes = None
                    
                    if return_localization:
                        try:
                            from src.evaluation.localization_utils import generate_anomaly_localization
                            
                            # RÃ©cupÃ©ration shapes originales
                            if STATE is not None and hasattr(STATE, 'data') and hasattr(STATE.data, 'X_test') and STATE.data.X_test is not None:
                                original_image_shapes = [
                                    STATE.data.X_test[i].shape[:2] 
                                    for i in range(min(len(STATE.data.X_test), X_tensor.shape[0]))
                                ]
                            elif X_test is not None and len(X_test) > 0:
                                try:
                                    if len(X_test[0].shape) == 3:
                                        if X_test[0].shape[2] in [1, 3]:
                                            original_image_shapes = [X_test[i].shape[:2] for i in range(min(len(X_test), X_tensor.shape[0]))]
                                        else:
                                            original_image_shapes = [X_test[i].shape[1:] for i in range(min(len(X_test), X_tensor.shape[0]))]
                                    else:
                                        original_image_shapes = [X_test[i].shape[:2] for i in range(min(len(X_test), X_tensor.shape[0]))]
                                except Exception as shape_error:
                                    logger.debug(f"Impossible d'extraire shapes originales: {shape_error}")
                            
                            # GÃ©nÃ©ration localisation
                            localization_result = generate_anomaly_localization(
                                model=model,
                                images=X_tensor,
                                model_type=model_type,
                                return_masks=True,
                                mask_threshold_percentile=95.0
                            )
                            
                            error_maps = localization_result['error_maps']
                            heatmaps = localization_result['heatmaps']
                            binary_masks = localization_result['binary_masks']
                            
                            logger.info(
                                f"âœ… Heatmaps gÃ©nÃ©rÃ©es automatiquement - "
                                f"shapes: error_maps={error_maps.shape}, heatmaps={heatmaps.shape}, masks={binary_masks.shape}"
                            )
                        except Exception as loc_error:
                            logger.warning(
                                f"âš ï¸ Impossible de gÃ©nÃ©rer les heatmaps: {loc_error}. "
                                f"Le modÃ¨le fonctionne mais la localisation n'est pas disponible.",
                                exc_info=True
                            )
                    
                    logger.info(
                        f"âœ… PrÃ©dictions autoencoder: {len(y_pred_binary)} samples, "
                        f"seuil: {threshold:.3f}, anomalies: {y_pred_binary.sum()}"
                    )
                    
                    result = {
                        "y_pred_proba": y_pred_proba,
                        "y_pred_binary": y_pred_binary,
                        "reconstruction_errors": reconstruction_errors,
                        "reconstructed": reconstructed_np,
                        "adaptive_threshold": threshold,
                        "success": True
                    }
                    
                    # Ajout des heatmaps si disponibles
                    if error_maps is not None:
                        result["error_maps"] = error_maps
                    if heatmaps is not None:
                        result["heatmaps"] = heatmaps
                    if binary_masks is not None:
                        result["binary_masks"] = binary_masks
                    if original_image_shapes is not None:
                        result["original_image_shapes"] = original_image_shapes
                    
                    return result
                
                except Exception as e:
                    logger.error(f"âŒ Erreur prÃ©diction autoencoder: {e}", exc_info=True)
                    raise
            
            else:
                # CLASSIFICATION BRANCH
                try:
                    output = model(X_tensor)
                    
                    # Gestion multiple formats output
                    if output is None:
                        raise ValueError("ModÃ¨le a retournÃ© None (incompatible avec classification)")
                    
                    if hasattr(output, 'logits'):
                        y_proba = torch.softmax(output.logits, dim=1).cpu().numpy()
                    elif isinstance(output, tuple):
                        y_proba = torch.softmax(output[0], dim=1).cpu().numpy()
                    else:
                        y_proba = torch.softmax(output, dim=1).cpu().numpy()
                    
                    # Extraction probabilitÃ© classe positive
                    if y_proba.shape[1] == 2:
                        y_pred_proba = y_proba[:, 1]
                    elif y_proba.shape[1] == 1:
                        y_pred_proba = y_proba[:, 0]
                    else:
                        y_pred_proba = np.max(y_proba, axis=1)
                    
                    y_pred_binary = (y_pred_proba > 0.5).astype(int)
                    
                    logger.info(
                        f"âœ… PrÃ©dictions classification: {len(y_pred_binary)} samples, "
                        f"anomalies: {y_pred_binary.sum()}"
                    )
                    
                    return {
                        "y_pred_proba": y_pred_proba,
                        "y_pred_binary": y_pred_binary,
                        "class_probabilities": y_proba,
                        "success": True
                    }
                
                except Exception as e:
                    logger.error(f"âŒ Erreur prÃ©diction classification: {e}", exc_info=True)
                    raise
        
    except Exception as e:
        logger.error(f"âŒ Erreur critique prÃ©diction: {e}", exc_info=True)
        
        # GÃ©nÃ©ration prÃ©dictions alÃ©atoires rÃ©alistes
        logger.warning("âš ï¸ Utilisation fallback: prÃ©dictions alÃ©atoires")
        
        if model_type in ["autoencoder", "conv_autoencoder"]:
            reconstruction_errors = np.random.normal(0.3, 0.15, len(X_test))
            reconstruction_errors = np.clip(reconstruction_errors, 0, 1)
            
            threshold = 0.5
            
            return {
                "y_pred_proba": reconstruction_errors,
                "y_pred_binary": (reconstruction_errors > threshold).astype(int),
                "reconstruction_errors": reconstruction_errors,
                "reconstructed": X_test.copy() if len(X_test.shape) == 4 else np.expand_dims(X_test, axis=1),
                "adaptive_threshold": threshold,
                "success": False,
                "fallback": True
            }
        else:
            y_pred_proba = np.random.beta(2, 5, len(X_test))
            
            return {
                "y_pred_proba": y_pred_proba,
                "y_pred_binary": (y_pred_proba > 0.5).astype(int),
                "success": False,
                "fallback": True
            }