"""
Helpers pour la page d'√©valuation d'anomalies
Fonctions m√©tier extraites de 5_anomaly_evaluation.py
"""
import numpy as np
import plotly.graph_objects as go
from typing import Dict, Any, Optional
from src.shared.logging import get_logger

logger = get_logger(__name__)


def safe_convert_history(history: Dict[str, Any]) -> Dict[str, Any]:
    """
    Corrige l'historique d'entra√Ænement.
    
    Args:
        history: Historique brut de l'entra√Ænement
    
    Returns:
        Historique corrig√© avec valeurs num√©riques
    """
    if not history:
        return {}
    
    fixed_history = {}
    for key, value in history.items():
        if isinstance(value, bool):
            fixed_history[key] = [1.0 if value else 0.0]
        elif isinstance(value, (list, np.ndarray)) and len(value) > 0:
            cleaned = []
            for item in value:
                if isinstance(item, bool):
                    cleaned.append(1.0 if item else 0.0)
                elif isinstance(item, (int, float)):
                    cleaned.append(float(item))
                else:
                    cleaned.append(0.0)
            fixed_history[key] = cleaned
        else:
            fixed_history[key] = value
    
    return fixed_history


def analyze_false_positives(
    X_test: np.ndarray,
    y_test: np.ndarray,
    y_pred_binary: np.ndarray
) -> Dict[str, Any]:
    """
    Analyse des erreurs de classification.
    
    Args:
        X_test: Images de test
        y_test: Labels r√©els
        y_pred_binary: Pr√©dictions binaires
    
    Returns:
        Dictionnaire avec analyse compl√®te des erreurs
    """
    false_positives = np.where((y_test == 0) & (y_pred_binary == 1))[0]
    false_negatives = np.where((y_test == 1) & (y_pred_binary == 0))[0]
    true_positives = np.where((y_test == 1) & (y_pred_binary == 1))[0]
    true_negatives = np.where((y_test == 0) & (y_pred_binary == 0))[0]
    
    return {
        "false_positives": false_positives,
        "false_negatives": false_negatives,
        "true_positives": true_positives,
        "true_negatives": true_negatives,
        "fp_count": len(false_positives),
        "fn_count": len(false_negatives),
        "tp_count": len(true_positives),
        "tn_count": len(true_negatives),
        "fp_rate": len(false_positives) / max(len(y_test[y_test == 0]), 1),
        "fn_rate": len(false_negatives) / max(len(y_test[y_test == 1]), 1),
        "total_errors": len(false_positives) + len(false_negatives)
    }


def get_performance_status(
    metric_value: float,
    metric_type: str
) -> tuple[str, str]:
    """
    Retourne le statut de performance bas√© sur une m√©trique.
    
    Args:
        metric_value: Valeur de la m√©trique
        metric_type: Type de m√©trique (auc_roc, f1_score, etc.)
    
    Returns:
        Tuple (status, status_text) avec le statut et son label
    """
    if metric_type == "auc_roc":
        if metric_value >= 0.9:
            return "excellent", "üéØ Excellent"
        elif metric_value >= 0.8:
            return "good", "‚úÖ Bon"
        elif metric_value >= 0.7:
            return "warning", "‚ö†Ô∏è Moyen"
        else:
            return "critical", "‚ùå Critique"
    elif metric_type in ["f1_score", "precision", "recall"]:
        if metric_value >= 0.85:
            return "excellent", "üéØ Excellent"
        elif metric_value >= 0.75:
            return "good", "‚úÖ Bon"
        elif metric_value >= 0.6:
            return "warning", "‚ö†Ô∏è Moyen"
        else:
            return "critical", "‚ùå Critique"
    else:
        if metric_value >= 0.8:
            return "good", "‚úÖ Bon"
        elif metric_value >= 0.6:
            return "warning", "‚ö†Ô∏è Moyen"
        else:
            return "critical", "‚ùå Critique"


def create_performance_summary(
    metrics: Dict[str, float],
    error_analysis: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Cr√©e un r√©sum√© des performances globales.
    
    Args:
        metrics: Dictionnaire de m√©triques
        error_analysis: Analyse des erreurs
    
    Returns:
        R√©sum√© avec score global, statut production, etc.
    """
    weights = {
        'auc_roc': 0.25,
        'f1_score': 0.25,
        'precision': 0.20,
        'recall': 0.20,
        'specificity': 0.10
    }
    
    total_score = sum(metrics.get(k, 0) * v for k, v in weights.items() if k in metrics)
    valid_weight = sum(v for k, v in weights.items() if k in metrics)
    overall_score = total_score / valid_weight if valid_weight > 0 else 0
    
    summary = {
        "overall_score": overall_score,
        "production_ready": overall_score >= 0.75,
        "risk_level": "low" if overall_score >= 0.85 else "medium" if overall_score >= 0.75 else "high",
        "strengths": [],
        "weaknesses": []
    }
    
    if overall_score >= 0.85:
        summary["status"] = "excellent"
        summary["strengths"] = ["Performances exceptionnelles", "Pr√™t production"]
    elif overall_score >= 0.75:
        summary["status"] = "good"
        summary["strengths"] = ["Bonnes performances"]
        summary["weaknesses"] = ["Optimisations possibles"]
    elif overall_score >= 0.6:
        summary["status"] = "warning"
        summary["weaknesses"] = ["Optimisations n√©cessaires"]
    else:
        summary["status"] = "critical"
        summary["weaknesses"] = ["Re-entra√Ænement recommand√©"]
    
    return summary


def generate_recommendations(
    metrics: Dict[str, float],
    model_type: str,
    error_analysis: Dict[str, Any],
    performance_summary: Dict[str, Any]
) -> list[Dict[str, str]]:
    """
    G√©n√®re des recommandations bas√©es sur les performances.
    
    Args:
        metrics: M√©triques calcul√©es
        model_type: Type de mod√®le
        error_analysis: Analyse des erreurs
        performance_summary: R√©sum√© des performances
    
    Returns:
        Liste de recommandations avec priorit√©
    """
    recommendations = []
    
    if performance_summary["overall_score"] < 0.6:
        recommendations.append({
            "priority": "high",
            "icon": "üî¥",
            "action": "Re-entra√Ænement complet",
            "message": "Performances insuffisantes. Re-entra√Æner avec plus de donn√©es."
        })
    
    if metrics.get('recall', 1) < 0.7:
        recommendations.append({
            "priority": "high",
            "icon": "üîç",
            "action": "Am√©liorer d√©tection",
            "message": "Rappel faible. Anomalies manqu√©es. Ajuster le seuil."
        })
    
    if metrics.get('precision', 1) < 0.7:
        recommendations.append({
            "priority": "medium",
            "icon": "‚öñÔ∏è",
            "action": "R√©duire faux positifs",
            "message": "Trop de faux positifs. Augmenter seuil ou am√©liorer donn√©es."
        })
    
    if error_analysis.get('fp_rate', 0) > 0.1:
        recommendations.append({
            "priority": "medium",
            "icon": "üìä",
            "action": "Analyser faux positifs",
            "message": f"Taux FP √©lev√© ({error_analysis['fp_rate']:.1%}). Examiner images."
        })
    
    if performance_summary["production_ready"]:
        recommendations.append({
            "priority": "low",
            "icon": "üöÄ",
            "action": "D√©ploiement production",
            "message": "Mod√®le pr√™t. Configurer monitoring."
        })
    
    return recommendations


def create_performance_radar(metrics: Dict[str, float]) -> go.Figure:
    """
    Cr√©e un graphique radar des performances.
    
    Args:
        metrics: Dictionnaire de m√©triques
    
    Returns:
        Figure Plotly avec graphique radar
    """
    categories = ['AUC-ROC', 'F1-Score', 'Precision', 'Recall', 'Specificity']
    values = [
        metrics.get('auc_roc', 0),
        metrics.get('f1_score', 0),
        metrics.get('precision', 0),
        metrics.get('recall', 0),
        metrics.get('specificity', 0)
    ]
    
    fig = go.Figure()
    
    fig.add_trace(go.Scatterpolar(
        r=values + [values[0]],
        theta=categories + [categories[0]],
        fill='toself',
        fillcolor='rgba(99, 102, 241, 0.3)',
        line=dict(color='#6366f1', width=3),
        name='Performance'
    ))
    
    fig.update_layout(
        polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
        showlegend=False,
        height=400,
        title="Analyse Multidimensionnelle"
    )
    
    return fig


def plot_error_distribution(error_analysis: Dict[str, Any]) -> go.Figure:
    """
    Graphique de distribution des erreurs.
    
    Args:
        error_analysis: Analyse des erreurs
    
    Returns:
        Figure Plotly avec graphique en camembert
    """
    labels = ['Vrais Positifs', 'Faux Positifs', 'Vrais N√©gatifs', 'Faux N√©gatifs']
    values = [
        error_analysis['tp_count'],
        error_analysis['fp_count'],
        error_analysis['tn_count'],
        error_analysis['fn_count']
    ]
    
    colors = ['#10b981', '#ef4444', '#3b82f6', '#f59e0b']
    
    fig = go.Figure(data=[go.Pie(
        labels=labels,
        values=values,
        hole=.4,
        marker_colors=colors,
        textinfo='label+percent+value'
    )])
    
    fig.update_layout(
        title="Distribution des Pr√©dictions",
        height=400
    )
    
    return fig


