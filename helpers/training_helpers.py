"""
Module de fonctions utilitaires pour l'entra√Ænement ML.
‚úÖ D√©plac√© depuis utils/training_helpers.py
‚úÖ Fonctions compl√®tes et production-ready
Version: 2.0
"""
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional
from src.models.catalog import MODEL_CATALOG
from src.config.constants import TRAINING_CONSTANTS
from src.shared.logging import get_logger

logger = get_logger(__name__)


class TrainingHelpers:
    """Helpers sp√©cifiques √† l'entra√Ænement des mod√®les"""
    
    @staticmethod
    def get_task_specific_models(task_type: str) -> List[str]:
        """Retourne les mod√®les disponibles pour une t√¢che"""
        try:
            models = list(MODEL_CATALOG.get(task_type, {}).keys())
            logger.info(f"‚úÖ {len(models)} mod√®les disponibles pour {task_type}")
            return models
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration mod√®les pour {task_type}: {e}")
            return []
    
    @staticmethod
    def get_default_models_for_task(task_type: str) -> List[str]:
        """Retourne les mod√®les par d√©faut recommand√©s pour une t√¢che"""
        default_models = {
            'classification': ['RandomForest', 'XGBoost', 'LogisticRegression', 'SVM'],
            'regression': ['RandomForest', 'XGBoost', 'LinearRegression', 'Ridge'],
            'clustering': ['KMeans', 'DBSCAN', 'GaussianMixture', 'AgglomerativeClustering']
        }
        
        available_models = TrainingHelpers.get_task_specific_models(task_type)
        recommended = [model for model in default_models.get(task_type, []) 
                      if model in available_models]
        
        logger.info(f"‚úÖ {len(recommended)} mod√®les recommand√©s pour {task_type}")
        return recommended
    
    @staticmethod
    def process_training_results(results: List[Dict], task_type: str) -> Dict[str, Any]:
        """Traite et analyse les r√©sultats d'entra√Ænement de fa√ßon robuste"""
        analysis = {
            "successful_models": [],
            "failed_models": [],
            "best_model": None,
            "performance_summary": {},
            "warnings": [],
            "recommendations": []
        }
        
        try:
            # S√©paration mod√®les r√©ussis/√©chou√©s
            for result in results:
                if result.get('success', False) and not result.get('metrics', {}).get('error'):
                    analysis["successful_models"].append(result)
                else:
                    analysis["failed_models"].append(result)
            
            # Analyse des mod√®les r√©ussis
            if analysis["successful_models"]:
                # D√©termination m√©trique principale
                primary_metric = (
                    'silhouette_score' if task_type == 'clustering' 
                    else 'r2' if task_type == 'regression' 
                    else 'accuracy'
                )
                
                # Recherche meilleur mod√®le
                valid_models = [
                    m for m in analysis["successful_models"] 
                    if m.get('metrics', {}).get(primary_metric) is not None
                ]
                
                if valid_models:
                    analysis["best_model"] = max(
                        valid_models, 
                        key=lambda x: x['metrics'][primary_metric]
                    )
                
                # Statistiques de performance
                metrics_data = {}
                for model in analysis["successful_models"]:
                    for metric, value in model.get('metrics', {}).items():
                        if isinstance(value, (int, float)) and not np.isnan(value):
                            if metric not in metrics_data:
                                metrics_data[metric] = []
                            metrics_data[metric].append(value)
                
                analysis["performance_summary"] = {
                    metric: {
                        'mean': np.mean(values),
                        'std': np.std(values),
                        'min': np.min(values),
                        'max': np.max(values),
                        'count': len(values)
                    }
                    for metric, values in metrics_data.items()
                }
                
                # Recommandations intelligentes
                analysis["recommendations"] = TrainingHelpers._generate_recommendations(
                    analysis, primary_metric, len(results)
                )
                
                # Warnings sp√©cifiques
                if analysis["performance_summary"].get(primary_metric, {}).get('std', 0) > 0.1:
                    analysis["warnings"].append("Grande variance entre les mod√®les - donn√©es possiblement incoh√©rentes")
            
            logger.info(f"‚úÖ Analyse r√©sultats: {len(analysis['successful_models'])} r√©ussis, "
                       f"{len(analysis['failed_models'])} √©chou√©s")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur analyse r√©sultats: {e}")
            analysis["warnings"].append(f"Erreur analyse: {str(e)[:100]}")
        
        return analysis
    
    @staticmethod
    def _generate_recommendations(analysis: Dict, primary_metric: str, total_models: int) -> List[str]:
        """G√©n√®re des recommandations intelligentes bas√©es sur les r√©sultats"""
        recommendations = []
        
        n_successful = len(analysis["successful_models"])
        
        # Taux de succ√®s
        if n_successful == 0:
            recommendations.append("‚ùå Aucun mod√®le r√©ussi - V√©rifiez la qualit√© des donn√©es et la configuration")
        elif n_successful < total_models / 2:
            recommendations.append("‚ö†Ô∏è Moins de 50% de r√©ussite - Optimisez le preprocessing et les hyperparam√®tres")
        elif n_successful == total_models:
            recommendations.append("‚úÖ Tous les mod√®les ont r√©ussi - Excellente configuration!")
        
        # Performance
        if analysis["best_model"]:
            best_score = analysis["best_model"]["metrics"].get(primary_metric, 0)
            
            if primary_metric == 'accuracy' and best_score < 0.7:
                recommendations.append("üìä Score accuracy faible (<70%) - Envisagez plus de donn√©es ou feature engineering")
            elif primary_metric == 'r2' and best_score < 0.5:
                recommendations.append("üìà Score R¬≤ faible (<0.5) - Essayez des mod√®les non-lin√©aires ou plus de features")
            elif primary_metric == 'silhouette_score' and best_score < 0.3:
                recommendations.append("üîç Score silhouette faible (<0.3) - Testez diff√©rents nombres de clusters")
        
        # Variance
        if analysis["performance_summary"].get(primary_metric, {}).get('std', 0) > 0.15:
            recommendations.append("‚öñÔ∏è Forte variance entre mod√®les - Donn√©es peut-√™tre instables ou besoin de validation crois√©e")
        
        return recommendations
    
    @staticmethod
    def estimate_training_time(
        df: pd.DataFrame, 
        n_models: int, 
        task_type: str, 
        optimize_hp: bool, 
        n_features: int, 
        use_smote: bool
    ) -> int:
        """
        Estime le temps d'entra√Ænement en secondes avec algorithme intelligent.
        
        Args:
            df: DataFrame des donn√©es
            n_models: Nombre de mod√®les √† entra√Æner
            task_type: Type de t√¢che
            optimize_hp: Optimisation des hyperparam√®tres
            n_features: Nombre de features
            use_smote: Utilisation de SMOTE
            
        Returns:
            Temps estim√© en secondes
        """
        try:
            # Param√®tres de base
            base_time_per_model = TRAINING_CONSTANTS.get("BASE_TIME_PER_MODEL", 5)
            
            # Facteurs d'√©chelle
            scaling_factor_rows = max(1, len(df) / 1000)
            scaling_factor_features = max(1, n_features / 10)
            
            # Multiplicateurs
            hp_optimization_multiplier = 5 if optimize_hp else 1
            smote_multiplier = 1.5 if use_smote and task_type == 'classification' else 1
            
            # Complexit√© par t√¢che
            task_complexity = {
                'classification': 1.2,
                'regression': 1.0,
                'clustering': 1.5
            }.get(task_type, 1.0)
            
            # Calcul
            estimated_seconds = (
                base_time_per_model * 
                n_models * 
                scaling_factor_rows * 
                scaling_factor_features * 
                hp_optimization_multiplier * 
                smote_multiplier * 
                task_complexity
            )
            
            # Contraintes (entre 10s et 1h)
            estimated_seconds = max(10, min(estimated_seconds, 3600))
            
            logger.info(f"‚è±Ô∏è Temps estim√©: {estimated_seconds:.0f}s pour {n_models} mod√®les")
            
            return int(estimated_seconds)
            
        except Exception as e:
            logger.error(f"‚ùå Erreur estimation temps: {e}")
            return 60  # Fallback: 1 minute
    
    @staticmethod
    def format_training_time(seconds: int) -> str:
        """Formate le temps d'entra√Ænement en format lisible"""
        if seconds < 60:
            return f"{seconds}s"
        elif seconds < 3600:
            minutes = seconds // 60
            secs = seconds % 60
            return f"{minutes}min {secs}s" if secs > 0 else f"{minutes}min"
        else:
            hours = seconds // 3600
            minutes = (seconds % 3600) // 60
            return f"{hours}h{minutes:02d}"
    
    @staticmethod
    def validate_model_selection(
        selected_models: List[str], 
        task_type: str,
        min_models: int = 1,
        max_models: int = 10
    ) -> Dict[str, Any]:
        """Valide la s√©lection de mod√®les"""
        validation = {
            "is_valid": True,
            "issues": [],
            "warnings": []
        }
        
        # V√©rification nombre de mod√®les
        n_models = len(selected_models)
        
        if n_models < min_models:
            validation["is_valid"] = False
            validation["issues"].append(f"S√©lectionnez au moins {min_models} mod√®le(s)")
        
        if n_models > max_models:
            validation["warnings"].append(f"Nombre √©lev√© de mod√®les ({n_models}) - Temps d'entra√Ænement long")
        
        # V√©rification disponibilit√©
        available_models = TrainingHelpers.get_task_specific_models(task_type)
        invalid_models = [m for m in selected_models if m not in available_models]
        
        if invalid_models:
            validation["is_valid"] = False
            validation["issues"].append(f"Mod√®les invalides: {', '.join(invalid_models)}")
        
        # Recommandations
        if n_models == 1:
            validation["warnings"].append("Un seul mod√®le s√©lectionn√© - Impossible de comparer les performances")
        
        logger.info(f"‚úÖ Validation s√©lection mod√®les: {validation['is_valid']}, "
                   f"{n_models} mod√®les pour {task_type}")
        
        return validation
    
    @staticmethod
    def get_model_complexity_info(model_name: str, task_type: str) -> Dict[str, str]:
        """Retourne les informations de complexit√© d'un mod√®le"""
        try:
            model_catalog = MODEL_CATALOG.get(task_type, {})
            model_config = model_catalog.get(model_name, {})
            
            complexity = model_config.get('complexity', 'medium')
            training_speed = model_config.get('training_speed', 'medium')
            
            complexity_labels = {
                'low': 'D√©butant',
                'medium': 'Interm√©diaire',
                'high': 'Expert'
            }
            
            return {
                'complexity': complexity,
                'complexity_label': complexity_labels.get(complexity, 'Interm√©diaire'),
                'training_speed': training_speed,
                'category': model_config.get('category', 'Autres')
            }
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration info complexit√© {model_name}: {e}")
            return {
                'complexity': 'medium',
                'complexity_label': 'Interm√©diaire',
                'training_speed': 'medium',
                'category': 'Autres'
            }


# ============================================================================
# FONCTIONS UTILITAIRES STANDALONE
# ============================================================================

def filter_models_by_criteria(
    available_models: Dict[str, Dict],
    complexity_filter: List[str],
    speed_filter: str
) -> Dict[str, Dict]:
    """
    Filtre les mod√®les selon des crit√®res de complexit√© et vitesse.
    
    Args:
        available_models: Dictionnaire des mod√®les disponibles
        complexity_filter: Liste des niveaux de complexit√© accept√©s
        speed_filter: Filtre de vitesse ('Toutes', 'Rapide', 'Moyenne', 'Lente')
        
    Returns:
        Dictionnaire filtr√© des mod√®les
    """
    filtered = {}
    
    complexity_map = {
        'D√©butant': 'low',
        'Interm√©diaire': 'medium', 
        'Expert': 'high'
    }
    
    target_complexities = [complexity_map.get(c, 'medium') for c in complexity_filter]
    
    for name, config in available_models.items():
        # Filtre complexit√©
        model_complexity = config.get('complexity', 'medium')
        if model_complexity not in target_complexities:
            continue
        
        # Filtre vitesse
        if speed_filter != "Toutes":
            model_speed = config.get('training_speed', 'medium')
            if model_speed != speed_filter.lower():
                continue
        
        filtered[name] = config
    
    logger.info(f"‚úÖ Filtrage mod√®les: {len(filtered)}/{len(available_models)} mod√®les retenus")
    return filtered


def categorize_models(models: Dict[str, Dict]) -> Dict[str, List[tuple]]:
    """
    Organise les mod√®les par cat√©gorie.
    
    Args:
        models: Dictionnaire des mod√®les
        
    Returns:
        Dictionnaire avec cat√©gories comme cl√©s et listes de (nom, config) comme valeurs
    """
    categories = {}
    
    for model_name, config in models.items():
        category = config.get('category', 'üß† Autres')
        
        if category not in categories:
            categories[category] = []
        
        categories[category].append((model_name, config))
    
    # Tri alphab√©tique dans chaque cat√©gorie
    for category in categories:
        categories[category].sort(key=lambda x: x[0])
    
    logger.info(f"‚úÖ Mod√®les organis√©s en {len(categories)} cat√©gories")
    return categories


def get_recommended_models(
    task_type: str,
    dataset_size: int,
    n_features: int,
    has_imbalance: bool = False
) -> List[str]:
    """
    Recommande des mod√®les bas√©s sur les caract√©ristiques du dataset.
    
    Args:
        task_type: Type de t√¢che
        dataset_size: Nombre d'√©chantillons
        n_features: Nombre de features
        has_imbalance: Pr√©sence de d√©s√©quilibre (classification)
        
    Returns:
        Liste de noms de mod√®les recommand√©s
    """
    recommendations = []
    
    if task_type == 'classification':
        # Petit dataset
        if dataset_size < 1000:
            recommendations = ['LogisticRegression', 'SVM', 'KNN']
        # Dataset moyen
        elif dataset_size < 10000:
            recommendations = ['RandomForest', 'XGBoost', 'SVM']
        # Grand dataset
        else:
            recommendations = ['XGBoost', 'LightGBM', 'RandomForest']
        
        # Ajustement pour d√©s√©quilibre
        if has_imbalance:
            # Privil√©gier les mod√®les robustes au d√©s√©quilibre
            if 'RandomForest' not in recommendations:
                recommendations.insert(0, 'RandomForest')
            if 'XGBoost' not in recommendations:
                recommendations.insert(0, 'XGBoost')
    
    elif task_type == 'regression':
        if dataset_size < 1000:
            recommendations = ['LinearRegression', 'Ridge', 'Lasso']
        elif dataset_size < 10000:
            recommendations = ['RandomForest', 'GradientBoosting', 'Ridge']
        else:
            recommendations = ['XGBoost', 'LightGBM', 'RandomForest']
    
    elif task_type == 'clustering':
        if n_features > 10:
            recommendations = ['KMeans', 'DBSCAN', 'SpectralClustering']
        else:
            recommendations = ['KMeans', 'GaussianMixture', 'AgglomerativeClustering']
    
    # V√©rifier disponibilit√©
    available = TrainingHelpers.get_task_specific_models(task_type)
    recommendations = [m for m in recommendations if m in available]
    
    logger.info(f"‚úÖ {len(recommendations)} mod√®les recommand√©s pour {task_type} "
               f"(dataset: {dataset_size}, features: {n_features})")
    
    return recommendations[:5]  # Max 5 recommandations


# Export
__all__ = [
    'TrainingHelpers',
    'filter_models_by_criteria',
    'categorize_models',
    'get_recommended_models'
]