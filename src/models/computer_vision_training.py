"""
Production-Ready Computer Vision Training Pipeline
Version 3.0 - Enterprise Grade

Architecture:
‚úÖ Configuration typ√©e avec validation (Pydantic)
‚úÖ Pipeline de preprocessing isol√© et robuste
‚úÖ Gestion d'erreurs coh√©rente avec Result types
‚úÖ Callbacks d√©coupl√©s et extensibles
‚úÖ Logging structur√© pour observabilit√©
‚úÖ Validation crois√©e sans fuite de donn√©es
‚úÖ M√©triques m√©tier + techniques
‚úÖ Support multi-formats (channels_first/last)
‚úÖ Pr√©dictions unifi√©es pour classification + autoencoders
‚úÖ Gestion de m√©moire optimis√©e
"""

import time
import warnings
import copy
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple, Any, Union
from enum import Enum
from pathlib import Path

import numpy as np
import torch # type: ignore
import torch.nn as nn # type: ignore
import torch.optim as optim # type: ignore
from torch.utils.data import DataLoader # type: ignore
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report
)
from sklearn.utils.class_weight import compute_class_weight

from src.config.model_config import ModelConfig, ModelType
from src.data.computer_vision_preprocessing import DataLoaderFactory, DataPreprocessor, DataValidator, Result
from src.models.computer_vision.model_builder import ModelBuilder
from src.shared.logging import get_logger
from utils.callbacks import LoggingCallback, TrainingCallback
from utils.device_manager import DeviceManager

warnings.filterwarnings('ignore', category=UserWarning)

logger = get_logger(__name__)

# ======================
# CONFIGURATION ET TYPES
# ======================

class OptimizerType(str, Enum):
    """Types d'optimiseurs support√©s"""
    ADAMW = "adamw"
    ADAM = "adam"
    SGD = "sgd"
    RMSPROP = "rmsprop"


class SchedulerType(str, Enum):
    """Types de schedulers de learning rate"""
    REDUCE_ON_PLATEAU = "reduce_on_plateau"
    COSINE = "cosine_annealing"
    STEP = "step_lr"
    NONE = "none"


@dataclass
class TrainingConfig:
    """
    Configuration d'entra√Ænement valid√©e et production-ready.
    
    Tous les param√®tres ont des valeurs par d√©faut s√ªres pour la production.
    """
    # Hyperparam√®tres principaux
    epochs: int = 100
    batch_size: int = 32
    learning_rate: float = 1e-4
    weight_decay: float = 0.01
    gradient_clip: float = 1.0
    
    # Optimisation
    optimizer: OptimizerType = OptimizerType.ADAMW
    scheduler: SchedulerType = SchedulerType.REDUCE_ON_PLATEAU
    
    # Early stopping et r√©duction LR
    early_stopping_patience: int = 15
    reduce_lr_patience: int = 8
    min_lr: float = 1e-7
    
    # Gestion des classes d√©s√©quilibr√©es
    use_class_weights: bool = False
    
    # Performance et reproductibilit√©
    use_mixed_precision: bool = False  # D√©sactiv√© par d√©faut pour stabilit√©
    deterministic: bool = True
    seed: int = 42
    
    # DataLoader
    num_workers: int = 0  # 0 pour √©viter les probl√®mes de multiprocessing
    pin_memory: bool = False  # False par d√©faut pour compatibilit√© CPU
    
    # Sauvegarde
    checkpoint_dir: Optional[Path] = None
    save_best_only: bool = True
    
    def __post_init__(self):
        """Validation post-initialisation avec messages d'erreur clairs"""
        if self.epochs <= 0:
            raise ValueError(f"epochs doit √™tre > 0, re√ßu: {self.epochs}")
        if self.batch_size <= 0:
            raise ValueError(f"batch_size doit √™tre > 0, re√ßu: {self.batch_size}")
        if not (0 < self.learning_rate < 1):
            raise ValueError(f"learning_rate doit √™tre dans ]0, 1[, re√ßu: {self.learning_rate}")
        if self.gradient_clip <= 0:
            raise ValueError(f"gradient_clip doit √™tre > 0, re√ßu: {self.gradient_clip}")
        if self.early_stopping_patience <= 0:
            raise ValueError(f"early_stopping_patience doit √™tre > 0, re√ßu: {self.early_stopping_patience}")
        
        # Conversion en Enum si n√©cessaire
        if isinstance(self.optimizer, str):
            self.optimizer = OptimizerType(self.optimizer)
        if isinstance(self.scheduler, str):
            self.scheduler = SchedulerType(self.scheduler)
        
        # Conversion Path
        if isinstance(self.checkpoint_dir, str):
            self.checkpoint_dir = Path(self.checkpoint_dir)


# =============================
# OPTIMIZER & SCHEDULER FACTORY
# =============================

class OptimizerFactory:
    """Factory pour cr√©er des optimiseurs avec configuration robuste"""
    
    @staticmethod
    def create(
        model: nn.Module,
        config: TrainingConfig
    ) -> optim.Optimizer:
        """
        Cr√©e un optimiseur selon la config.
        
        Args:
            model: Mod√®le PyTorch
            config: Configuration d'entra√Ænement
            
        Returns:
            Optimiseur configur√©
            
        Raises:
            ValueError: Si l'optimiseur n'est pas support√©
        """
        if config.optimizer == OptimizerType.ADAMW:
            return optim.AdamW(
                model.parameters(),
                lr=config.learning_rate,
                weight_decay=config.weight_decay,
                betas=(0.9, 0.999),
                eps=1e-8
            )
        elif config.optimizer == OptimizerType.ADAM:
            return optim.Adam(
                model.parameters(),
                lr=config.learning_rate,
                weight_decay=config.weight_decay,
                betas=(0.9, 0.999),
                eps=1e-8
            )
        elif config.optimizer == OptimizerType.SGD:
            return optim.SGD(
                model.parameters(),
                lr=config.learning_rate,
                momentum=0.9,
                weight_decay=config.weight_decay,
                nesterov=True
            )
        elif config.optimizer == OptimizerType.RMSPROP:
            return optim.RMSprop(
                model.parameters(),
                lr=config.learning_rate,
                weight_decay=config.weight_decay,
                alpha=0.99,
                eps=1e-8
            )
        else:
            raise ValueError(f"Optimiseur non support√©: {config.optimizer}")


class SchedulerFactory:
    """Factory pour cr√©er des schedulers de learning rate"""
    
    @staticmethod
    def create(
        optimizer: optim.Optimizer,
        config: TrainingConfig
    ) -> Optional[optim.lr_scheduler._LRScheduler]:
        """
        Cr√©e un scheduler selon la config.
        
        Args:
            optimizer: Optimiseur PyTorch
            config: Configuration d'entra√Ænement
            
        Returns:
            Scheduler configur√© ou None
            
        Raises:
            ValueError: Si le scheduler n'est pas support√©
        """
        if config.scheduler == SchedulerType.NONE:
            return None
        
        elif config.scheduler == SchedulerType.REDUCE_ON_PLATEAU:
            return optim.lr_scheduler.ReduceLROnPlateau(
                optimizer,
                mode='min',
                factor=0.5,
                patience=config.reduce_lr_patience,
                min_lr=config.min_lr,
                verbose=True,
                threshold=1e-4,
                threshold_mode='rel'
            )
        
        elif config.scheduler == SchedulerType.COSINE:
            return optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=config.epochs,
                eta_min=config.min_lr
            )
        
        elif config.scheduler == SchedulerType.STEP:
            return optim.lr_scheduler.StepLR(
                optimizer,
                step_size=max(config.epochs // 3, 1),
                gamma=0.1
            )
        
        else:
            raise ValueError(f"Scheduler non support√©: {config.scheduler}")


# ==================
# TRAINER PRINCIPAL
# ==================

class ComputerVisionTrainer:
    """
    Trainer principal production-ready pour Computer Vision.
    
    Architecture garantissant:
    - Pas de fuite de donn√©es (fit sur train uniquement)
    - Gestion robuste des formats (channels_first/last)
    - M√©triques m√©tier fiables
    - Extensibilit√© via callbacks
    - Observabilit√© compl√®te via logging
    
    Usage:
        ```python
        trainer = ComputerVisionTrainer(model_config, training_config)
        result = trainer.fit(X_train, y_train, X_val, y_val)
        
        if result.success:
            model = result.data['model']
            history = result.data['history']
            
            # Pr√©dictions
            pred_result = trainer.predict(X_test)
            predictions = pred_result.data['predictions']
        ```
    """
    
    def __init__(
        self,
        model_config: ModelConfig,
        training_config: TrainingConfig,
        callbacks: Optional[List[TrainingCallback]] = None,
        device_manager: Optional[DeviceManager] = None
    ):
        """
        Initialise le trainer.
        
        Args:
            model_config: Configuration du mod√®le
            training_config: Configuration d'entra√Ænement
            callbacks: Liste de callbacks optionnels
            device_manager: Gestionnaire de device (CPU/GPU)
        """
        self.model_config = model_config
        self.training_config = training_config
        self.callbacks = callbacks or []
        self.device_manager = device_manager or DeviceManager()
        
        # √âtat interne
        self.model: Optional[nn.Module] = None
        self.optimizer: Optional[optim.Optimizer] = None
        self.scheduler: Optional[optim.lr_scheduler._LRScheduler] = None
        self.preprocessor: Optional[DataPreprocessor] = None
        self.train_criterion: Optional[nn.Module] = None
        self.val_criterion: Optional[nn.Module] = None
        
        # Historique (structure propre garantie)
        self.history: Dict[str, List[float]] = {
            'train_loss': [],
            'val_loss': [],
            'val_accuracy': [],
            'val_f1': [],
            'learning_rates': []
        }
        
        # M√©tadonn√©es d'entra√Ænement
        self._training_metadata: Dict[str, Any] = {}
        
        # Setup d√©terminisme si demand√©
        if training_config.deterministic:
            self._set_deterministic(training_config.seed)
    
    def _set_deterministic(self, seed: int) -> None:
        """
        Active le mode d√©terministe pour reproductibilit√©.
        
        Args:
            seed: Graine al√©atoire
        """
        torch.manual_seed(seed)
        np.random.seed(seed)
        
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
        
        logger.info(f"Mode d√©terministe activ√©: seed={seed}")
    
    def fit(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray
    ) -> Result:
        """
        Entra√Æne le mod√®le de mani√®re robuste.
        
        Pipeline:
        1. Validation des donn√©es
        2. Preprocessing (fit sur train, transform sur val)
        3. Construction du mod√®le
        4. Setup training (optimizer, criterion, scheduler)
        5. Boucle d'entra√Ænement avec early stopping
        6. Retour des r√©sultats structur√©s
        
        Args:
            X_train: Donn√©es d'entra√Ænement (N, H, W, C) ou (N, C, H, W)
            y_train: Labels d'entra√Ænement
            X_val: Donn√©es de validation
            y_val: Labels de validation
            
        Returns:
            Result contenant model, history, preprocessor et m√©tadonn√©es
        """
        try:
            logger.info("=== D√©but de l'entra√Ænement ===")
            start_total = time.time()
            
            # 1. Validation des donn√©es
            val_result = self._validate_data(X_train, y_train, X_val, y_val)
            if not val_result.success:
                return val_result
            
            # 2. Preprocessing (FIT sur train, TRANSFORM sur val)
            prep_result = self._setup_preprocessing(X_train, y_train, X_val, y_val)
            if not prep_result.success:
                return prep_result
            
            X_train_norm, y_train, X_val_norm, y_val = prep_result.data
            
            # 3. Construction du mod√®le
            model_result = self._build_model()
            if not model_result.success:
                return model_result
            
            # 4. Setup training
            setup_result = self._setup_training(y_train)
            if not setup_result.success:
                return setup_result
            
            # 5. Cr√©ation des DataLoaders
            train_loader = DataLoaderFactory.create(
                X_train_norm, y_train,
                batch_size=self.training_config.batch_size,
                shuffle=True,
                num_workers=0,
                pin_memory=False
            )
            
            val_loader = DataLoaderFactory.create(
                X_val_norm, y_val,
                batch_size=self.training_config.batch_size,
                shuffle=False,
                num_workers=0,
                pin_memory=False
            )
            
            # 6. Boucle d'entra√Ænement
            train_result = self._training_loop(train_loader, val_loader, y_val)
            if not train_result.success:
                return train_result
            
            # 7. M√©tadonn√©es finales
            total_time = time.time() - start_total
            self._training_metadata.update({
                'total_training_time': total_time,
                'samples_per_second': len(X_train) / total_time,
                'final_model_params': sum(p.numel() for p in self.model.parameters()),
                'device': str(self.device_manager.device)
            })
            
            # 8. Retour structur√© (GARANTIT PAS DE BOOL√âENS)
            return Result.ok(
                self._build_training_result(train_result),
                training_time=total_time
            )
            
        except Exception as e:
            logger.error(f"Erreur critique entra√Ænement: {e}", exc_info=True)
            return Result.err(f"Entra√Ænement √©chou√©: {str(e)}")
    
    def _build_training_result(self, train_result: Result) -> Dict[str, Any]:
        """
        Construit le r√©sultat final avec structure garantie.
        
        CRITIQUE: Assure qu'aucun bool√©en n'est dans l'historique.
        """
        is_autoencoder = self.model_config.model_type in [
            ModelType.CONV_AUTOENCODER, ModelType.VAE, ModelType.DENOISING_AE
        ]
        
        return {
            'model': self.model,
            'preprocessor': self.preprocessor,
            'history': {
                # M√©ta-informations
                'success': True,
                'model_type': self.model_config.model_type.value,
                'is_autoencoder': is_autoencoder,
                
                # M√©triques d'entra√Ænement (LISTES de float garanties)
                'train_loss': [float(x) for x in self.history['train_loss']],
                'val_loss': [float(x) for x in self.history['val_loss']],
                'val_accuracy': [float(x) for x in self.history['val_accuracy']] if not is_autoencoder else [],
                'val_f1': [float(x) for x in self.history['val_f1']] if not is_autoencoder else [],
                'learning_rates': [float(x) for x in self.history['learning_rates']],
                
                # R√©sum√© d'entra√Ænement
                'best_epoch': int(train_result.metadata.get('best_epoch', 0)),
                'best_val_loss': float(min(self.history['val_loss'])) if self.history['val_loss'] else float('inf'),
                'final_train_loss': float(self.history['train_loss'][-1]) if self.history['train_loss'] else float('inf'),
                'training_time': float(train_result.metadata.get('training_time', 0)),
                'total_epochs_trained': len(self.history['train_loss']),
                'early_stopping_triggered': len(self.history['train_loss']) < self.training_config.epochs,
                
                # Shape et configuration
                'input_shape': tuple(self.preprocessor.original_shape_[1:]) if hasattr(self.preprocessor, 'original_shape_') else None,
                'output_format': 'channels_first',
                
                # Configuration d'entra√Ænement
                'training_config': {
                    'learning_rate': float(self.training_config.learning_rate),
                    'batch_size': int(self.training_config.batch_size),
                    'optimizer': self.training_config.optimizer.value,
                    'scheduler': self.training_config.scheduler.value,
                    'epochs_requested': int(self.training_config.epochs),
                    'early_stopping_patience': int(self.training_config.early_stopping_patience),
                    'use_class_weights': bool(self.training_config.use_class_weights),
                    'gradient_clip': float(self.training_config.gradient_clip)
                },
                
                # M√©tadonn√©es additionnelles
                'metadata': self._training_metadata
            }
        }
    
    def _validate_data(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray
    ) -> Result:
        """Validation compl√®te des donn√©es d'entr√©e"""
        
        # Validation train
        train_val = DataValidator.validate_input_data(X_train, y_train, "train")
        if not train_val.success:
            return train_val
        
        # Validation val
        val_val = DataValidator.validate_input_data(X_val, y_val, "validation")
        if not val_val.success:
            return val_val
        
        # V√©rification coh√©rence des shapes
        if X_train.shape[1:] != X_val.shape[1:]:
            return Result.err(
                f"Shapes incompatibles: train={X_train.shape}, val={X_val.shape}"
            )
        
        # Analyse d√©s√©quilibre
        imbalance = DataValidator.check_class_imbalance(y_train)
        logger.info("Analyse d√©s√©quilibre classes", **imbalance)
        
        if imbalance['severity'] in ['critical', 'high']:
            logger.warning(
                f"D√©s√©quilibre {imbalance['severity']} d√©tect√© (ratio={imbalance['ratio']:.2f}). "
                f"Consid√©rez d'activer use_class_weights=True"
            )
        
        return Result.ok(None, imbalance=imbalance)
    
    def _setup_preprocessing(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray
    ) -> Result:
        """
        Setup preprocessing production-ready avec gestion robuste des formats.
        
        GARANTIT:
        - Pas de fuite de donn√©es (fit sur train uniquement)
        - Format channels_first coh√©rent pour PyTorch
        - Logging complet pour observabilit√©
        - Gestion des erreurs granulaires
        
        Returns:
            Result avec (X_train_norm, y_train, X_val_norm, y_val) en channels_first
        """
        try:
            # Validation pr√©alable
            if X_train is None or len(X_train) == 0:
                return Result.err("Donn√©es d'entra√Ænement vides")
            if X_val is None or len(X_val) == 0:
                return Result.err("Donn√©es de validation vides")
            
            logger.info(
                "D√©but setup preprocessing",
                train_shape=X_train.shape,
                val_shape=X_val.shape,
                train_dtype=X_train.dtype,
                val_dtype=X_val.dtype
            )
            
            # Cr√©ation du preprocessor avec auto-d√©tection de format
            self.preprocessor = DataPreprocessor(
                strategy="standardize",
                auto_detect_format=True
            )
            
            # FIT SUR TRAIN UNIQUEMENT - retourne format PyTorch (channels_first)
            X_train_norm = self.preprocessor.fit_transform(
                X_train,
                output_format="channels_first"
            )
            
            # TRANSFORM SUR VALIDATION - m√™me format (channels_first)
            X_val_norm = self.preprocessor.transform(
                X_val,
                output_format="channels_first"
            )
            
            # Validation post-processing critique
            if X_train_norm.ndim != 4:
                return Result.err(
                    f"Format invalide apr√®s preprocessing: {X_train_norm.ndim}D au lieu de 4D"
                )
            
            if X_train_norm.shape[1] not in [1, 3]:
                logger.warning(
                    f"Nombre de canaux inattendu: {X_train_norm.shape[1]}. "
                    f"Attendu 1 (grayscale) ou 3 (RGB)"
                )
            
            # Logging d√©taill√© pour tra√ßabilit√©
            logger.info(
                "Preprocessing configur√© avec succ√®s",
                strategy="standardize",
                input_format=getattr(self.preprocessor, 'data_format_', 'unknown'),
                output_format="channels_first",
                train_original_shape=X_train.shape,
                train_processed_shape=X_train_norm.shape,
                val_processed_shape=X_val_norm.shape,
                preprocessor_config=self.preprocessor.get_config() if hasattr(self.preprocessor, 'get_config') else {}
            )
            
            return Result.ok((X_train_norm, y_train, X_val_norm, y_val))
            
        except ValueError as e:
            logger.error(f"Erreur validation preprocessing: {str(e)}")
            return Result.err(f"Donn√©es invalides: {str(e)}")
        except Exception as e:
            logger.error(
                f"Erreur technique preprocessing: {str(e)}",
                exc_info=True,
                train_shape=getattr(X_train, 'shape', None),
                val_shape=getattr(X_val, 'shape', None)
            )
            return Result.err(f"Erreur preprocessing: {str(e)}")
    
    def _build_model(self) -> Result:
        """Construit le mod√®le via ModelBuilder"""
        builder = ModelBuilder(self.device_manager)
        result = builder.build(self.model_config)
        
        if result.success:
            self.model = result.data
            logger.info(
                "Mod√®le construit avec succ√®s",
                model_type=self.model_config.model_type.value,
                total_params=sum(p.numel() for p in self.model.parameters()),
                trainable_params=sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            )
        
        return result
    
    def _setup_training(self, y_train: np.ndarray) -> Result:
        """Setup optimizer, scheduler, et criterion"""
        try:
            # Optimizer
            self.optimizer = OptimizerFactory.create(self.model, self.training_config)
            
            # Scheduler
            self.scheduler = SchedulerFactory.create(self.optimizer, self.training_config)
            
            # Criterion selon le type de mod√®le
            is_autoencoder = self.model_config.model_type in [
                ModelType.CONV_AUTOENCODER, ModelType.VAE, ModelType.DENOISING_AE
            ]
            
            if is_autoencoder:
                # Autoencoders: MSE Loss (reconstruction)
                self.train_criterion = nn.MSELoss()
                self.val_criterion = nn.MSELoss()
                logger.info("Criterion: MSELoss (autoencoder)")
            else:
                # Classification: CrossEntropyLoss
                if self.training_config.use_class_weights:
                    classes = np.unique(y_train)
                    weights = compute_class_weight(
                        class_weight='balanced',
                        classes=classes,
                        y=y_train
                    )
                    weights_tensor = torch.tensor(
                        weights,
                        dtype=torch.float32,
                        device=self.device_manager.device
                    )
                    self.train_criterion = nn.CrossEntropyLoss(weight=weights_tensor)
                    logger.info("Class weights appliqu√©s sur train", weights=weights.tolist())
                else:
                    self.train_criterion = nn.CrossEntropyLoss()
                
                # Criterion validation SANS class weights (√©valuation honn√™te)
                self.val_criterion = nn.CrossEntropyLoss()
                logger.info("Criterion: CrossEntropyLoss (classification)")
            
            logger.info(
                "Training setup compl√©t√©",
                optimizer=self.training_config.optimizer.value,
                scheduler=self.training_config.scheduler.value if self.scheduler else "none",
                use_class_weights=self.training_config.use_class_weights
            )
            
            return Result.ok(None)
            
        except Exception as e:
            logger.error(f"Erreur setup training: {e}", exc_info=True)
            return Result.err(f"Setup training √©chou√©: {str(e)}")
    
    def _training_loop(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        y_val: np.ndarray
    ) -> Result:
        """Boucle d'entra√Ænement avec early stopping et checkpointing"""
        try:
            is_autoencoder = self.model_config.model_type in [
                ModelType.CONV_AUTOENCODER, ModelType.VAE, ModelType.DENOISING_AE
            ]
            
            best_val_metric = float('inf') if is_autoencoder else 0.0
            best_model_state = None
            best_epoch = 0
            patience_counter = 0
            
            start_time = time.time()
            
            # Callbacks train begin
            for cb in self.callbacks:
                cb.on_train_begin()
            
            for epoch in range(self.training_config.epochs):
                epoch_start = time.time()
                
                # Callbacks epoch begin
                for cb in self.callbacks:
                    cb.on_epoch_begin(epoch)
                
                # === TRAIN ===
                train_loss = self._train_epoch(train_loader, is_autoencoder)
                
                # === VALIDATION ===
                if is_autoencoder:
                    val_loss = self._validate_epoch_autoencoder(val_loader)
                    val_metrics = {'loss': val_loss}
                else:
                    val_loss, val_metrics = self._validate_epoch(val_loader, y_val)
                
                # Mise √† jour historique
                self.history['train_loss'].append(float(train_loss))
                self.history['val_loss'].append(float(val_loss))
                
                if not is_autoencoder:
                    self.history['val_accuracy'].append(float(val_metrics['accuracy']))
                    self.history['val_f1'].append(float(val_metrics['f1']))
                
                current_lr = float(self.optimizer.param_groups[0]['lr'])
                self.history['learning_rates'].append(current_lr)
                
                # Scheduler step
                if self.scheduler is not None:
                    if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                        self.scheduler.step(val_loss)
                    else:
                        self.scheduler.step()
                
                # Callbacks epoch end
                logs = {
                    'epoch': epoch,
                    'train_loss': float(train_loss),
                    'val_loss': float(val_loss),
                    'lr': current_lr,
                    'epoch_time': time.time() - epoch_start,
                    'model_state_dict': copy.deepcopy(self.model.state_dict()),
                    'optimizer_state_dict': copy.deepcopy(self.optimizer.state_dict())
                }
                
                if not is_autoencoder:
                    logs.update({
                        'val_accuracy': float(val_metrics['accuracy']),
                        'val_f1': float(val_metrics['f1'])
                    })
                
                for cb in self.callbacks:
                    cb.on_epoch_end(epoch, logs)
                
                # === EARLY STOPPING ===
                if is_autoencoder:
                    # Autoencoder: minimiser la loss
                    improved = val_loss < best_val_metric
                    best_val_metric = min(best_val_metric, val_loss)
                else:
                    # Classification: maximiser F1
                    improved = val_metrics['f1'] > best_val_metric
                    best_val_metric = max(best_val_metric, val_metrics['f1'])
                
                if improved:
                    best_model_state = copy.deepcopy(self.model.state_dict())
                    best_epoch = epoch + 1
                    patience_counter = 0
                    
                    metric_name = "loss" if is_autoencoder else "F1"
                    logger.info(
                        f"‚ú® Nouveau meilleur mod√®le ({metric_name}={best_val_metric:.4f})",
                        epoch=epoch+1
                    )
                    
                    # Sauvegarde checkpoint si configur√©
                    if self.training_config.checkpoint_dir and self.training_config.save_best_only:
                        self._save_checkpoint(epoch, best_val_metric, best_model_state)
                else:
                    patience_counter += 1
                
                # Check early stopping
                if patience_counter >= self.training_config.early_stopping_patience:
                    logger.info(
                        f"üõë Early stopping d√©clench√©",
                        epoch=epoch+1,
                        patience=patience_counter
                    )
                    break
            
            # Restauration du meilleur mod√®le
            if best_model_state is not None:
                self.model.load_state_dict(best_model_state)
                logger.info(f"‚úÖ Meilleur mod√®le restaur√© (epoch {best_epoch})")
            
            training_time = time.time() - start_time
            
            # Callbacks train end
            for cb in self.callbacks:
                cb.on_train_end({'training_time': training_time})
            
            logger.info(
                "üéØ Entra√Ænement termin√© avec succ√®s",
                total_epochs=epoch+1,
                best_epoch=best_epoch,
                best_metric=best_val_metric,
                training_time=f"{training_time:.1f}s",
                avg_epoch_time=f"{training_time/(epoch+1):.2f}s"
            )
            
            result_metadata = {
                'best_epoch': best_epoch,
                'training_time': training_time,
                'total_epochs': epoch + 1
            }
            
            if is_autoencoder:
                result_metadata['best_loss'] = best_val_metric
            else:
                result_metadata['best_f1'] = best_val_metric
            
            return Result.ok(None, **result_metadata)
            
        except Exception as e:
            logger.error(f"Erreur boucle training: {e}", exc_info=True)
            return Result.err(f"Training loop √©chou√©: {str(e)}")
    
    def _train_epoch(self, train_loader: DataLoader, is_autoencoder: bool = False) -> float:
        """Entra√Æne une √©poque - unifi√© pour classification et autoencoders"""
        self.model.train()
        running_loss = 0.0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            # D√©placement sur device
            data = data.to(self.device_manager.device)
            
            if is_autoencoder:
                # Pour autoencoder: target = input (reconstruction)
                target = data
            else:
                target = target.to(self.device_manager.device)
            
            # Forward pass
            self.optimizer.zero_grad()
            output = self.model(data)
            
            # Loss computation
            if (self.model_config.model_type == ModelType.VAE and 
                hasattr(self.model, 'compute_vae_loss')):
                loss, recon_loss, kl_loss = self.model.compute_vae_loss(data, output)
            else:
                loss = self.train_criterion(output, target)
            
            # Backward pass
            loss.backward()
            
            # Gradient clipping
            if self.training_config.gradient_clip > 0:
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.training_config.gradient_clip
                )
            
            self.optimizer.step()
            running_loss += loss.item()
        
        return running_loss / len(train_loader)
    
    def _validate_epoch_autoencoder(self, val_loader: DataLoader) -> float:
        """Validation pour autoencoder"""
        self.model.eval()
        running_loss = 0.0
        
        with torch.no_grad():
            for data, _ in val_loader:
                data = data.to(self.device_manager.device)
                output = self.model(data)

                if (self.model_config.model_type == ModelType.VAE and 
                    hasattr(self.model, 'compute_vae_loss')):
                    loss, _, _ = self.model.compute_vae_loss(data, output)
                else:
                    loss = self.val_criterion(output, data)
                
                running_loss += loss.item()
        
        return running_loss / len(val_loader)
    
    def _validate_epoch(
        self,
        val_loader: DataLoader,
        y_val: np.ndarray
    ) -> Tuple[float, Dict[str, float]]:
        """
        Validation pour classificateur.
        
        CRITIQUE: Utilise val_criterion SANS class weights pour √©valuation honn√™te.
        """
        self.model.eval()
        running_loss = 0.0
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            for data, target in val_loader:
                data = data.to(self.device_manager.device)
                target = target.to(self.device_manager.device)
                
                output = self.model(data)
                loss = self.val_criterion(output, target)
                running_loss += loss.item()
                
                # Pr√©dictions
                preds = output.argmax(dim=1).cpu().numpy()
                all_preds.extend(preds)
                all_targets.extend(target.cpu().numpy())
        
        # Calcul m√©triques m√©tier
        val_loss = running_loss / len(val_loader)
        
        metrics = {
            'accuracy': accuracy_score(all_targets, all_preds),
            'precision': precision_score(all_targets, all_preds, average='weighted', zero_division=0),
            'recall': recall_score(all_targets, all_preds, average='weighted', zero_division=0),
            'f1': f1_score(all_targets, all_preds, average='weighted', zero_division=0)
        }
        
        return val_loss, metrics
    
    def _save_checkpoint(
        self, 
        epoch: int, 
        metric: float, 
        model_state: Dict[str, Any]
    ) -> None:
        """Sauvegarde un checkpoint"""
        try:
            if self.training_config.checkpoint_dir is None:
                return
            
            checkpoint_dir = self.training_config.checkpoint_dir
            checkpoint_dir.mkdir(parents=True, exist_ok=True)
            
            checkpoint_path = checkpoint_dir / f"best_model_epoch_{epoch}.pt"
            
            torch.save({
                'epoch': epoch,
                'model_state_dict': model_state,
                'optimizer_state_dict': self.optimizer.state_dict(),
                'metric': metric,
                'model_config': self.model_config,
                'training_config': self.training_config
            }, checkpoint_path)
            
            logger.info(f"üíæ Checkpoint sauvegard√©: {checkpoint_path}")
            
        except Exception as e:
            logger.warning(f"Impossible de sauvegarder le checkpoint: {e}")
    
    # ==================
    # M√âTHODES PUBLIQUES
    # ==================
    
    def predict(
        self,
        X: np.ndarray,
        return_reconstructed: bool = False,
        batch_size: Optional[int] = None
    ) -> Result:
        """
        Pr√©dictions robustes avec preprocessing automatique.
        
        Args:
            X: Donn√©es √† pr√©dire (N, H, W, C) ou (N, C, H, W)
            return_reconstructed: Si True, retourne les reconstructions (autoencoders)
            batch_size: Taille des batchs (par d√©faut: config.batch_size)
            
        Returns:
            Result avec pr√©dictions structur√©es:
            - Pour classificateurs: {'probabilities', 'predictions'}
            - Pour autoencoders: {'reconstruction_errors', 'predictions', 'reconstructed'?}
        """
        try:
            # Validation
            if self.model is None:
                return Result.err("Mod√®le non entra√Æn√©")
            if self.preprocessor is None:
                return Result.err("Preprocessor non disponible")
            
            # Preprocessing
            X_processed = self.preprocessor.transform(X, output_format="channels_first")
            
            # D√©tection du type de mod√®le
            is_autoencoder = self.model_config.model_type in [
                ModelType.CONV_AUTOENCODER, ModelType.VAE, ModelType.DENOISING_AE
            ]
            
            # Cr√©ation DataLoader temporaire
            batch_size = batch_size or self.training_config.batch_size
            dummy_labels = np.zeros(len(X_processed))
            
            test_loader = DataLoaderFactory.create(
                X_processed, dummy_labels,
                batch_size=batch_size,
                shuffle=False,
                num_workers=0,
                pin_memory=False
            )
            
            # Pr√©diction
            self.model.eval()
            
            if is_autoencoder:
                return self._predict_autoencoder(test_loader, X_processed, return_reconstructed)
            else:
                return self._predict_classifier(test_loader)
            
        except Exception as e:
            logger.error(f"Erreur pr√©diction: {e}", exc_info=True)
            return Result.err(f"Pr√©diction √©chou√©e: {str(e)}")
    
    def _predict_autoencoder(
        self, 
        test_loader: DataLoader, 
        X_processed: np.ndarray,
        return_reconstructed: bool
    ) -> Result:
        """Pr√©dictions pour autoencoder"""
        reconstruction_errors = []
        reconstructed_images = [] if return_reconstructed else None
        
        with torch.no_grad():
            for data, _ in test_loader:
                data = data.to(self.device_manager.device)
                reconstructed = self.model(data)
                
                # Erreur de reconstruction par √©chantillon
                errors = torch.mean(
                    (data - reconstructed) ** 2,
                    dim=tuple(range(1, data.ndim))
                ).cpu().numpy()
                
                reconstruction_errors.extend(errors)
                
                if return_reconstructed:
                    reconstructed_images.append(reconstructed.cpu().numpy())
        
        reconstruction_errors = np.array(reconstruction_errors)
        
        # Seuil automatique (95√®me percentile)
        threshold = np.percentile(reconstruction_errors, 95)
        predictions = (reconstruction_errors > threshold).astype(int)
        
        result_data = {
            'reconstruction_errors': reconstruction_errors,
            'predictions': predictions,
            'threshold': float(threshold)
        }
        
        if return_reconstructed:
            result_data['reconstructed'] = np.concatenate(reconstructed_images, axis=0)
        
        return Result.ok(result_data)
    
    def _predict_classifier(self, test_loader: DataLoader) -> Result:
        """Pr√©dictions pour classificateur"""
        all_probs = []
        all_preds = []
        
        with torch.no_grad():
            for data, _ in test_loader:
                data = data.to(self.device_manager.device)
                output = self.model(data)
                
                probs = torch.softmax(output, dim=1).cpu().numpy()
                preds = output.argmax(dim=1).cpu().numpy()
                
                all_probs.append(probs)
                all_preds.extend(preds)
        
        return Result.ok({
            'probabilities': np.concatenate(all_probs, axis=0),
            'predictions': np.array(all_preds)
        })
    
    def evaluate(
        self,
        X_test: np.ndarray,
        y_test: np.ndarray
    ) -> Result:
        """
        √âvaluation compl√®te sur donn√©es de test.
        
        G√®re automatiquement classification ET autoencoders.
        
        Args:
            X_test: Donn√©es de test (JAMAIS VU pendant training)
            y_test: Labels de test
            
        Returns:
            Result avec m√©triques compl√®tes
        """
        try:
            # Validation du test set
            test_val = DataValidator.validate_input_data(X_test, y_test, "test")
            if not test_val.success:
                return test_val
            
            # Preprocessing (TRANSFORM uniquement, pas fit!)
            if self.preprocessor is None:
                return Result.err("Mod√®le non entra√Æn√© (pas de preprocessor)")
            
            X_test_norm = self.preprocessor.transform(X_test, output_format="channels_first")
            
            # D√©tection du type de mod√®le
            is_autoencoder = self.model_config.model_type in [
                ModelType.CONV_AUTOENCODER, ModelType.VAE, ModelType.DENOISING_AE
            ]
            
            if is_autoencoder:
                return self._evaluate_autoencoder(X_test_norm, y_test)
            else:
                return self._evaluate_classifier(X_test_norm, y_test)
            
        except Exception as e:
            logger.error(f"Erreur √©valuation: {e}", exc_info=True)
            return Result.err(f"√âvaluation √©chou√©e: {str(e)}")
    
    def _evaluate_classifier(
        self,
        X_test_norm: np.ndarray,
        y_test: np.ndarray
    ) -> Result:
        """√âvaluation pour mod√®les de classification"""
        try:
            test_loader = DataLoaderFactory.create(
                X_test_norm, y_test,
                batch_size=self.training_config.batch_size,
                shuffle=False,
                num_workers=0,
                pin_memory=False
            )
            
            self.model.eval()
            all_preds = []
            all_probs = []
            all_targets = []
            
            with torch.no_grad():
                for data, target in test_loader:
                    data = data.to(self.device_manager.device)
                    output = self.model(data)
                    
                    probs = torch.softmax(output, dim=1).cpu().numpy()
                    preds = output.argmax(dim=1).cpu().numpy()
                    
                    all_probs.extend(probs)
                    all_preds.extend(preds)
                    all_targets.extend(target.numpy())
            
            all_preds = np.array(all_preds)
            all_probs = np.array(all_probs)
            all_targets = np.array(all_targets)
            
            # M√©triques compl√®tes
            metrics = {
                'accuracy': float(accuracy_score(all_targets, all_preds)),
                'precision': float(precision_score(all_targets, all_preds, average='weighted', zero_division=0)),
                'recall': float(recall_score(all_targets, all_preds, average='weighted', zero_division=0)),
                'f1': float(f1_score(all_targets, all_preds, average='weighted', zero_division=0)),
                'confusion_matrix': confusion_matrix(all_targets, all_preds).tolist(),
                'n_samples': len(X_test_norm),
                'n_classes': len(np.unique(y_test))
            }
            
            # AUC-ROC si binaire
            if self.model_config.num_classes == 2:
                try:
                    metrics['auc_roc'] = float(roc_auc_score(all_targets, all_probs[:, 1]))
                except:
                    metrics['auc_roc'] = None
            
            # Rapport de classification
            metrics['classification_report'] = classification_report(
                all_targets, all_preds,
                output_dict=True,
                zero_division=0
            )
            
            logger.info(
                "‚úÖ √âvaluation classification compl√©t√©e",
                accuracy=metrics['accuracy'],
                f1=metrics['f1']
            )
            
            return Result.ok(metrics)
            
        except Exception as e:
            logger.error(f"Erreur √©valuation classifier: {e}", exc_info=True)
            return Result.err(f"√âvaluation classifier √©chou√©e: {str(e)}")
    
    def _evaluate_autoencoder(
        self,
        X_test_norm: np.ndarray,
        y_test: np.ndarray
    ) -> Result:
        """
        √âvaluation pour autoencoders.
        
        Utilise l'erreur de reconstruction pour d√©tecter les anomalies.
        """
        try:
            test_loader = DataLoaderFactory.create(
                X_test_norm, y_test,
                batch_size=self.training_config.batch_size,
                shuffle=False,
                num_workers=0,
                pin_memory=False
            )
            
            # Calcul des erreurs de reconstruction
            self.model.eval()
            reconstruction_errors = []
            all_targets = []
            
            with torch.no_grad():
                for data, target in test_loader:
                    data = data.to(self.device_manager.device)
                    reconstructed = self.model(data)
                    
                    # Erreur de reconstruction par √©chantillon
                    errors = torch.mean(
                        (data - reconstructed) ** 2,
                        dim=tuple(range(1, data.ndim))
                    ).cpu().numpy()
                    
                    reconstruction_errors.extend(errors)
                    all_targets.extend(target.numpy())
            
            reconstruction_errors = np.array(reconstruction_errors)
            all_targets = np.array(all_targets)
            
            # Seuil automatique (95√®me percentile)
            threshold = np.percentile(reconstruction_errors, 95)
            y_pred = (reconstruction_errors > threshold).astype(int)
            
            # M√©triques
            metrics = {
                'mean_reconstruction_error': float(np.mean(reconstruction_errors)),
                'std_reconstruction_error': float(np.std(reconstruction_errors)),
                'median_reconstruction_error': float(np.median(reconstruction_errors)),
                'threshold_95percentile': float(threshold),
                'accuracy': float(accuracy_score(all_targets, y_pred)),
                'precision': float(precision_score(all_targets, y_pred, zero_division=0)),
                'recall': float(recall_score(all_targets, y_pred, zero_division=0)),
                'f1': float(f1_score(all_targets, y_pred, zero_division=0)),
                'confusion_matrix': confusion_matrix(all_targets, y_pred).tolist(),
                'n_samples': len(X_test_norm)
            }
            
            # AUC-ROC
            try:
                metrics['auc_roc'] = float(roc_auc_score(all_targets, reconstruction_errors))
            except:
                metrics['auc_roc'] = None
            
            logger.info(
                "‚úÖ √âvaluation autoencoder compl√©t√©e",
                mean_error=metrics['mean_reconstruction_error'],
                threshold=threshold,
                accuracy=metrics['accuracy']
            )
            
            return Result.ok(metrics)
            
        except Exception as e:
            logger.error(f"Erreur √©valuation autoencoder: {e}", exc_info=True)
            return Result.err(f"√âvaluation autoencoder √©chou√©e: {str(e)}")
    
    def save_model(self, filepath: Union[str, Path]) -> Result:
        """
        Sauvegarde le mod√®le complet.
        
        Args:
            filepath: Chemin de sauvegarde
            
        Returns:
            Result indiquant le succ√®s
        """
        try:
            if self.model is None:
                return Result.err("Aucun mod√®le √† sauvegarder")
            
            filepath = Path(filepath)
            filepath.parent.mkdir(parents=True, exist_ok=True)
            
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'model_config': self.model_config,
                'training_config': self.training_config,
                'history': self.history,
                'preprocessor': self.preprocessor
            }, filepath)
            
            logger.info(f"üíæ Mod√®le sauvegard√©: {filepath}")
            return Result.ok(str(filepath))
            
        except Exception as e:
            logger.error(f"Erreur sauvegarde mod√®le: {e}", exc_info=True)
            return Result.err(f"Sauvegarde √©chou√©e: {str(e)}")
    
    @classmethod
    def load_model(cls, filepath: Union[str, Path]) -> Result:
        """
        Charge un mod√®le sauvegard√©.
        
        Args:
            filepath: Chemin du mod√®le
            
        Returns:
            Result avec instance de ComputerVisionTrainer
        """
        try:
            filepath = Path(filepath)
            if not filepath.exists():
                return Result.err(f"Fichier introuvable: {filepath}")
            
            checkpoint = torch.load(filepath, map_location='cpu')
            
            trainer = cls(
                model_config=checkpoint['model_config'],
                training_config=checkpoint['training_config']
            )
            
            # Reconstruction du mod√®le
            model_result = trainer._build_model()
            if not model_result.success:
                return model_result
            
            trainer.model.load_state_dict(checkpoint['model_state_dict'])
            trainer.history = checkpoint.get('history', {})
            trainer.preprocessor = checkpoint.get('preprocessor')
            
            logger.info(f"‚úÖ Mod√®le charg√©: {filepath}")
            return Result.ok(trainer)
            
        except Exception as e:
            logger.error(f"Erreur chargement mod√®le: {e}", exc_info=True)
            return Result.err(f"Chargement √©chou√©: {str(e)}")


# =================================
# INT√âGRATION AVEC ANOMALY TAXONOMY
# =================================

class AnomalyAwareTrainer:
    """
    Trainer sp√©cialis√© pour la d√©tection d'anomalies avec taxonomie.
    
    Fonctionnalit√©s:
    - Configuration optimale par type d'anomalie
    - Seuils adapt√©s √† la difficult√© de d√©tection
    - M√©triques ajust√©es selon l'impact business
    - Support configuration manuelle ou automatique
    """
    
    def __init__(
        self,
        anomaly_type: str,
        *,
        model_config: Optional[ModelConfig] = None,
        training_config: Optional[TrainingConfig] = None,
        taxonomy_config: Optional[Dict[str, Any]] = None,
        callbacks: Optional[List[TrainingCallback]] = None
    ):
        """
        Initialise le trainer pour anomalies.
        
        Args:
            anomaly_type: Type d'anomalie (structural, visual, geometric, etc.)
            model_config: Configuration du mod√®le (optionnel, sinon auto-configur√©)
            training_config: Configuration d'entra√Ænement (optionnel, sinon auto-configur√©)
            taxonomy_config: Configuration de taxonomie personnalis√©e (optionnel)
            callbacks: Callbacks pour monitoring (optionnel)
        """
        self.anomaly_type = anomaly_type
        self.taxonomy_config = taxonomy_config or self._get_default_taxonomy()
        self.callbacks = callbacks or []
        
        # Validation de model_config si fourni
        if model_config is not None:
            if not isinstance(model_config.model_type, ModelType):
                raise ValueError(
                    f"model_config.model_type doit √™tre une instance de ModelType, "
                    f"re√ßu: {type(model_config.model_type)}"
                )
        
        # Configuration automatique OU manuelle
        if model_config is None or training_config is None:
            auto_model_config, auto_training_config = self._configure_for_anomaly()
            self.model_config = model_config or auto_model_config
            self.training_config = training_config or auto_training_config
            logger.info(f"üîß Configuration automatique pour anomalie '{anomaly_type}'")
        else:
            self.model_config = model_config
            self.training_config = training_config
            logger.info(f"‚öôÔ∏è Configuration manuelle pour anomalie '{anomaly_type}'")
        
        # Attributs pour compatibilit√©
        self.model: Optional[nn.Module] = None
        self.preprocessor: Optional[DataPreprocessor] = None
        self.history: Dict[str, Any] = {}
        
        logger.info(
            "‚úÖ AnomalyAwareTrainer initialis√©",
            anomaly_type=anomaly_type,
            model_type=self.model_config.model_type.value
        )
    
    def _get_default_taxonomy(self) -> Dict[str, Any]:
        """Taxonomie par d√©faut si module externe indisponible"""
        return {
            "structural": {
                "recommended_model": ModelType.CONV_AUTOENCODER,
                "difficulty": "high",
                "threshold": 0.90,
                "params": {
                    "latent_dim": 256,
                    "learning_rate": 1e-4,
                    "base_filters": 32
                }
            },
            "visual": {
                "recommended_model": ModelType.DENOISING_AE,
                "difficulty": "medium",
                "threshold": 0.85,
                "params": {
                    "latent_dim": 128,
                    "learning_rate": 1e-3,
                    "base_filters": 64
                }
            },
            "geometric": {
                "recommended_model": ModelType.CUSTOM_RESNET,
                "difficulty": "low",
                "threshold": 0.95,
                "params": {
                    "learning_rate": 1e-3,
                    "dropout_rate": 0.5
                }
            }
        }
    
    def _configure_for_anomaly(self) -> Tuple[ModelConfig, TrainingConfig]:
        """Configure mod√®le et training selon le type d'anomalie"""
        category = self._get_anomaly_category(self.anomaly_type)
        config = self.taxonomy_config.get(category, self.taxonomy_config["structural"])
        
        # Configuration du mod√®le
        model_config = ModelConfig(
            model_type=config["recommended_model"],
            num_classes=2,  # Anomalie binaire
            **config.get("params", {})
        )
        
        # Configuration de l'entra√Ænement
        training_config = TrainingConfig(
            learning_rate=config["params"].get("learning_rate", 1e-4),
            epochs=100,
            batch_size=32,
            early_stopping_patience=15,
            use_class_weights=True
        )
        
        return model_config, training_config
    
    def _get_anomaly_category(self, anomaly_type: str) -> str:
        """D√©termine la cat√©gorie d'anomalie"""
        structural_types = ["crack", "corrosion", "deformation", "structural"]
        visual_types = ["scratch", "stain", "discoloration", "visual"]
        geometric_types = ["misalignment", "dimension_error", "geometric"]
        
        if anomaly_type in structural_types:
            return "structural"
        elif anomaly_type in visual_types:
            return "visual"
        elif anomaly_type in geometric_types:
            return "geometric"
        else:
            return "structural"  # Fallback
    
    def train(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray,
        callbacks: Optional[List[TrainingCallback]] = None
    ) -> Result:
        """
        Entra√Æne le mod√®le pour le type d'anomalie sp√©cifique.
        
        Args:
            X_train: Training features
            y_train: Training labels
            X_val: Validation features
            y_val: Validation labels
            callbacks: Callbacks suppl√©mentaires (optionnel)
            
        Returns:
            Result avec succ√®s/√©chec et m√©tadonn√©es
        """
        active_callbacks = callbacks or self.callbacks
        
        trainer = ComputerVisionTrainer(
            model_config=self.model_config,
            training_config=self.training_config,
            callbacks=active_callbacks
        )
        
        result = trainer.fit(X_train, y_train, X_val, y_val)
        
        # Copier les attributs pour compatibilit√©
        if result.success:
            self.model = trainer.model
            self.preprocessor = trainer.preprocessor
            self.history = result.data['history']
        
        return result
    
    def predict(self, X: np.ndarray, **kwargs) -> Result:
        """Wrapper pour pr√©dictions - d√©l√®gue au trainer interne"""
        if self.model is None or self.preprocessor is None:
            return Result.err("Mod√®le non entra√Æn√©")
        
        # Cr√©er un trainer temporaire pour predict
        temp_trainer = ComputerVisionTrainer(
            self.model_config,
            self.training_config
        )
        temp_trainer.model = self.model
        temp_trainer.preprocessor = self.preprocessor
        temp_trainer.device_manager = DeviceManager()
        
        return temp_trainer.predict(X, **kwargs)


# ==================
# UTILITIES AVANC√âES
# ==================

class ModelInterpreter:
    """Interpr√©tation des pr√©dictions du mod√®le"""
    
    @staticmethod
    def get_feature_importance(
        model: nn.Module,
        X_sample: np.ndarray,
        preprocessor: DataPreprocessor
    ) -> Result:
        """
        Calcule l'importance des features via gradient.
        
        Simple approximation pour d√©monstration.
        En production, utiliser SHAP, Grad-CAM, etc.
        
        Args:
            model: Mod√®le PyTorch
            X_sample: √âchantillon √† analyser
            preprocessor: Preprocessor du mod√®le
            
        Returns:
            Result avec carte d'importance
        """
        try:
            model.eval()
            X_norm = preprocessor.transform(X_sample, output_format="channels_first")
            X_tensor = torch.tensor(X_norm, dtype=torch.float32, requires_grad=True)
            
            if torch.cuda.is_available():
                X_tensor = X_tensor.cuda()
                model = model.cuda()
            
            # Forward pass
            output = model(X_tensor)
            pred_class = output.argmax(dim=1)
            
            # Backward pour obtenir gradients
            output[0, pred_class[0]].backward()
            
            # Importance = magnitude du gradient
            importance = X_tensor.grad.abs().cpu().numpy()
            
            return Result.ok({
                'importance_map': importance,
                'predicted_class': int(pred_class[0].cpu().numpy())
            })
            
        except Exception as e:
            logger.error(f"Erreur interpr√©tation: {e}")
            return Result.err(f"Interpr√©tation √©chou√©e: {str(e)}")


class DataAugmenter:
    """Augmentation de donn√©es pour am√©liorer la g√©n√©ralisation"""
    
    @staticmethod
    def augment(
        X: np.ndarray,
        y: np.ndarray,
        factor: int = 2,
        methods: Optional[List[str]] = None
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Augmente le dataset.
        
        Args:
            X: Images (N, H, W, C)
            y: Labels
            factor: Facteur d'augmentation
            methods: Liste de m√©thodes ('flip', 'rotate', 'noise')
            
        Returns:
            (X_augmented, y_augmented)
        """
        methods = methods or ['flip', 'rotate']
        
        augmented_X = [X]
        augmented_y = [y]
        
        for _ in range(factor - 1):
            X_aug = X.copy()
            
            if 'flip' in methods and np.random.rand() > 0.5:
                X_aug = np.flip(X_aug, axis=2)  # Flip horizontal
            
            if 'rotate' in methods and np.random.rand() > 0.5:
                angle = np.random.choice([90, 180, 270])
                X_aug = np.rot90(X_aug, k=angle//90, axes=(1, 2))
            
            if 'noise' in methods and np.random.rand() > 0.5:
                noise = np.random.normal(0, 0.01, X_aug.shape)
                X_aug = X_aug + noise
            
            augmented_X.append(X_aug)
            augmented_y.append(y)
        
        return np.concatenate(augmented_X), np.concatenate(augmented_y)


# ========================
# INT√âGRATION AVEC MLFLOW 
# ========================

class MLflowIntegration:
    """
    Int√©gration avec MLflow pour tracking d'exp√©riences.
    
    Note: N√©cessite mlflow install√©
    """
    
    def __init__(self, experiment_name: str = "computer_vision_training"):
        self.experiment_name = experiment_name
        self.mlflow_available = False
        
        try:
            import mlflow # type: ignore
            self.mlflow = mlflow
            self.mlflow_available = True
            mlflow.set_experiment(experiment_name)
            logger.info(f"üìä MLflow activ√©: experiment={experiment_name}")
        except ImportError:
            logger.warning("‚ö†Ô∏è MLflow non disponible, tracking d√©sactiv√©")
    
    def log_training(
        self,
        model_config: ModelConfig,
        training_config: TrainingConfig,
        history: Dict[str, Any],
        test_metrics: Dict[str, Any]
    ) -> None:
        """
        Log une session d'entra√Ænement compl√®te.
        
        Args:
            model_config: Configuration du mod√®le
            training_config: Configuration d'entra√Ænement
            history: Historique d'entra√Ænement
            test_metrics: M√©triques sur test set
        """
        if not self.mlflow_available:
            return
        
        try:
            with self.mlflow.start_run():
                # Log configs
                self.mlflow.log_params({
                    'model_type': model_config.model_type.value,
                    'num_classes': model_config.num_classes,
                    'input_channels': model_config.input_channels,
                    'learning_rate': training_config.learning_rate,
                    'batch_size': training_config.batch_size,
                    'epochs': training_config.epochs,
                    'optimizer': training_config.optimizer.value,
                    'use_class_weights': training_config.use_class_weights
                })
                
                # Log m√©triques finales
                for metric_name, metric_value in test_metrics.items():
                    if isinstance(metric_value, (int, float)):
                        self.mlflow.log_metric(f"test_{metric_name}", metric_value)
                
                # Log courbes d'entra√Ænement
                train_losses = history.get('train_loss', [])
                val_losses = history.get('val_loss', [])
                
                for epoch, (train_loss, val_loss) in enumerate(zip(train_losses, val_losses)):
                    self.mlflow.log_metric('train_loss', train_loss, step=epoch)
                    self.mlflow.log_metric('val_loss', val_loss, step=epoch)
                
                logger.info("‚úÖ M√©triques logu√©es dans MLflow")
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur logging MLflow: {e}")


# =====================
# CONFIGURATION FACTORY
# =====================

class ConfigFactory:
    """
    Factory pour cr√©er des configurations pr√©-d√©finies.
    """
    
    @staticmethod
    def get_config(preset: str) -> Tuple[ModelConfig, TrainingConfig]:
        """
        Retourne des configurations pr√©-d√©finies.
        
        Presets disponibles:
        - 'quick_test': Entra√Ænement rapide pour tests
        - 'balanced': Configuration √©quilibr√©e
        - 'high_accuracy': Optimis√© pour pr√©cision
        - 'production': Configuration production robuste
        
        Args:
            preset: Nom du preset
            
        Returns:
            (ModelConfig, TrainingConfig)
            
        Raises:
            ValueError: Si le preset n'existe pas
        """
        
        presets = {
            'quick_test': (
                ModelConfig(
                    model_type=ModelType.SIMPLE_CNN,
                    num_classes=2,
                    dropout_rate=0.3
                ),
                TrainingConfig(
                    epochs=5,
                    batch_size=32,
                    learning_rate=1e-3,
                    early_stopping_patience=3,
                    deterministic=True,
                    num_workers=0,
                    pin_memory=False
                )
            ),
            
            'balanced': (
                ModelConfig(
                    model_type=ModelType.CUSTOM_RESNET,
                    num_classes=2,
                    dropout_rate=0.5,
                    base_filters=64
                ),
                TrainingConfig(
                    epochs=50,
                    batch_size=32,
                    learning_rate=1e-4,
                    optimizer=OptimizerType.ADAMW,
                    scheduler=SchedulerType.REDUCE_ON_PLATEAU,
                    early_stopping_patience=10,
                    use_class_weights=True,
                    deterministic=True,
                    num_workers=0,
                    pin_memory=False
                )
            ),
            
            'high_accuracy': (
                ModelConfig(
                    model_type=ModelType.TRANSFER_LEARNING,
                    num_classes=2,
                    pretrained=True,
                    freeze_layers=100,
                    dropout_rate=0.5
                ),
                TrainingConfig(
                    epochs=100,
                    batch_size=16,
                    learning_rate=1e-5,
                    optimizer=OptimizerType.ADAMW,
                    scheduler=SchedulerType.COSINE,
                    early_stopping_patience=20,
                    use_class_weights=True,
                    use_mixed_precision=False,
                    deterministic=True,
                    num_workers=0,
                    pin_memory=False
                )
            ),
            
            'production': (
                ModelConfig(
                    model_type=ModelType.CUSTOM_RESNET,
                    num_classes=2,
                    dropout_rate=0.5,
                    base_filters=64
                ),
                TrainingConfig(
                    epochs=100,
                    batch_size=32,
                    learning_rate=1e-4,
                    optimizer=OptimizerType.ADAMW,
                    scheduler=SchedulerType.REDUCE_ON_PLATEAU,
                    early_stopping_patience=15,
                    reduce_lr_patience=8,
                    use_class_weights=True,
                    gradient_clip=1.0,
                    use_mixed_precision=False,
                    deterministic=True,
                    seed=42,
                    num_workers=0,
                    pin_memory=False
                )
            )
        }
        
        if preset not in presets:
            raise ValueError(
                f"Preset inconnu: {preset}. "
                f"Disponibles: {list(presets.keys())}"
            )
        
        return presets[preset]
