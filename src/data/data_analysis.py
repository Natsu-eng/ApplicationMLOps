"""
Module d'analyse de donn√©es robuste pour le machine learning.
Optimis√© pour la production avec gestion m√©moire avanc√©e et monitoring.
src/data/data_analysis.py
"""

import pandas as pd
import numpy as np
import time
from typing import Union, Tuple, Dict, Any, List, Optional
import gc

# Import des modules d√©plac√©s
from monitoring.decorators import monitor_performance, safe_execute
from monitoring.system_monitor import check_system_resources as check_resources
from helpers.dask_helpers import is_dask_dataframe, compute_if_dask
from helpers.data_samplers import safe_sample
from helpers.data_transformers import optimize_dataframe
from helpers.streamlit_helpers import conditional_cache

# Configuration du logging
from src.shared.logging import get_logger
logger = get_logger(__name__)

# Tentative d'import de d√©pendances optionnelles
try:
    import dask.dataframe as dd
    DASK_AVAILABLE = True
except ImportError:
    DASK_AVAILABLE = False
    logger.warning("Dask non disponible, utilisation de Pandas uniquement")

try:
    from scipy.stats import pointbiserialr, f_oneway
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    logger.warning("SciPy non disponible, certaines analyses statistiques limit√©es")

try:
    import streamlit as st
    STREAMLIT_AVAILABLE = True
except ImportError:
    STREAMLIT_AVAILABLE = False


# D√©finition locale de cleanup_memory (pour compatibilit√© et nettoyage m√©moire)
def cleanup_memory():
    """Nettoyage m√©moire simple et robuste."""
    gc.collect()


# =============================
# Fonctions principales
# =============================

@conditional_cache(use_cache=True)
@safe_execute(fallback_value={"numeric": [], "categorical": [], "text_or_high_cardinality": [], "datetime": []})
@monitor_performance
def auto_detect_column_types(
    df: Union[pd.DataFrame, 'dd.DataFrame'], 
    sample_frac: float = 0.01, 
    max_rows: int = 10000,
    high_cardinality_threshold: int = 100
) -> Dict[str, List[str]]:
    """
    D√©tecte automatiquement les types de colonnes (num√©rique, cat√©gorielle, datetime, texte).
    Version robuste avec gestion d'erreurs am√©lior√©e.
    
    Args:
        df: DataFrame Pandas ou Dask
        sample_frac: Fraction de l'√©chantillon pour Dask ou gros datasets
        max_rows: Nombre maximum de lignes √† analyser
        high_cardinality_threshold: Seuil pour la cardinalit√© √©lev√©e
        
    Returns:
        Dictionnaire avec les listes de colonnes par type
    """
    try:
        if df is None or df.empty or len(df.columns) == 0:
            logger.warning("‚ö†Ô∏è DataFrame vide ou sans colonnes")
            return {"numeric": [], "categorical": [], "text_or_high_cardinality": [], "datetime": []}
        
        # √âchantillonnage s√©curis√©
        sample_df = safe_sample(df, sample_frac, max_rows)
        
        if sample_df.empty:
            logger.warning("‚ö†Ô∏è √âchantillon DataFrame vide")
            return {"numeric": [], "categorical": [], "text_or_high_cardinality": [], "datetime": []}

        # Initialisation du r√©sultat
        result = {
            "numeric": [],
            "datetime": [],
            "categorical": [],
            "text_or_high_cardinality": []
        }

        # D√©tection des colonnes num√©riques (types natifs)
        try:
            numeric_cols = sample_df.select_dtypes(include=[np.number]).columns.tolist()
            result["numeric"] = [col for col in numeric_cols if col in df.columns]
            logger.debug(f"üî¢ Colonnes num√©riques d√©tect√©es: {len(result['numeric'])}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è D√©tection colonnes num√©riques √©chou√©e: {e}")

        # D√©tection des colonnes datetime (types natifs et conversion)
        try:
            # Types datetime natifs
            datetime_cols = sample_df.select_dtypes(include=["datetime", "datetimetz"]).columns.tolist()
            
            # Tentative de conversion des colonnes objet qui ressemblent √† des dates
            object_cols = sample_df.select_dtypes(include=["object"]).columns
            for col in object_cols:
                if col not in datetime_cols and col not in result["numeric"]:
                    try:
                        # Essayer de convertir en datetime
                        converted = pd.to_datetime(sample_df[col], errors='coerce')
                        if converted.notna().mean() > 0.8:  # Si >80% de conversion r√©ussie
                            datetime_cols.append(col)
                    except:
                        pass
            
            result["datetime"] = [col for col in datetime_cols if col in df.columns]
            logger.debug(f"üìÖ Colonnes datetime d√©tect√©es: {len(result['datetime'])}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è D√©tection colonnes datetime √©chou√©e: {e}")

        # Analyse des colonnes object/category
        try:
            object_cols = sample_df.select_dtypes(include=["object", "category"]).columns.tolist()
            # Exclure les colonnes d√©j√† class√©es comme datetime
            object_cols = [col for col in object_cols if col not in result["datetime"]]
            
            for col in object_cols:
                try:
                    if col not in df.columns:
                        continue
                        
                    col_series = sample_df[col].dropna()
                    if len(col_series) == 0:
                        result["text_or_high_cardinality"].append(col)
                        continue
                        
                    unique_count = col_series.nunique()
                    total_count = len(col_series)
                    unique_ratio = unique_count / total_count if total_count > 0 else 1
                    
                    # Logique de classification am√©lior√©e
                    if unique_ratio < 0.5 and unique_count <= high_cardinality_threshold:
                        result["categorical"].append(col)
                    else:
                        result["text_or_high_cardinality"].append(col)
                        
                except Exception as e:
                    logger.debug(f"‚ùå Erreur analyse colonne {col}: {e}")
                    result["text_or_high_cardinality"].append(col)
                    
            logger.debug(f"üè∑Ô∏è Colonnes cat√©gorielles d√©tect√©es: {len(result['categorical'])}")
            logger.debug(f"üìù Colonnes texte/haute cardinalit√©: {len(result['text_or_high_cardinality'])}")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Analyse colonnes object √©chou√©e: {e}")

        # Nettoyage m√©moire
        del sample_df
        gc.collect()

        total_detected = sum(len(cols) for cols in result.values())
        logger.info(f"‚úÖ D√©tection types colonnes termin√©e: {total_detected} colonnes classifi√©es")
        return result

    except Exception as e:
        logger.error(f"‚ùå Erreur critique dans auto_detect_column_types: {e}", exc_info=True)
        return {"numeric": [], "categorical": [], "text_or_high_cardinality": [], "datetime": []}

@conditional_cache(use_cache=True)
@safe_execute(fallback_value={"count": 0, "missing_values": 0, "missing_percentage": "100.00%"})
@monitor_performance
def get_column_profile(
    series: Union[pd.Series, 'dd.Series'], 
    sample_frac: float = 0.01, 
    max_rows: int = 10000
) -> Dict[str, Any]:
    """
    G√©n√®re un profil statistique pour une colonne donn√©e.
    Version robuste avec gestion d'erreurs.
    
    Args:
        series: S√©rie Pandas ou Dask
        sample_frac: Fraction de l'√©chantillon pour Dask ou gros datasets
        max_rows: Nombre maximum de lignes √† analyser
        
    Returns:
        Dictionnaire avec les statistiques de la colonne
    """
    try:
        if series is None or len(series) == 0:
            return {"count": 0, "missing_values": 0, "missing_percentage": "100.00%"}
        
        # √âchantillonnage s√©curis√©
        is_dask = is_dask_dataframe(series) if hasattr(series, 'dtype') else False
        n_rows = len(series) if not is_dask else compute_if_dask(series.shape[0])
        
        if is_dask or n_rows > max_rows:
            sample_size = min(max_rows, max(100, int(n_rows * sample_frac)))
            if is_dask:
                actual_frac = min(0.1, sample_size / n_rows)
                sample_series = series.sample(frac=actual_frac).head(sample_size)
            else:
                sample_series = series.sample(n=sample_size, random_state=42)
        else:
            sample_series = series
            
        sample_series = compute_if_dask(sample_series)

        # Statistiques de base
        total_count = len(sample_series)
        valid_count = sample_series.count()
        missing_count = total_count - valid_count
        missing_percentage = (missing_count / total_count * 100) if total_count > 0 else 0

        profile = {
            "count": valid_count,
            "missing_values": missing_count,
            "missing_percentage": f"{missing_percentage:.2f}%",
            "total_rows_analyzed": total_count,
            "dtype": str(sample_series.dtype)
        }

        # Statistiques sp√©cifiques au type
        if valid_count > 0:
            try:
                if pd.api.types.is_numeric_dtype(sample_series.dtype):
                    valid_series = sample_series.dropna()
                    stats = {
                        "mean": float(valid_series.mean()),
                        "std_dev": float(valid_series.std()),
                        "min": float(valid_series.min()),
                        "25%": float(valid_series.quantile(0.25)),
                        "median": float(valid_series.median()),
                        "75%": float(valid_series.quantile(0.75)),
                        "max": float(valid_series.max()),
                    }
                    
                    # Skewness seulement si assez de donn√©es
                    if len(valid_series) > 1:
                        stats["skewness"] = float(valid_series.skew())
                    
                    profile.update(stats)
                    
                elif pd.api.types.is_datetime64_any_dtype(sample_series.dtype):
                    valid_series = sample_series.dropna()
                    profile.update({
                        "min_date": str(valid_series.min()),
                        "max_date": str(valid_series.max()),
                        "unique_dates": int(valid_series.nunique()),
                        "date_range_days": (valid_series.max() - valid_series.min()).days
                    })
                else:
                    # Colonnes cat√©gorielles ou texte
                    valid_series = sample_series.dropna()
                    unique_count = valid_series.nunique()
                    profile.update({
                        "unique_values": int(unique_count),
                        "unique_ratio": float(unique_count / len(valid_series)) if len(valid_series) > 0 else 0
                    })
                    
                    # Top valeurs pour les colonnes cat√©gorielles
                    if unique_count <= 20:
                        try:
                            top_values = valid_series.value_counts().head(10).to_dict()
                            profile["top_values"] = {str(k): int(v) for k, v in top_values.items()}
                        except Exception as e:
                            logger.debug(f"‚ùå Calcul top valeurs √©chou√©: {e}")
                            
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Calcul statistiques avanc√©es √©chou√© pour {series.name}: {e}")
                profile["computation_error"] = str(e)

        logger.debug(f"‚úÖ Profil calcul√© pour colonne {series.name}: {profile.get('count', 0)} valeurs valides")
        return profile

    except Exception as e:
        logger.error(f"‚ùå Erreur critique dans get_column_profile for {getattr(series, 'name', 'unknown')}: {e}")
        return {"error": str(e), "count": 0, "missing_values": 0, "missing_percentage": "100.00%"}

@conditional_cache(use_cache=True)
@safe_execute(fallback_value={})
@monitor_performance
def get_data_profile(
    df: Union[pd.DataFrame, 'dd.DataFrame'], 
    sample_frac: float = 0.01, 
    max_rows: int = 10000
) -> Dict[str, Dict[str, Any]]:
    """
    G√©n√®re un profil global du dataset par colonne.
    Version optimis√©e avec traitement par batch.
    
    Args:
        df: DataFrame Pandas ou Dask
        sample_frac: Fraction de l'√©chantillon pour Dask ou gros datasets
        max_rows: Nombre maximum de lignes √† analyser
        
    Returns:
        Dictionnaire avec les profils par colonne
    """
    try:
        if df is None or df.empty or len(df.columns) == 0:
            logger.warning("‚ö†Ô∏è DataFrame vide ou sans colonnes")
            return {}

        profiles = {}
        total_columns = len(df.columns)
        
        logger.info(f"üìä Calcul profil donn√©es pour {total_columns} colonnes")
        
        # Traitement par batch pour √©viter la surcharge m√©moire
        batch_size = min(10, total_columns)
        
        for i in range(0, total_columns, batch_size):
            batch_cols = df.columns[i:i + batch_size]
            logger.debug(f"üîß Traitement batch {i//batch_size + 1}/{(total_columns + batch_size - 1)//batch_size}")
            
            for col in batch_cols:
                try:
                    profiles[col] = get_column_profile(df[col], sample_frac, max_rows)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Profilage colonne {col} √©chou√©: {e}")
                    profiles[col] = {"error": str(e), "count": 0}
                    
            # Nettoyage m√©moire p√©riodique
            if i % (batch_size * 3) == 0:
                gc.collect()

        logger.info(f"‚úÖ Profil donn√©es termin√© pour {len(profiles)} colonnes")
        return profiles

    except Exception as e:
        logger.error(f"‚ùå Erreur critique dans get_data_profile: {e}")
        return {}

@conditional_cache(use_cache=True)
@safe_execute(fallback_value={"constant": [], "id_like": []})
@monitor_performance
def analyze_columns(
    df: Union[pd.DataFrame, 'dd.DataFrame'], 
    sample_frac: float = 0.01, 
    max_rows: int = 10000
) -> Dict[str, List[str]]:
    """
    D√©tecte les colonnes constantes ou de type ID.
    Version optimis√©e avec gestion d'erreurs.
    
    Args:
        df: DataFrame Pandas ou Dask
        sample_frac: Fraction de l'√©chantillon pour Dask ou gros datasets
        max_rows: Nombre maximum de lignes √† analyser
        
    Returns:
        Dictionnaire avec les colonnes constantes et ID-like
    """
    try:
        if df is None or df.empty or len(df.columns) == 0:
            return {"constant": [], "id_like": []}

        # √âchantillonnage s√©curis√©
        sample_df = safe_sample(df, sample_frac, max_rows)
        
        if sample_df.empty:
            return {"constant": [], "id_like": []}

        constant_cols = []
        id_like_cols = []
        
        try:
            nunique = sample_df.nunique()
            n_rows = len(sample_df)
            
            for col in sample_df.columns:
                try:
                    unique_count = nunique.get(col, 0)
                    
                    # Colonnes constantes
                    if unique_count <= 1:
                        constant_cols.append(col)
                        
                    # Colonnes de type ID (unique pour chaque ligne)
                    elif unique_count == n_rows and n_rows > 10:
                        id_like_cols.append(col)
                        
                except Exception as e:
                    logger.debug(f"‚ùå Erreur analyse colonne {col}: {e}")
                    continue
                    
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Analyse colonnes √©chou√©e: {e}")
            return {"constant": [], "id_like": []}

        # Nettoyage m√©moire
        del sample_df
        gc.collect()

        logger.info(f"‚úÖ Analyse colonnes termin√©e: {len(constant_cols)} constantes, {len(id_like_cols)} ID-like")
        return {"constant": constant_cols, "id_like": id_like_cols}

    except Exception as e:
        logger.error(f"‚ùå Erreur critique dans analyze_columns: {e}")
        return {"constant": [], "id_like": []}

@conditional_cache(use_cache=True)
@safe_execute(fallback_value={"is_imbalanced": False, "imbalance_ratio": 1.0, "message": "Error in detection"})
@monitor_performance
def detect_imbalance(
    df: Union[pd.DataFrame, 'dd.DataFrame'], 
    target_column: str, 
    threshold: float = 0.8,
    sample_frac: float = 0.1,
    max_rows: int = 10000
) -> Dict[str, Any]:
    """
    D√©tecte le d√©s√©quilibre des classes pour les probl√®mes de classification.
    Version robuste avec gestion d'erreurs.
    
    Args:
        df: DataFrame Pandas ou Dask
        target_column: Nom de la colonne cible
        threshold: Seuil de d√©s√©quilibre (0.8 = 80% dans une classe)
        sample_frac: Fraction d'√©chantillonnage
        max_rows: Nombre maximum de lignes √† analyser
        
    Returns:
        Dictionnaire avec les r√©sultats de d√©tection
    """
    try:
        # Validation des entr√©es
        if target_column not in df.columns:
            return {
                "is_imbalanced": False,
                "imbalance_ratio": 1.0,
                "message": f"Colonne cible '{target_column}' non trouv√©e",
                "class_distribution": {}
            }
        
        # √âchantillonnage s√©curis√©
        sample_df = safe_sample(df, sample_frac, max_rows)
        
        if sample_df.empty:
            return {
                "is_imbalanced": False,
                "imbalance_ratio": 1.0,
                "message": "√âchantillon vide",
                "class_distribution": {}
            }
        
        target_series = sample_df[target_column].dropna()
        
        if len(target_series) == 0:
            return {
                "is_imbalanced": False,
                "imbalance_ratio": 1.0,
                "message": "Aucune valeur valide dans la colonne cible",
                "class_distribution": {}
            }
        
        # Calcul de la distribution des classes
        class_distribution = target_series.value_counts().to_dict()
        total_samples = len(target_series)
        
        if len(class_distribution) <= 1:
            return {
                "is_imbalanced": False,
                "imbalance_ratio": 1.0,
                "message": "Une seule classe d√©tect√©e",
                "class_distribution": class_distribution
            }
        
        # Calcul du ratio de d√©s√©quilibre
        majority_class_count = max(class_distribution.values())
        imbalance_ratio = majority_class_count / total_samples
        
        # D√©tection du d√©s√©quilibre
        is_imbalanced = imbalance_ratio > threshold
        
        # Calcul de m√©triques suppl√©mentaires
        minority_class_count = min(class_distribution.values())
        balance_ratio = minority_class_count / majority_class_count if majority_class_count > 0 else 0
        
        result = {
            "is_imbalanced": is_imbalanced,
            "imbalance_ratio": float(imbalance_ratio),
            "balance_ratio": float(balance_ratio),
            "threshold_used": float(threshold),
            "total_samples": total_samples,
            "total_classes": len(class_distribution),
            "class_distribution": class_distribution,
            "majority_class": {
                "class": max(class_distribution, key=class_distribution.get),
                "count": majority_class_count,
                "percentage": float(majority_class_count / total_samples * 100)
            },
            "minority_class": {
                "class": min(class_distribution, key=class_distribution.get),
                "count": minority_class_count,
                "percentage": float(minority_class_count / total_samples * 100)
            }
        }
        
        # Messages explicatifs
        if is_imbalanced:
            result["message"] = f"üö® D√©s√©quilibre d√©tect√© : {result['majority_class']['percentage']:.1f}% dans la classe majoritaire"
            result["recommendation"] = "Envisagez d'activer SMOTE ou d'utiliser l'√©chantillonnage"
        else:
            result["message"] = "‚úÖ Classes √©quilibr√©es"
            result["recommendation"] = "Aucune action n√©cessaire"
        
        logger.info(f"‚úÖ Analyse d√©s√©quilibre termin√©e : {result['message']}")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Erreur dans detect_imbalance : {e}")
        return {
            "is_imbalanced": False,
            "imbalance_ratio": 1.0,
            "message": f"Erreur d'analyse : {str(e)}",
            "class_distribution": {}
        }

@conditional_cache(use_cache=True)
@safe_execute(fallback_value={"target_type": "unknown", "task": "unknown"})
def get_target_and_task(
    df: Union[pd.DataFrame, 'dd.DataFrame'],
    target: Optional[str]
) -> Dict[str, str]:
    """
    D√©tecte le type de t√¢che ML selon la colonne cible.
    Version robuste avec validation.
    
    Args:
        df: DataFrame Pandas ou Dask
        target: Nom de la colonne cible ou None pour non supervis√©
        
    Returns:
        Dictionnaire avec le type de cible et la t√¢che ML
    """
    try:
        # Cas non supervis√©
        if target is None:
            return {"target_type": "unsupervised", "task": "clustering"}

        if target not in df.columns:
            logger.warning(f"‚ö†Ô∏è Colonne cible '{target}' non trouv√©e dans le DataFrame")
            return {"target_type": "unknown", "task": "unknown"}
            
        # √âchantillonnage pour l'analyse
        sample_df = safe_sample(df, sample_frac=0.05, max_rows=20000)
        
        if sample_df.empty or target not in sample_df.columns:
            return {"target_type": "unknown", "task": "unknown"}
            
        target_series = sample_df[target].dropna()
        
        if len(target_series) == 0:
            logger.warning(f"‚ö†Ô∏è Colonne cible '{target}' sans valeurs valides")
            return {"target_type": "unknown", "task": "unknown"}
            
        unique_vals = target_series.nunique()
        total_vals = len(target_series)

        # Logique de d√©tection am√©lior√©e
        if pd.api.types.is_numeric_dtype(target_series):
            # Pour les variables num√©riques
            unique_ratio = unique_vals / total_vals
            
            if unique_vals <= 20 or unique_ratio < 0.05:
                # Peu de valeurs uniques -> classification
                return {"target_type": "classification", "task": "classification"}
            else:
                # Beaucoup de valeurs uniques -> r√©gression
                return {"target_type": "regression", "task": "regression"}
        else:
            # Pour les variables non-num√©riques -> toujours classification
            return {"target_type": "classification", "task": "classification"}
            
    except Exception as e:
        logger.error(f"‚ùå Erreur dans get_target_and_task pour target '{target}': {e}")
        return {"target_type": "unknown", "task": "unknown"}

@conditional_cache(use_cache=True)
@safe_execute(fallback_value={"numeric": [], "categorical": []})
@monitor_performance
def get_relevant_features(
    df: Union[pd.DataFrame, 'dd.DataFrame'],
    target: str,
    sample_frac: float = 0.05,
    max_rows: int = 20000,
    correlation_threshold: float = 0.1,
    p_value_threshold: float = 0.05
) -> Dict[str, List[str]]:
    """
    S√©lectionne les features pertinentes via corr√©lation/ANOVA.
    Version robuste avec validation am√©lior√©e.
    
    Args:
        df: DataFrame Pandas ou Dask
        target: Nom de la colonne cible
        sample_frac: Fraction de l'√©chantillon pour Dask ou gros datasets
        max_rows: Nombre maximum de lignes √† analyser
        correlation_threshold: Seuil de corr√©lation minimum
        p_value_threshold: Seuil de p-value maximum
        
    Returns:
        Dictionnaire avec les features num√©riques et cat√©gorielles pertinentes
    """
    try:
        if target not in df.columns:
            logger.warning(f"‚ö†Ô∏è Colonne cible '{target}' non trouv√©e")
            return {"numeric": [], "categorical": []}

        # √âchantillonnage s√©curis√©
        sample_df = safe_sample(df, sample_frac, max_rows)
        
        if sample_df.empty or target not in sample_df.columns:
            return {"numeric": [], "categorical": []}

        target_series = sample_df[target].dropna()
        
        if len(target_series) < 10:  # Minimum pour les tests statistiques
            logger.warning(f"‚ö†Ô∏è Donn√©es insuffisantes pour analyse features ({len(target_series)} valeurs target valides)")
            return {"numeric": [], "categorical": []}

        features = {"numeric": [], "categorical": []}
        
        # Aligner les indices pour √©viter les erreurs de correspondance
        valid_indices = target_series.index
        sample_df_aligned = sample_df.loc[valid_indices]

        # Analyse des features num√©riques
        numeric_cols = sample_df_aligned.select_dtypes(include=[np.number]).columns
        numeric_cols = [col for col in numeric_cols if col != target]
        
        for col in numeric_cols:
            try:
                feature_series = sample_df_aligned[col].fillna(0)  # Imputation simple
                
                if feature_series.nunique() > 1:  # √âviter les colonnes constantes
                    try:
                        # Corr√©lation de Pearson pour variables continues
                        if pd.api.types.is_numeric_dtype(target_series):
                            corr_coef = np.corrcoef(target_series, feature_series)[0, 1]
                            if not np.isnan(corr_coef) and abs(corr_coef) > correlation_threshold:
                                features["numeric"].append(col)
                        else:
                            # Point-biserial pour target cat√©gorielle
                            if SCIPY_AVAILABLE:
                                corr, p_val = pointbiserialr(target_series.astype('category').cat.codes, 
                                                        feature_series)
                                if not np.isnan(corr) and abs(corr) > correlation_threshold and p_val < p_value_threshold:
                                    features["numeric"].append(col)
                            else:
                                # Fallback sans scipy
                                corr_coef = np.corrcoef(target_series.astype('category').cat.codes, feature_series)[0, 1]
                                if not np.isnan(corr_coef) and abs(corr_coef) > correlation_threshold:
                                    features["numeric"].append(col)
                                
                    except Exception as e:
                        logger.debug(f"‚ùå Analyse corr√©lation √©chou√©e pour {col}: {e}")
                        
            except Exception as e:
                logger.debug(f"‚ùå Erreur traitement feature num√©rique {col}: {e}")
                continue

        # Analyse des features cat√©gorielles
        categorical_cols = sample_df_aligned.select_dtypes(include=["object", "category"]).columns
        categorical_cols = [col for col in categorical_cols if col != target]
        
        for col in categorical_cols:
            try:
                feature_series = sample_df_aligned[col].dropna()
                
                if len(feature_series) < 10 or feature_series.nunique() <= 1:
                    continue
                    
                # √âviter les colonnes avec trop de cat√©gories
                if feature_series.nunique() > min(50, len(feature_series) * 0.5):
                    continue
                
                try:
                    # ANOVA pour tester la diff√©rence de moyennes entre groupes
                    if SCIPY_AVAILABLE and pd.api.types.is_numeric_dtype(target_series):
                        groups = []
                        for value in feature_series.unique():
                            if pd.notna(value):
                                group_data = target_series[sample_df_aligned[col] == value].dropna()
                                if len(group_data) > 0:
                                    groups.append(group_data)
                        
                        if len(groups) > 1 and all(len(g) > 0 for g in groups):
                            _, p_val = f_oneway(*groups)
                            if not np.isnan(p_val) and p_val < p_value_threshold:
                                features["categorical"].append(col)
                    else:
                        # Pour target cat√©gorielle ou sans scipy, utiliser une heuristique simple
                        if feature_series.nunique() >= 2:
                            features["categorical"].append(col)
                                
                except Exception as e:
                    logger.debug(f"‚ùå ANOVA √©chou√©e pour {col}: {e}")
                    
            except Exception as e:
                logger.debug(f"‚ùå Erreur traitement feature cat√©gorielle {col}: {e}")
                continue

        # Nettoyage m√©moire
        del sample_df, sample_df_aligned
        gc.collect()

        total_features = len(features["numeric"]) + len(features["categorical"])
        logger.info(f"‚úÖ Analyse pertinence features termin√©e: {total_features} features pertinentes trouv√©es")
        
        return features

    except Exception as e:
        logger.error(f"‚ùå Erreur critique dans get_relevant_features: {e}")
        return {"numeric": [], "categorical": []}

@conditional_cache(use_cache=True)
@safe_execute(fallback_value=[])
@monitor_performance
def detect_useless_columns(
    df: Union[pd.DataFrame, 'dd.DataFrame'],
    threshold_missing: float = 0.6,
    min_unique_ratio: float = 0.0001,
    max_sample_size: int = 50000
) -> List[str]:
    """
    D√©tecte les colonnes inutiles - version robuste.
    """
    try:
        # Validation initiale
        if df is None or len(df.columns) == 0:
            logger.warning("‚ö†Ô∏è DataFrame vide ou sans colonnes")
            return []
        
        if hasattr(df, 'empty') and df.empty:
            return []

        # √âchantillonnage
        sample_df = safe_sample(df, sample_frac=0.05, max_rows=max_sample_size)
        if sample_df.empty or len(sample_df.columns) == 0:
            return []

        useless_columns = set()

        # 1. Colonnes avec trop de NaN
        try:
            missing_ratios = sample_df.isna().mean()
            high_missing = missing_ratios[missing_ratios > threshold_missing].index
            useless_columns.update(high_missing)
            logger.debug(f"Colonnes avec NaN excessifs: {len(high_missing)}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è V√©rification NaN √©chou√©e: {e}")

        # 2. Colonnes constantes
        try:
            nunique_counts = sample_df.nunique(dropna=True)
            constant_cols = nunique_counts[nunique_counts <= 1].index
            useless_columns.update(constant_cols)
            logger.debug(f"Colonnes constantes: {len(constant_cols)}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è V√©rification constantes √©chou√©e: {e}")

        # 3. Colonnes √† faible variance (version robuste)
        try:
            numeric_cols = sample_df.select_dtypes(include=np.number).columns
            for col in numeric_cols:
                if col in useless_columns:  # D√©j√† d√©tect√©
                    continue
                    
                std_val = sample_df[col].std()
                if pd.isna(std_val):
                    continue
                    
                # Variance nulle
                if std_val == 0:
                    useless_columns.add(col)
                    continue
                    
                # Faible coefficient de variation
                mean_val = sample_df[col].mean()
                if mean_val != 0 and abs(std_val / mean_val) < min_unique_ratio:
                    useless_columns.add(col)
                    
            logger.debug(f"Colonnes num√©riques v√©rifi√©es: {len(numeric_cols)}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è V√©rification variance √©chou√©e: {e}")

        # 4. Validation finale
        valid_columns = [col for col in useless_columns if col in df.columns]
        
        logger.info(f"‚úÖ Colonnes inutiles d√©tect√©es: {len(valid_columns)}/{len(df.columns)}")
        if valid_columns:
            logger.debug(f"Colonnes: {valid_columns}")
            
        return valid_columns

    except Exception as e:
        logger.error(f"‚ùå Erreur critique: {e}")
        return []

@conditional_cache(use_cache=True)
@safe_execute(fallback_value={})
@monitor_performance
def compute_global_metrics(df: Union[pd.DataFrame, 'dd.DataFrame']) -> Dict[str, Any]:
    """
    Calcule les m√©triques globales du dataset.
    """
    try:
        if df is None or df.empty:
            return {}

        # Si c'est un Dask DataFrame, on calcule les m√©triques de mani√®re distribu√©e
        if is_dask_dataframe(df):
            n_rows = compute_if_dask(df.shape[0])
            n_cols = compute_if_dask(df.shape[1])
            missing_count = compute_if_dask(df.isnull().sum().sum())
            duplicate_rows = compute_if_dask(df.duplicated().sum())
        else:
            n_rows = len(df)
            n_cols = len(df.columns)
            missing_count = df.isnull().sum().sum()
            duplicate_rows = df.duplicated().sum()

        total_cells = n_rows * n_cols
        missing_percentage = (missing_count / total_cells * 100) if total_cells > 0 else 0

        return {
            'n_rows': n_rows,
            'n_cols': n_cols,
            'missing_count': missing_count,
            'missing_percentage': missing_percentage,
            'duplicate_rows': duplicate_rows,
            'total_cells': total_cells
        }

    except Exception as e:
        logger.error(f"Erreur dans compute_global_metrics: {e}")
        return {}
    
@conditional_cache(use_cache=True)
@safe_execute(fallback_value={})
@monitor_performance
def detect_outliers_iqr(data: pd.Series) -> Dict:
    """D√©tection des outliers avec m√©thode IQR - Logique m√©tier"""
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers = data[(data < lower_bound) | (data > upper_bound)]
    
    return {
        'outliers': outliers,
        'bounds': {'lower': lower_bound, 'upper': upper_bound},
        'stats': {'Q1': Q1, 'Q3': Q3, 'IQR': IQR},
        'count': len(outliers),
        'percentage': (len(outliers) / len(data)) * 100
    }

@conditional_cache(use_cache=True)
@safe_execute(fallback_value={})
@monitor_performance
def calculate_correlation_significance(df: pd.DataFrame, var1: str, var2: str) -> Dict:
    """Calcule la significativit√© des corr√©lations - Logique m√©tier"""
    try:
        from scipy.stats import pearsonr, spearmanr
        
        data = df[[var1, var2]].dropna()
        
        if len(data) < 3:
            return {'error': 'Donn√©es insuffisantes'}
        
        # Pearson pour relation lin√©aire
        pearson_corr, pearson_p = pearsonr(data[var1], data[var2])
        
        # Spearman pour relation monotone
        spearman_corr, spearman_p = spearmanr(data[var1], data[var2])
        
        return {
            'pearson': {'correlation': pearson_corr, 'p_value': pearson_p},
            'spearman': {'correlation': spearman_corr, 'p_value': spearman_p},
            'sample_size': len(data)
        }
    except ImportError:
        # Fallback sans scipy
        correlation = data[var1].corr(data[var2])
        return {
            'pearson': {'correlation': correlation, 'p_value': None},
            'spearman': {'correlation': correlation, 'p_value': None},
            'sample_size': len(data),
            'warning': 'SciPy non disponible, calculs limit√©s'
        }

@conditional_cache(use_cache=True)
@safe_execute(fallback_value={})
@monitor_performance  
def chi_square_test(contingency_table: pd.DataFrame) -> Dict:
    """Test du chi-carr√© pour tables de contingence - Logique m√©tier"""
    try:
        from scipy.stats import chi2_contingency
        chi2, p_value, dof, expected = chi2_contingency(contingency_table)
        return {
            'chi2': chi2,
            'p_value': p_value,
            'degrees_of_freedom': dof,
            'significant': p_value < 0.05
        }
    except Exception as e:
        return {'error': str(e)}

# Export des fonctions principales
__all__ = [
    'auto_detect_column_types', # Pour d√©tection types colonnes
    'get_column_profile', # Pour le profil d'une colonne
    'get_data_profile', # Pour le profil global du dataset
    'analyze_columns', # Pour d√©tection colonnes constantes/ID
    'detect_imbalance', # Pour d√©tection d√©s√©quilibre des classes
    'get_target_and_task', # Pour d√©tection type t√¢che ML
    'get_relevant_features', # Pour s√©lection features pertinentes
    'detect_useless_columns', # Pour d√©tection colonnes inutiles
    'cleanup_memory', # Nettoyage m√©moire
    'safe_sample', # √âchantillonnage s√©curis√©
    'optimize_dataframe', # Optimisation DataFrame
    'compute_global_metrics', # Nouvelle fonction pour m√©triques globales
    'detect_outliers_iqr', # D√©tection outliers IQR
    'calculate_correlation_significance', # Pour corr√©lation et significativit√©
    'chi_square_test' # Test du chi-carr√©
]