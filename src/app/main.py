"""
Application principale Streamlit pour DataLab Pro.
Version optimis√©e pour la production avec gestion robuste des erreurs et monitoring.
"""
import pkg_resources
import sys
import os
# Ajout de la racine du projet √† sys.path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import numpy as np
import streamlit as st
import pandas as pd
import logging
import warnings
import time
import psutil
from src.data.data_loader import load_data
from src.shared.logging import setup_logging, get_logger
from typing import Dict, Any
import gc

# Import des constantes ET de la navigation
from src.config.constants import ANOMALY_CONFIG, APP_CONSTANTS, TRAINING_CONSTANTS
from helpers.navigation_manager import NavigationManager

# Configuration du logger
logger = get_logger(__name__)

def _get_production_css():
    """CSS pour masquer les √©l√©ments Streamlit en production."""
    return """
    <style>
    #MainMenu {visibility: hidden;}
    .stDeployButton {display:none;}
    footer {visibility: hidden;}
    #stDecoration {display:none;}
    .stAlert > div  {
        padding-top: 0.5rem;
        padding-bottom: 0.5rem;
    }
    .main > div {
        padding-top: 1rem;
    }
    </style>
    """

# --- Configuration Production ---
def setup_production_environment():
    """Configuration pour l'environnement de production."""
    warnings.filterwarnings("ignore", category=FutureWarning, module='numpy')
    warnings.filterwarnings("ignore", category=UserWarning, module='streamlit')
    
    try:
        import mlflow
        logger.info("MLflow is available for tracking experiments")
    except ImportError:
        logger.warning("MLflow not installed, experiment tracking disabled") 
    
    setup_logging(mlflow_integration=True)
    
    if 'production_setup_done' not in st.session_state:
        st.session_state.production_setup_done = True
        if os.getenv('STREAMLIT_ENV') == 'production':
            st.markdown(_get_production_css(), unsafe_allow_html=True)

# --- Fonctions de Monitoring ---
def get_system_metrics() -> Dict[str, Any]:
    """R√©cup√®re les m√©triques syst√®me actuelles."""
    try:
        memory = psutil.virtual_memory()
        return {
            'memory_percent': memory.percent,
            'memory_available_mb': memory.available / (1024 * 1024),
            'timestamp': time.time()
        }
    except Exception as e:
        logger.error(f"Failed to get system metrics: {e}")
        return {'memory_percent': 0, 'memory_available_mb': 0, 'timestamp': time.time()}

def check_system_health():
    """V√©rifie la sant√© du syst√®me et affiche des alertes si n√©cessaire."""
    metrics = get_system_metrics()
    if metrics['memory_percent'] > TRAINING_CONSTANTS["HIGH_MEMORY_THRESHOLD"]:
        st.warning(f"‚ö†Ô∏è Utilisation m√©moire √©lev√©e: {metrics['memory_percent']:.1f}%")
        logger.warning(f"High memory usage detected: {metrics['memory_percent']:.1f}%")
        if metrics['memory_percent'] > 90:
            if st.button("üßπ Nettoyer la m√©moire", help="Lib√®re la m√©moire et vide les caches"):
                cleanup_memory()
                st.success("Nettoyage m√©moire effectu√©")
                st.rerun()

def cleanup_memory():
    """Nettoyage m√©moire robuste avec logs."""
    try:
        collected = gc.collect()
        if hasattr(st, 'cache_data'):
            st.cache_data.clear()
        if hasattr(st, 'cache_resource'):
            st.cache_resource.clear()
        for key in list(st.session_state.keys()):
            if key.startswith("_") or key in ["df", "df_raw", "X", "y", "data_dir"]:
                continue
            if isinstance(st.session_state[key], (pd.DataFrame, dict, list)):
                del st.session_state[key]
        logger.info(f"Memory cleanup: {collected} objects collected")
        return collected
    except Exception as e:
        logger.error(f"Memory cleanup failed: {e}", exc_info=True)
        return 0

# --- Fonctions de Gestion d'√âtat ---
def initialize_session():
    """Initialise l'√©tat de base de la session de fa√ßon robuste."""
    required_keys = {
        'df': None,
        'df_raw': None,
        'uploaded_file_name': None,
        'target_column_for_ml_config': None,
        'task_type': APP_CONSTANTS["DEFAULT_TASK_TYPE"],
        'config': None,
        'model_name': None,
        'model_params': {},
        'preprocessing': {},
        'n_splits': APP_CONSTANTS["DEFAULT_N_SPLITS"],
        'model': None,
        'metrics_summary': None,
        'preprocessor': None,
        'ml_results': [],
        'last_system_check': 0,
        'error_count': 0,
        # ‚úÖ NOUVEAUX KEYS POUR NAVIGATION
        'data_type': 'none',  # 'tabular', 'images', 'none'
        'X': None,  # Pour donn√©es images
        'y': None,  # Pour labels images
        'data_dir': None,  # Pour datasets images
        'dataset_structure': None,  # Structure d√©tect√©e
        'dataset_info': None,  # Infos dataset
        'current_page': 'main.py',  # Page actuelle pour navigation
        'dashboard_version': 1,  # Version pour cache
        'dataset_hash': '',  # Hash pour d√©tection changements
        'column_types': None,  # Types de colonnes d√©tect√©s
        'selected_univar_col': None,  # S√©lection univari√©e
        'selected_bivar_col1': None,  # S√©lection bivari√©e
        'selected_bivar_col2': None,  # S√©lection bivari√©e
        'useless_candidates': [],  # Colonnes inutiles d√©tect√©es
        'rename_list': []  # Liste de renommage
    }
    for key, default_value in required_keys.items():
        if key not in st.session_state:
            st.session_state[key] = default_value
    current_time = time.time()
    if current_time - st.session_state.last_system_check > 300:
        check_system_health()
        st.session_state.last_system_check = current_time

def reset_app_state():
    """R√©initialise toutes les variables de session li√©es √† un jeu de donn√©es."""
    logger.info("R√©initialisation de l'√©tat de l'application pour un nouveau fichier")
    try:
        old_error_count = st.session_state.get('error_count', 0)
        old_current_page = st.session_state.get('current_page', 'main.py')
        
        reset_keys = [
            'df', 'df_raw', 'uploaded_file_name', 'target_column_for_ml_config',
            'task_type', 'config', 'model_name', 'model_params', 'preprocessing',
            'n_splits', 'model', 'metrics_summary', 'preprocessor', 'ml_results',
            # ‚úÖ R√âINITIALISATION DES DONN√âES IMAGES AUSSI
            'X', 'y', 'data_dir', 'dataset_structure', 'dataset_info', 'data_type',
            'dashboard_version', 'dataset_hash', 'column_types', 'selected_univar_col',
            'selected_bivar_col1', 'selected_bivar_col2', 'useless_candidates', 'rename_list'
        ]
        for key in reset_keys:
            if key in st.session_state:
                del st.session_state[key]
        
        # R√©initialisation avec valeurs par d√©faut
        st.session_state.update({
            'df': None,
            'df_raw': None,
            'uploaded_file_name': None,
            'target_column_for_ml_config': None,
            'task_type': APP_CONSTANTS["DEFAULT_TASK_TYPE"],
            'config': None,
            'model_name': None,
            'model_params': {},
            'preprocessing': {},
            'n_splits': APP_CONSTANTS["DEFAULT_N_SPLITS"],
            'model': None,
            'metrics_summary': None,
            'preprocessor': None,
            'ml_results': [],
            'data_type': 'none',
            'X': None,
            'y': None,
            'data_dir': None,
            'dataset_structure': None,
            'dataset_info': None,
            'current_page': old_current_page,
            'dashboard_version': 1,
            'dataset_hash': '',
            'column_types': None,
            'selected_univar_col': None,
            'selected_bivar_col1': None,
            'selected_bivar_col2': None,
            'useless_candidates': [],
            'rename_list': [],
            'error_count': old_error_count
        })
        
        cleanup_memory()
        logger.info("√âtat de l'application r√©initialis√© avec succ√®s")
        st.toast("Application r√©initialis√©e pour le nouveau fichier", icon="üîÑ")
    except Exception as e:
        logger.error(f"Erreur lors de la r√©initialisation : {e}")
        st.error(f"Erreur lors de la r√©initialisation : {e}")

def validate_session_state() -> bool:
    """Valide l'int√©grit√© de l'√©tat de la session."""
    try:
        # V√©rification des donn√©es tabulaires
        if 'df' in st.session_state and st.session_state.df is not None:
            df = st.session_state.df
            if not hasattr(df, 'columns') or len(df.columns) == 0:
                logger.warning("DataFrame in session_state is corrupted")
                return False
        
        # V√©rification des donn√©es images
        if 'X' in st.session_state and st.session_state.X is not None:
            X = st.session_state.X
            if len(X) == 0:
                logger.warning("Image data in session_state is corrupted")
                return False
        
        return True
    except Exception as e:
        logger.error(f"Session state validation failed: {e}")
        return False

# --- Initialisation de l'application ---
st.set_page_config(
    page_title="DataLab Pro | Accueil",
    page_icon="üß™",
    layout="centered",
    initial_sidebar_state="collapsed"
)

setup_production_environment()

try:
    initialize_session()
    if not validate_session_state():
        logger.warning("Invalid session state detected, resetting...")
        reset_app_state()
except Exception as e:
    logger.error(f"Session initialization failed: {e}")
    st.error("Erreur d'initialisation de la session. Veuillez recharger la page.")
    st.stop()

# ‚úÖ MISE √Ä JOUR DE LA PAGE COURANTE POUR LA NAVIGATION
st.session_state.current_page = "main.py"

# Header avec informations syst√®me
col_title, col_system = st.columns([3, 1])
with col_title:
    st.title("üß™ DataLab Pro")
    st.markdown("Plateforme d'analyse de donn√©es et de Machine Learning automatis√©")
with col_system:
    metrics = get_system_metrics()
    if metrics['memory_percent'] > 0:
        color = "üî¥" if metrics['memory_percent'] > TRAINING_CONSTANTS["HIGH_MEMORY_THRESHOLD"] else "üü°" if metrics['memory_percent'] > 70 else "üü¢"
        st.caption(f"{color} RAM: {metrics['memory_percent']:.0f}%")

st.markdown("---")

# Section principale de chargement
st.header("üìÇ Importation des donn√©es")

with st.expander("‚ÑπÔ∏è Formats support√©s et limites", expanded=False):
    st.markdown(f"""
    **Formats accept√©s :** {', '.join(APP_CONSTANTS["SUPPORTED_EXTENSIONS"]).upper()}
    
    **Limites :**
    - Taille maximale : {APP_CONSTANTS["MAX_FILE_SIZE_MB"]:,} MB
    - Automatiquement optimis√© selon la taille (Pandas ‚â§ 100MB, Dask > 100MB)
    - Validation d'int√©grit√© avant chargement
    
    **Fonctionnalit√©s automatiques :**
    - D√©tection et suppression des doublons
    - Conversion intelligente des types de donn√©es
    - Optimisation m√©moire pour les gros datasets
    """)

uploaded_file = st.file_uploader(
    "Choisissez votre fichier de donn√©es",
    type=list(APP_CONSTANTS["SUPPORTED_EXTENSIONS"]),
    key="file_uploader",
    help=f"Formats support√©s: {', '.join(APP_CONSTANTS['SUPPORTED_EXTENSIONS']).upper()} ‚Ä¢ Maximum {APP_CONSTANTS['MAX_FILE_SIZE_MB']}MB"
)

if uploaded_file is not None:
    try:
        file_size_mb = uploaded_file.size / (1024 * 1024) if hasattr(uploaded_file, 'size') else 0
        if file_size_mb > APP_CONSTANTS["MAX_FILE_SIZE_MB"]:
            st.error(f"‚ùå Fichier trop volumineux: {file_size_mb:.1f}MB > {APP_CONSTANTS['MAX_FILE_SIZE_MB']}MB")
            logger.error(f"File too large: {file_size_mb:.1f}MB")
            st.stop()
        
        if st.session_state.uploaded_file_name != uploaded_file.name:
            logger.info(f"New file detected: {uploaded_file.name}")
            reset_app_state()
            
            progress_container = st.container()
            with progress_container:
                st.info(f"üì• Chargement de **{uploaded_file.name}** ({file_size_mb:.1f}MB)...")
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                progress_bar.progress(20)
                status_text.text("Validation du fichier...")
                time.sleep(0.5)
                
                progress_bar.progress(40)
                status_text.text("Chargement des donn√©es...")
                
                try:
                    df, report, df_raw = load_data(
                        file_path=uploaded_file,
                        blocksize="64MB",
                        sanitize_for_display=True
                    )
                    progress_bar.progress(80)
                    status_text.text("Finalisation...")
                except Exception as load_error:
                    progress_bar.empty()
                    status_text.empty()
                    st.error(f"‚ùå Erreur lors du chargement: {str(load_error)}")
                    logger.error(f"Data loading failed: {load_error}")
                    st.session_state.error_count += 1
                    st.stop()
                
                progress_bar.progress(100)
                status_text.text("Termin√©!")
                time.sleep(0.5)
                progress_container.empty()
            
            if df is not None:
                st.session_state.df = df
                st.session_state.df_raw = df_raw
                st.session_state.uploaded_file_name = uploaded_file.name
                st.session_state.data_type = "tabular"
                logger.info(f"File loaded successfully: {uploaded_file.name}")
                
                if report and report.get("actions"):
                    st.success("‚úÖ Fichier charg√© avec succ√®s!")
                    with st.expander("üìã Rapport de chargement", expanded=False):
                        for action in report["actions"]:
                            st.write(f"‚Ä¢ {action}")
                        if report.get("changes"):
                            st.subheader("üîß Conversions de types automatiques")
                            changes_df = pd.DataFrame([
                                {"Colonne": col, "Conversion": change}
                                for col, change in report["changes"].items()
                            ])
                            st.dataframe(changes_df, use_container_width=True)
                        if report.get("warnings"):
                            st.subheader("‚ö†Ô∏è Avertissements")
                            for warning in report["warnings"]:
                                st.warning(warning)
                
                st.rerun()
            else:
                error_messages = report.get("actions", ["Erreur inconnue"]) if report else ["Erreur inconnue"]
                st.error(f"‚ùå √âchec du chargement: {error_messages[0]}")
                logger.error(f"Data loading failed: {error_messages[0]}")
                st.session_state.error_count += 1
                st.markdown("""
                **Suggestions pour r√©soudre le probl√®me:**
                - V√©rifiez le format du fichier
                - Assurez-vous que le fichier n'est pas corrompu
                - Essayez avec un fichier plus petit
                - V√©rifiez l'encodage (UTF-8 recommand√©)
                """)
    except Exception as e:
        st.error(f"‚ùå Erreur inattendue: {str(e)}")
        logger.error(f"Unexpected error during file processing: {e}", exc_info=True)
        st.session_state.error_count += 1

# Section pour le dataset MVTec AD - VERSION CORRIG√âE
st.header("üì∑ Dataset MVTec AD - D√©tection d'Anomalies")

with st.expander("‚ÑπÔ∏è √Ä propos du dataset MVTec AD", expanded=False):
    st.markdown("""
    **Dataset MVTec AD** : Benchmark industriel pour la d√©tection d'anomalies visuelles
    
    **üìÅ Structure attendue :**
    ```
    votre_dataset/
    ‚îú‚îÄ‚îÄ train/
    ‚îÇ   ‚îî‚îÄ‚îÄ good/          # Images normales pour l'entra√Ænement
    ‚îÇ       ‚îú‚îÄ‚îÄ image1.png
    ‚îÇ       ‚îî‚îÄ‚îÄ image2.png
    ‚îî‚îÄ‚îÄ test/
        ‚îú‚îÄ‚îÄ good/          # Images normales pour le test
        ‚îî‚îÄ‚îÄ defect_type/   # Images avec d√©fauts sp√©cifiques
    ```
    
    **‚öôÔ∏è Configuration automatique :**
    - Redimensionnement : 256√ó256 pixels
    - Normalisation : Standard ImageNet
    - Format : PNG/JPG (RGB)
    - Augmentation optionnelle disponible
    """)

# Configuration en deux colonnes
col_config, col_info = st.columns([2, 1])

with col_config:
    dataset_option = st.radio(
        "Mode de chargement",
        options=["üìÇ Charger depuis un dossier local", "üîÑ Utiliser un dataset exemple"],
        help="Choisissez comment charger vos donn√©es d'images"
    )
    
    if dataset_option == "üìÇ Charger depuis un dossier local":
        st.subheader("Emplacement du dataset")
        
        # Chemins par d√©faut intelligents
        default_paths = [
            os.path.join(project_root, "data", "mvtec_ad"),
            os.path.join(project_root, "src", "data", "mvtec_ad"),
            os.path.join(os.path.expanduser("~"), "Downloads", "mvtec_ad")
        ]
        
        existing_path = None
        for path in default_paths:
            if os.path.exists(path):
                existing_path = path
                break
        
        data_dir = st.text_input(
            "üìÅ Chemin absolu du dossier MVTec AD",
            value=existing_path or default_paths[0],
            placeholder=f"ex: {default_paths[0]}",
            help="Chemin complet vers le dossier racine de votre dataset MVTec AD"
        )
        
        # Validation en temps r√©el
        if data_dir:
            from src.data.image_processing import detect_dataset_structure
            
            structure = detect_dataset_structure(data_dir)
            
            if structure["type"] == "invalid":
                st.error("‚ùå Dossier introuvable - V√©rifiez le chemin")
            elif structure["type"] != "mvtec_ad":
                st.warning(f"‚ö†Ô∏è Structure '{structure['type']}' d√©tect√©e (MVTec AD attendu)")
                st.info("Le dataset sera charg√© avec la structure d√©tect√©e")
            else:
                st.success("‚úÖ Structure MVTec AD d√©tect√©e")
                    
    else:  # Dataset exemple
        st.subheader("Dataset d'exemple")
        example_datasets = {
            "bottle": "Bouteilles industrielles",
            "cable": "C√¢bles √©lectriques", 
            "capsule": "Capsules m√©dicaments",
            "metal_nut": "√âcrous m√©talliques"
        }
        
        selected_example = st.selectbox(
            "Choisissez une cat√©gorie d'exemple",
            options=list(example_datasets.keys()),
            format_func=lambda x: f"{x} - {example_datasets[x]}",
            help="Dataset MVTec AD de d√©monstration"
        )
        
        example_path = os.path.join(project_root, "src", "data", "mvtec_ad", selected_example)
        data_dir = example_path
        
        st.info(f"**Dataset s√©lectionn√© :** {example_datasets[selected_example]}")
        
        if not os.path.exists(example_path):
            st.warning(f"‚ö†Ô∏è Dataset exemple '{selected_example}' non disponible")
            st.markdown("""
            **üì• T√©l√©chargement des donn√©es d'exemple :**
            1. Visitez [MVTec AD Dataset](https://www.mvtec.com/company/research/datasets/mvtec-ad)
            2. T√©l√©chargez la cat√©gorie souhait√©e
            3. Extrayez dans : `data/mvtec_ad/`
            """)

with col_info:
    st.subheader("üìä Informations")
    
    if 'data_dir' in locals() and data_dir and os.path.exists(data_dir):
        try:
            from src.data.image_processing import get_dataset_info
            
            info = get_dataset_info(data_dir)
            
            if "total" in info:
                st.metric("üì∑ Images totales", f"{info['total']:,}")
            
            if "normal" in info and "anomaly" in info:
                st.metric("üü¢ Normales", f"{info['normal']:,}")
                st.metric("üî¥ Anomalies", f"{info['anomaly']:,}")
            
            with st.expander("üìÅ Structure d√©taill√©e", expanded=False):
                st.json(info)
            
        except Exception as e:
            st.info("‚ÑπÔ∏è Analyse de la structure en attente...")
            logger.error(f"Info display error: {e}")

# Bouton de chargement unique
if 'data_dir' in locals() and data_dir and os.path.exists(data_dir):
    st.markdown("---")
    
    col_load, col_status = st.columns([1, 2])
    
    with col_load:
        load_button = st.button(
            "üöÄ Charger le Dataset", 
            type="primary",
            key="load_mvtec_dataset",
            help="Pr√©pare le dataset pour l'analyse et l'entra√Ænement"
        )
    
    with col_status:
        current_dataset = st.session_state.get("data_dir")
        if current_dataset == data_dir:
            st.success("‚úÖ Dataset d√©j√† charg√© et pr√™t")
        elif current_dataset:
            st.info("‚ÑπÔ∏è Un dataset diff√©rent est actuellement charg√©")

    if load_button:
        try:
            with st.spinner("üîç Validation et chargement du dataset..."):
                from src.data.image_processing import (
                    detect_dataset_structure,
                    load_images_flexible,
                    get_dataset_info
                )
                
                # === √âTAPE 1 : Validation structure ===
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                status_text.text("üîç Validation de la structure...")
                structure = detect_dataset_structure(data_dir)
                
                if structure["type"] == "invalid":
                    st.error(f"‚ùå Structure invalide: {structure.get('error', 'Erreur inconnue')}")
                    st.stop()
                
                progress_bar.progress(20)
                
                # === √âTAPE 2 : Chargement des images ===
                status_text.text("üì• Chargement des images...")
                
                try:
                    X, y = load_images_flexible(
                        data_dir,
                        target_size=(256, 256)
                    )
                    
                    if len(X) == 0:
                        st.error("‚ùå Aucune image trouv√©e dans le dataset")
                        st.stop()
                    
                    progress_bar.progress(60)
                    
                except Exception as load_error:
                    st.error(f"‚ùå Erreur chargement images: {str(load_error)}")
                    logger.error(f"Image loading failed: {load_error}", exc_info=True)
                    st.stop()
                
                # === √âTAPE 3 : Normalisation ===
                status_text.text("‚öôÔ∏è Normalisation des images...")
                
                # Normaliser si n√©cessaire
                if X.max() > 1.0:
                    X_normalized = X / 255.0
                else:
                    X_normalized = X.copy()
                
                progress_bar.progress(80)
                
                # === √âTAPE 4 : Mise en session ===
                status_text.text("üíæ Sauvegarde en session...")
                
                # R√©initialisation propre
                reset_app_state()
                
                # Calcul des infos
                info = get_dataset_info(data_dir)
                
                st.session_state.update({
                    "X": X,
                    "X_normalized": X_normalized,
                    "y": y,
                    "data_dir": data_dir,
                    "data_type": "images",
                    "task_type": "anomaly_detection" if structure["type"] == "mvtec_ad" else "classification",
                    "dataset_structure": structure,
                    "dataset_info": info,
                    "dataset_loaded_at": time.time(),
                    "image_count": len(X),
                    "image_shape": X.shape,
                    "n_classes": len(np.unique(y))
                })
                
                progress_bar.progress(100)
                status_text.text("‚úÖ Termin√©!")
                
                logger.info(f"Dataset loaded: {data_dir} | Images: {len(X)} | Classes: {len(np.unique(y))}")
                
                time.sleep(0.5)
                progress_bar.empty()
                status_text.empty()
                
                # === AFFICHAGE R√âSUM√â ===
                st.success(f"‚úÖ Dataset charg√© avec succ√®s!")
                
                col_summary1, col_summary2, col_summary3 = st.columns(3)
                
                with col_summary1:
                    st.metric("üì∑ Images", f"{len(X):,}")
                
                with col_summary2:
                    st.metric("üìê Dimensions", f"{X.shape[1]}√ó{X.shape[2]}")
                
                with col_summary3:
                    st.metric("üéØ Classes", len(np.unique(y)))
                
                # Info additionnelle
                with st.expander("üìã D√©tails du chargement", expanded=False):
                    st.write(f"**Structure d√©tect√©e:** {structure['type']}")
                    st.write(f"**Type de t√¢che:** {st.session_state.task_type}")
                    st.write(f"**Shape compl√®te:** {X.shape}")
                    st.write(f"**Plage valeurs:** [{X.min():.2f}, {X.max():.2f}]")
                    st.write(f"**M√©moire:** {X.nbytes / (1024**2):.1f} MB")
                
                # Redirection automatique
                st.info("üéØ Redirection vers le Dashboard...")
                time.sleep(1.5)
                st.switch_page("pages/1_dashboard.py")
                
        except Exception as e:
            error_msg = f"Erreur lors du chargement: {str(e)[:200]}"
            st.error(f"‚ùå {error_msg}")
            logger.error(f"MVTec dataset loading failed: {error_msg}", exc_info=True)
            st.session_state.error_count = st.session_state.get('error_count', 0) + 1
            
            # Afficher les d√©tails pour debug
            with st.expander("üîß D√©tails de l'erreur (debug)", expanded=False):
                st.code(str(e))

# Section d'aide contextuelle
if not st.session_state.get("data_dir") and not st.session_state.get("X"):
    st.markdown("---")
    with st.expander("üÜò Guide de d√©marrage rapide", expanded=False):
        st.markdown("""
        **Pour utiliser la d√©tection d'anomalies :**
        
        1. **üì• T√©l√©chargez MVTec AD** depuis [le site officiel](https://www.mvtec.com/company/research/datasets/mvtec-ad)
        2. **üìÅ Organisez vos donn√©es** selon la structure MVTec AD
        3. **üöÄ Chargez le dataset** via l'interface ci-dessus
        4. **üîç Explorez** dans le Dashboard
        5. **ü§ñ Entra√Ænez** vos mod√®les dans l'onglet ML
        
        **üìö Cat√©gories disponibles :**
        - **bottle, cable, capsule** - Objets manufactur√©s
        - **metal_nut, pill, screw** - Composants industriels  
        - **carpet, leather, tile** - Textures et surfaces
        - **grid, transistor, wood** - Structures complexes
        
        **üí° Formats support√©s :**
        - Structure MVTec AD (train/test avec good/d√©fauts)
        - Dossiers cat√©goriels (un dossier = une classe)
        - Dossier plat (toutes images m√©lang√©es)
        """)

# --- Affichage de l'√©tat actuel ---
if st.session_state.df is not None or st.session_state.X is not None:
    try:
        if st.session_state.data_type == "images":
            # Affichage pour donn√©es images
            X = st.session_state.X
            st.success(f"‚úÖ Dataset **{st.session_state.data_dir}** pr√™t pour l'analyse")
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("üì∑ Images", f"{len(X):,}")
            with col2:
                st.metric("üìê Dimensions", f"{X.shape[1]}√ó{X.shape[2]}")
            with col3:
                st.metric("üéØ Classes", f"{st.session_state.n_classes}")
            with col4:
                memory_mb = X.nbytes / (1024**2)
                st.metric("üíæ M√©moire", f"{memory_mb:.1f} MB")
            
            st.subheader("üéØ Prochaines √©tapes")
            st.info("Utilisez la barre lat√©rale pour naviguer vers le Dashboard d'analyse d'images")
            
        else:
            # Affichage pour donn√©es tabulaires
            df = st.session_state.df
            st.success(f"‚úÖ Dataset **{st.session_state.uploaded_file_name}** pr√™t pour l'analyse")
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                n_rows = len(df) if not hasattr(df, 'npartitions') else "Dask"
                st.metric("Lignes", f"{n_rows:,}" if isinstance(n_rows, int) else n_rows)
            with col2:
                st.metric("Colonnes", f"{len(df.columns)}")
            with col3:
                if not hasattr(df, 'npartitions'):
                    memory_mb = df.memory_usage(deep=True).sum() / (1024 * 1024)
                    st.metric("M√©moire", f"{memory_mb:.1f} MB")
                else:
                    st.metric("Partitions", f"{df.npartitions}")
            with col4:
                df_type = "Dask" if hasattr(df, 'npartitions') else "Pandas"
                st.metric("Type", df_type)
            
            st.subheader("Aper√ßu des donn√©es")
            try:
                preview_rows = min(100, len(df) if not hasattr(df, 'npartitions') else 100)
                if hasattr(df, 'npartitions'):
                    df_preview = df.head(preview_rows).compute()
                else:
                    df_preview = df.head(preview_rows)
                st.dataframe(df_preview, use_container_width=True, height=300)
                if len(df_preview) == preview_rows:
                    st.caption(f"Affichage des {preview_rows} premi√®res lignes")
            except Exception as preview_error:
                st.warning(f"‚ö†Ô∏è Erreur d'aper√ßu: {preview_error}")
                logger.error(f"Preview error: {preview_error}")
                try:
                    df_fallback = df.head(50).astype(str)
                    if hasattr(df_fallback, 'compute'):
                        df_fallback = df_fallback.compute()
                    st.dataframe(df_fallback, use_container_width=True)
                    st.caption("Aper√ßu avec conversion forc√©e en texte")
                except:
                    st.error("Impossible d'afficher l'aper√ßu des donn√©es")
        
        st.markdown("---")
        st.subheader("üöÄ √âtapes suivantes")
        col_nav1, col_nav2, col_nav3 = st.columns(3)
        with col_nav1:
            st.markdown("""
            **üìä Dashboard**
            - Vue d'ensemble des donn√©es
            - Analyse des valeurs manquantes
            - Distribution des variables
            """)
        with col_nav2:
            st.markdown("""
            **ü§ñ AutoML**
            - Configuration automatique
            - Entra√Ænement de mod√®les
            - √âvaluation des performances
            """)
        with col_nav3:
            st.markdown("""
            **üìà R√©sultats**
            - M√©triques d√©taill√©es
            - Visualisations
            - Export des mod√®les
            """)
        st.info("üí° Utilisez la barre lat√©rale pour naviguer entre les pages")
        
    except Exception as display_error:
        st.error(f"‚ùå Erreur d'affichage: {display_error}")
        logger.error(f"Display error: {display_error}", exc_info=True)
        st.session_state.error_count += 1
        if st.button("üîÑ R√©initialiser l'application"):
            reset_app_state()
            st.rerun()
else:
    st.info("üìÅ Chargez un fichier ou un dataset d'images pour commencer l'analyse des donn√©es")
    with st.expander("üí° Conseils pour de meilleurs r√©sultats", expanded=False):
        st.markdown("""
        **Pour donn√©es tabulaires:**
        - Nettoyez vos donn√©es avant le chargement si possible
        - Utilisez des noms de colonnes clairs et sans espaces
        - √âvitez les caract√®res sp√©ciaux dans les noms de colonnes
        
        **Pour donn√©es images:**
        - Structurez selon le format MVTec AD
        - Images en format PNG/JPG recommand√©
        - Taille minimale recommand√©e: 128√ó128 pixels
        
        **Performance:**
        - Les fichiers > 100MB utiliseront automatiquement Dask
        - Format Parquet recommand√© pour les gros volumes
        - CSV avec s√©parateurs standards (virgule, point-virgule)
        """)

# Footer avec informations de debug et actions utiles
st.markdown("---")
footer_col1, footer_col2, footer_col3 = st.columns(3)
with footer_col1:
    if st.session_state.get('error_count', 0) > 0:
        st.caption(f"‚ö†Ô∏è Erreurs: {st.session_state.error_count}")
    else:
        st.caption("‚úÖ Aucune erreur")
with footer_col2:
    current_time = time.strftime("%H:%M:%S")
    st.caption(f"‚è∞ Session: {current_time}")
with footer_col3:
    if st.button("üßπ Nettoyer cache", help="Lib√®re la m√©moire et vide les caches"):
        cleanup_memory()
        st.success("Cache nettoy√©")
        st.rerun()

if 'last_error_check' not in st.session_state:
    st.session_state.last_error_check = time.time()

if time.time() - st.session_state.last_error_check > 600:
    if st.session_state.get('error_count', 0) > 10:
        st.warning("‚ö†Ô∏è Plusieurs erreurs d√©tect√©es. Consid√©rez recharger l'application.")
        if st.button("üîÑ Recharger l'application"):
            st.session_state.clear()
            st.rerun()
    st.session_state.last_error_check = time.time()

# ‚úÖ NETTOYAGE FINAL
gc.collect()