"""
Page Streamlit: √âvaluation D√©tection d'Anomalies - Premium Dashboard
Version compl√®te avec design moderne et analyse approfondie
"""

import streamlit as st # type: ignore
import numpy as np # type: ignore
import pandas as pd # type: ignore
import time
import json
from pathlib import Path
from datetime import datetime
import plotly.graph_objects as go # type: ignore
from plotly.subplots import make_subplots # type: ignore
import plotly.express as px # type: ignore
from sklearn.metrics import ( # type: ignore    
    confusion_matrix, classification_report, roc_auc_score, 
    f1_score, precision_score, recall_score, accuracy_score,
    roc_curve, precision_recall_curve
)

# Imports m√©tier
try:
    from src.evaluation.anomaly_typing import AnomalyTypeAnalyzer
    from src.config.anomaly_taxonomy import ANOMALY_TAXONOMY
    from src.evaluation.computer_vision_metrics import (
        compute_anomaly_metrics, compute_reconstruction_metrics
    )
    from src.shared.logging import get_logger
    from src.config.constants import ANOMALY_CONFIG
except ImportError:
    # Fallback pour d√©veloppement
    class AnomalyTypeAnalyzer:
        def compute_metrics_by_anomaly_type(self, y_true, y_pred, types, threshold):
            return {}
        def generate_type_specific_recommendations(self, metrics):
            return []
        def create_performance_heatmap(self, metrics):
            return go.Figure()
        def create_category_summary(self, metrics):
            return pd.DataFrame()
    
    ANOMALY_TAXONOMY = {}
    ANOMALY_CONFIG = {"MLFLOW_ENABLED": False}
    
    def get_logger(name):
        # Utiliser le syst√®me de logging centralis√© m√™me en fallback
        try:
            from src.shared.logging import get_logger
            return get_logger(name)
        except ImportError:
            import logging
            return logging.getLogger(name)

import torch # type: ignore
from scipy.ndimage import zoom  # type: ignore

from monitoring.state_managers import init, AppPage

# ‚úÖ IMPORTS UI CENTRALIS√âS
from ui.anomaly_evaluation_styles import AnomalyEvaluationStyles
from helpers.ui_components.anomaly_evaluation import (
    safe_convert_history,
    analyze_false_positives,
    get_performance_status,
    create_performance_summary,
    generate_recommendations,
    create_performance_radar,
    plot_error_distribution
)
from helpers.anomaly_prediction_helpers import robust_predict_with_preprocessor

STATE = init()
logger = get_logger(__name__)

# ============================================================================
# CONFIGURATION STREAMLIT
# ============================================================================

st.set_page_config(
    page_title="Evaluation Dashboard | DataLab Pro",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# ============================================================================
# CSS ULTRA-MODERNE
# ============================================================================

# ‚úÖ Injection CSS centralis√©
st.markdown(AnomalyEvaluationStyles.get_css(), unsafe_allow_html=True)


# ============================================================================
# V√âRIFICATIONS INITIALES
# ============================================================================
# Note: Toutes les fonctions m√©tier sont import√©es depuis les helpers:
# - helpers/ui_components/anomaly_evaluation.py
# - helpers/anomaly_prediction_helpers.py
# ============================================================================

if not hasattr(STATE, 'training_results') or STATE.training_results is None:
    st.error("‚ùå Aucun mod√®le entra√Æn√©")
    st.info("üí° Veuillez d'abord entra√Æner un mod√®le dans la section Computer Vision")
    if st.button("üöÄ Aller √† l'Entra√Ænement", type="primary"):
        st.switch_page("pages/4_training_computer.py")
    st.stop()

if not isinstance(STATE.training_results, dict):
    st.error("‚ùå Format invalide des r√©sultats d'entra√Ænement")
    st.info("Type re√ßu: " + str(type(STATE.training_results)))
    st.stop()

if 'model' not in STATE.training_results:
    st.error("‚ùå Mod√®le manquant dans les r√©sultats")
    st.info("Cl√©s disponibles: " + str(list(STATE.training_results.keys())))
    st.stop()

# R√©cup√©ration donn√©es avec fallbacks
try:
    model = STATE.training_results["model"]
    history = safe_convert_history(STATE.training_results.get("history", {}))
    
    # Acc√®s safe √† model_config (avec fallback depuis training_results)
    if not hasattr(STATE, 'model_config') or STATE.model_config is None:
        # Fallback: r√©cup√©rer depuis training_results
        if isinstance(STATE.training_results, dict) and "model_config" in STATE.training_results:
            STATE.model_config = STATE.training_results["model_config"]
            logger.info("‚úÖ model_config r√©cup√©r√© depuis training_results")
        else:
            st.error("‚ùå Configuration du mod√®le manquante")
            st.info("üí° Veuillez relancer l'entra√Ænement pour g√©n√©rer la configuration")
            st.stop()
    
    model_type = STATE.model_config.get("model_type", "autoencoder") if isinstance(STATE.model_config, dict) else getattr(STATE.model_config, "model_type", "autoencoder")
    
    # R√©cup√©ration safe du preprocessor
    preprocessor = STATE.training_results.get("preprocessor")
    
    # V√©rification donn√©es test avec acc√®s via STATE.data
    if not hasattr(STATE.data, 'X_test') or STATE.data.X_test is None:
        st.error("‚ùå Donn√©es de test (X_test) manquantes")
        st.info("Veuillez relancer l'entra√Ænement pour g√©n√©rer les donn√©es de test")
        st.stop()
    
    if not hasattr(STATE.data, 'y_test') or STATE.data.y_test is None:
        st.error("‚ùå Labels de test (y_test) manquants")
        st.stop()
    
    X_test = STATE.data.X_test
    y_test = STATE.data.y_test
    
    # VALIDATION : Coh√©rence des donn√©es
    if len(X_test) != len(y_test):
        st.error(f"‚ùå Incoh√©rence: {len(X_test)} images mais {len(y_test)} labels")
        st.stop()
    
    logger.info(f"‚úÖ Donn√©es charg√©es: {len(X_test)} √©chantillons, mod√®le: {model_type}")
        
except KeyError as e:
    st.error(f"‚ùå Cl√© manquante dans les r√©sultats: {e}")
    st.info("Structure attendue: training_results[model, history, preprocessor]")
    st.stop()
except Exception as e:
    st.error(f"‚ùå Erreur chargement: {str(e)}")
    logger.error(f"Erreur chargement donn√©es: {e}", exc_info=True)
    st.stop()


# ============================================================================
# SIDEBAR
# ============================================================================

with st.sidebar:
    st.markdown("### ‚öôÔ∏è Configuration")
    
    threshold = st.slider(
        "**Seuil de Classification**",
        0.0, 1.0, 0.5, 0.01,
        help="Niveau de confiance requis"
    )
    
    if threshold < 0.3:
        st.error("üîª Seuil Bas - Plus de d√©tection")
    elif threshold > 0.7:
        st.warning("üî∫ Seuil √âlev√© - Plus de pr√©cision")
    else:
        st.success("‚úÖ Seuil Optimal")
    
    st.markdown("---")
    
    st.markdown("### üìä Options")
    show_error_analysis = st.checkbox("Analyse Erreurs", True)
    show_recommendations = st.checkbox("Recommandations", True)
    n_samples_viz = st.slider("√âchantillons", 1, 12, 6)
    
    st.markdown("---")
    
    st.markdown("### üîß Infos")
    st.metric("Type", model_type)
    st.metric("√âchantillons", len(X_test))


# ============================================================================
# INTERFACE PRINCIPALE
# ============================================================================

# Hero Header
st.markdown(f'''
<div class="hero-header">
    <h1 class="hero-title">üìä Dashboard d'√âvaluation</h1>
    <p class="hero-subtitle">Analyse approfondie des performances en d√©tection d'anomalies</p>
</div>
''', unsafe_allow_html=True)

# Pr√©dictions
with st.spinner("üîÆ Calcul des pr√©dictions..."):
    # ‚úÖ Utilisation du helper centralis√© avec STATE pour r√©cup√©ration shapes originales
    prediction_results = robust_predict_with_preprocessor(
        model, X_test, preprocessor, model_type,
        return_localization=True, STATE=STATE
    )
    y_pred_proba = prediction_results["y_pred_proba"]
    y_pred_binary = prediction_results["y_pred_binary"]

# M√©triques
with st.spinner("üìà Calcul des m√©triques..."):
    try:
        # Validation pr√©dictions
        if not isinstance(prediction_results, dict):
            st.error("‚ùå Format invalide des pr√©dictions")
            st.stop()
        
        y_pred_proba = prediction_results.get("y_pred_proba")
        y_pred_binary = prediction_results.get("y_pred_binary")
        
        if y_pred_proba is None or y_pred_binary is None:
            st.error("‚ùå Pr√©dictions manquantes")
            st.stop()
        
        # Validation coh√©rence
        if len(y_pred_binary) != len(y_test):
            st.error(f"‚ùå Incoh√©rence: {len(y_pred_binary)} pr√©dictions pour {len(y_test)} labels")
            st.stop()
        
        # Calcul m√©triques avec gestion erreurs individuelles
        metrics = {}
        
        # AUC-ROC (n√©cessite au moins 2 classes dans y_test)
        try:
            if len(np.unique(y_test)) >= 2:
                metrics['auc_roc'] = roc_auc_score(y_test, y_pred_proba)
            else:
                logger.warning("‚ö†Ô∏è AUC-ROC impossible: une seule classe dans y_test")
                metrics['auc_roc'] = 0.5
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur calcul AUC-ROC: {e}")
            metrics['auc_roc'] = 0.5
        
        # F1-Score
        try:
            metrics['f1_score'] = f1_score(y_test, y_pred_binary, zero_division=0)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur calcul F1: {e}")
            metrics['f1_score'] = 0.0
        
        # Precision
        try:
            metrics['precision'] = precision_score(y_test, y_pred_binary, zero_division=0)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur calcul Precision: {e}")
            metrics['precision'] = 0.0
        
        # Recall
        try:
            metrics['recall'] = recall_score(y_test, y_pred_binary, zero_division=0)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur calcul Recall: {e}")
            metrics['recall'] = 0.0
        
        # Accuracy
        try:
            metrics['accuracy'] = accuracy_score(y_test, y_pred_binary)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur calcul Accuracy: {e}")
            metrics['accuracy'] = 0.0
        
        # Specificity (n√©cessite au moins une classe 0)
        try:
            if np.any(y_test == 0):
                metrics['specificity'] = recall_score(
                    1 - y_test, 1 - y_pred_binary, zero_division=0
                )
            else:
                logger.warning("‚ö†Ô∏è Specificity impossible: aucune classe 0")
                metrics['specificity'] = 0.0
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur calcul Specificity: {e}")
            metrics['specificity'] = 0.0
        
        # M√©triques sp√©cifiques autoencoder
        if model_type in ["autoencoder", "conv_autoencoder", "variational_autoencoder", "denoising_autoencoder"]:
            reconstructed = prediction_results.get("reconstructed", X_test.copy())
            reconstruction_errors = prediction_results.get('reconstruction_errors')
            
            if reconstruction_errors is not None:
                metrics['reconstruction_error'] = float(np.mean(reconstruction_errors))
                metrics['reconstruction_std'] = float(np.std(reconstruction_errors))
                metrics['adaptive_threshold'] = prediction_results.get('adaptive_threshold', 0.5)
        
        # Matrice de confusion
        try:
            metrics['confusion_matrix'] = confusion_matrix(y_test, y_pred_binary).tolist()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur matrice confusion: {e}")
            metrics['confusion_matrix'] = [[0, 0], [0, 0]]
        
        logger.info(f"‚úÖ M√©triques calcul√©es: {list(metrics.keys())}")
        
        # Alerte fallback
        if prediction_results.get("fallback"):
            st.warning("‚ö†Ô∏è **Attention**: Pr√©dictions en mode fallback (al√©atoires). R√©sultats non fiables.")
        
    except Exception as e:
        st.error(f"‚ùå Erreur calcul m√©triques: {str(e)}")
        logger.error(f"Erreur m√©triques: {e}", exc_info=True)
        
        # M√©triques par d√©faut en cas d'√©chec total
        metrics = {
            'auc_roc': 0.5,
            'f1_score': 0.0,
            'precision': 0.0,
            'recall': 0.0,
            'accuracy': 0.0,
            'specificity': 0.0,
            'confusion_matrix': [[0, 0], [0, 0]]
        }

# Analyse
error_analysis = analyze_false_positives(X_test, y_test, y_pred_binary)
performance_summary = create_performance_summary(metrics, error_analysis)
recommendations = generate_recommendations(metrics, model_type, error_analysis, performance_summary)


# ============================================================================
# SECTION: M√âTRIQUES PRINCIPALES
# ============================================================================

st.markdown("### üìä M√©triques de Performance")

col1, col2, col3, col4 = st.columns(4)

main_metrics = [
    ("AUC-ROC", "auc_roc", "üéØ", col1),
    ("F1-Score", "f1_score", "‚ö°", col2),
    ("Precision", "precision", "üé™", col3),
    ("Recall", "recall", "üîç", col4)
]

for label, metric_key, icon, col in main_metrics:
    with col:
        value = metrics.get(metric_key, 0)
        status, status_text = get_performance_status(value, metric_key)
        
        st.markdown(f'''
        <div class="metric-card-premium">
            <span class="metric-icon">{icon}</span>
            <div class="metric-label">{label}</div>
            <div class="metric-value">{value:.3f}</div>
            <span class="status-badge badge-{status}">{status_text}</span>
        </div>
        ''', unsafe_allow_html=True)


# ============================================================================
# SECTION: R√âSUM√â PERFORMANCE
# ============================================================================

st.markdown("### üéØ R√©sum√© Ex√©cutif")

col_summary1, col_summary2, col_summary3 = st.columns([2, 1, 1])

with col_summary1:
    overall_score = performance_summary["overall_score"]
    status = performance_summary["status"]
    
    st.markdown(f'''
    <div class="panel-card">
        <div class="panel-header">
            <span class="panel-icon">üìà</span>
            <h3 class="panel-title">Performance Globale</h3>
        </div>
        <div style="text-align: center;">
            <div style="font-size: 3rem; font-weight: 800; color: #1f2937; margin: 1rem 0;">
                {overall_score:.1%}
            </div>
            <span class="status-badge badge-{status}" style="font-size: 1rem; padding: 0.75rem 1.5rem;">
                {get_performance_status(overall_score, "auc_roc")[1]}
            </span>
        </div>
        <div class="progress-wrapper" style="margin-top: 1.5rem;">
            <div class="progress-bar progress-{status}" style="width: {overall_score*100}%;"></div>
        </div>
    </div>
    ''', unsafe_allow_html=True)

with col_summary2:
    st.markdown(f'''
    <div class="panel-card">
        <div class="panel-header">
            <span class="panel-icon">üöÄ</span>
            <h3 class="panel-title">Production</h3>
        </div>
        <div style="text-align: center; padding: 1rem 0;">
            <div style="font-size: 3rem;">{"‚úÖ" if performance_summary["production_ready"] else "‚ö†Ô∏è"}</div>
            <div style="font-weight: 600; color: #6b7280; margin-top: 0.5rem;">
                {"Pr√™t" if performance_summary["production_ready"] else "Optimisation requise"}
            </div>
        </div>
    </div>
    ''', unsafe_allow_html=True)

with col_summary3:
    risk_colors = {"low": "#10b981", "medium": "#f59e0b", "high": "#ef4444"}
    risk_labels = {"low": "Faible", "medium": "Moyen", "high": "√âlev√©"}
    risk_level = performance_summary["risk_level"]
    
    st.markdown(f'''
    <div class="panel-card">
        <div class="panel-header">
            <span class="panel-icon">üõ°Ô∏è</span>
            <h3 class="panel-title">Risque</h3>
        </div>
        <div style="text-align: center; padding: 1rem 0;">
            <div style="font-size: 2.5rem; color: {risk_colors[risk_level]};">‚óè</div>
            <div style="font-weight: 600; color: {risk_colors[risk_level]}; margin-top: 0.5rem;">
                {risk_labels[risk_level]}
            </div>
        </div>
    </div>
    ''', unsafe_allow_html=True)


# ============================================================================
# SECTION: RADAR CHART
# ============================================================================

st.markdown("### üéØ Analyse Multidimensionnelle")

col_radar1, col_radar2 = st.columns([2, 1])

with col_radar1:
    fig_radar = create_performance_radar(metrics)
    st.plotly_chart(fig_radar, use_container_width=True)

with col_radar2:
    st.markdown("#### üìã D√©tails")
    
    detail_metrics = {
        "Accuracy": metrics.get('accuracy', 0),
        "Specificity": metrics.get('specificity', 0),
        "Erreur Totale": error_analysis['total_errors']
    }
    
    for label, value in detail_metrics.items():
        if isinstance(value, float):
            st.metric(label, f"{value:.3f}")
        else:
            st.metric(label, value)


# ============================================================================
# ONGLETS PRINCIPAUX
# ============================================================================

tabs = st.tabs([
    "üìä M√©triques D√©taill√©es",
    "üîç Analyse des Erreurs",
    "üéØ Mod√®le vs R√©alit√©",  # ‚úÖ NOUVEAU: Visualisation compar√©e
    "üí° Recommandations",
    "üé® Visualisations",
    "üìã Rapport"
])


# TAB 1: M√âTRIQUES D√âTAILL√âES
with tabs[0]:
    st.markdown("### üìä M√©triques Compl√®tes")
    
    if metrics:
        col_metrics1, col_metrics2 = st.columns(2)
        
        with col_metrics1:
            st.markdown("#### üéØ M√©triques de Classification")
            
            classification_metrics = {
                "AUC-ROC": metrics.get('auc_roc', 0),
                "F1-Score": metrics.get('f1_score', 0),
                "Precision": metrics.get('precision', 0),
                "Recall": metrics.get('recall', 0),
                "Accuracy": metrics.get('accuracy', 0),
                "Specificity": metrics.get('specificity', 0)
            }
            
            for metric, value in classification_metrics.items():
                status, status_text = get_performance_status(value, metric.lower().replace('-', '_'))
                st.markdown(f'''
                <div style="display: flex; justify-content: space-between; align-items: center; 
                            padding: 0.75rem; margin: 0.5rem 0; background: #f9fafb; border-radius: 8px;">
                    <span style="font-weight: 600;">{metric}</span>
                    <div>
                        <span style="font-size: 1.25rem; font-weight: 700; margin-right: 0.5rem;">{value:.3f}</span>
                        <span class="status-badge badge-{status}" style="font-size: 0.7rem; padding: 0.25rem 0.5rem;">
                            {status_text}
                        </span>
                    </div>
                </div>
                ''', unsafe_allow_html=True)
        
        with col_metrics2:
            st.markdown("#### üìà M√©triques d'Erreur")
            
            error_metrics = {
                "Faux Positifs": error_analysis['fp_count'],
                "Faux N√©gatifs": error_analysis['fn_count'],
                "Taux FP": f"{error_analysis['fp_rate']:.1%}",
                "Taux FN": f"{error_analysis['fn_rate']:.1%}",
                "Erreurs Totales": error_analysis['total_errors'],
                "Pr√©cisions Correctes": error_analysis['tp_count'] + error_analysis['tn_count']
            }
            
            for metric, value in error_metrics.items():
                st.markdown(f'''
                <div style="display: flex; justify-content: space-between; align-items: center; 
                            padding: 0.75rem; margin: 0.5rem 0; background: #f9fafb; border-radius: 8px;">
                    <span style="font-weight: 600;">{metric}</span>
                    <span style="font-size: 1.25rem; font-weight: 700;">{value}</span>
                </div>
                ''', unsafe_allow_html=True)
    
    # Matrice de confusion
    if 'confusion_matrix' in metrics:
        st.markdown("---")
        st.markdown("#### üìã Matrice de Confusion")
        
        cm = np.array(metrics['confusion_matrix'])
        labels = ["Normal", "Anomalie"]
        
        fig_cm = go.Figure(data=go.Heatmap(
            z=cm,
            x=labels,
            y=labels,
            colorscale='Blues',
            text=cm,
            texttemplate='%{text}',
            textfont={"size": 20},
            showscale=True
        ))
        
        fig_cm.update_layout(
            title="Matrice de Confusion",
            xaxis_title="Pr√©diction",
            yaxis_title="R√©alit√©",
            height=400
        )
        
        st.plotly_chart(fig_cm, use_container_width=True)


# TAB 2: ANALYSE DES ERREURS
with tabs[1]:
    st.markdown("### üîç Analyse D√©taill√©e des Erreurs")
    
    if show_error_analysis:
        # Distribution
        col_err1, col_err2 = st.columns([1, 1])
        
        with col_err1:
            fig_pie = plot_error_distribution(error_analysis)
            st.plotly_chart(fig_pie, use_container_width=True)
        
        with col_err2:
            st.markdown("#### üìä Statistiques")
            
            st.markdown(f'''
            <div class="error-box error-fp">
                <h4 style="margin: 0 0 0.5rem 0;">‚ùå Faux Positifs</h4>
                <div style="font-size: 2rem; font-weight: 800;">{error_analysis['fp_count']}</div>
                <div style="opacity: 0.8;">Taux: {error_analysis['fp_rate']:.1%}</div>
            </div>
            ''', unsafe_allow_html=True)
            
            st.markdown(f'''
            <div class="error-box error-fn">
                <h4 style="margin: 0 0 0.5rem 0;">‚ö†Ô∏è Faux N√©gatifs</h4>
                <div style="font-size: 2rem; font-weight: 800;">{error_analysis['fn_count']}</div>
                <div style="opacity: 0.8;">Taux: {error_analysis['fn_rate']:.1%}</div>
            </div>
            ''', unsafe_allow_html=True)
            
            st.markdown(f'''
            <div class="error-box error-tp">
                <h4 style="margin: 0 0 0.5rem 0;">‚úÖ Pr√©dictions Correctes</h4>
                <div style="font-size: 2rem; font-weight: 800;">
                    {error_analysis['tp_count'] + error_analysis['tn_count']}
                </div>
                <div style="opacity: 0.8;">
                    {(1 - (error_analysis['total_errors'] / len(y_test))):.1%} de pr√©cision
                </div>
            </div>
            ''', unsafe_allow_html=True)
        
        # Exemples d'images
        st.markdown("---")
        st.markdown("#### üñºÔ∏è Exemples d'Erreurs")
        
        if len(error_analysis["false_positives"]) > 0:
            st.markdown("##### ‚ùå Faux Positifs (Normales class√©es comme Anomalies)")
            sample_fp = error_analysis["false_positives"][:min(n_samples_viz, len(error_analysis["false_positives"]))]
            
            cols_fp = st.columns(len(sample_fp))
            for i, idx in enumerate(sample_fp):
                with cols_fp[i]:
                    if len(X_test.shape) > 3:
                        # Normaliser l'image pour affichage
                        img = X_test[idx]
                        img_display = (img - img.min()) / (img.max() - img.min()) if img.max() > img.min() else img
                        st.image(img_display, caption=f"FP #{idx}", use_column_width=True)
                    st.caption(f"Confiance: {y_pred_proba[idx]:.3f}")
        
        if len(error_analysis["false_negatives"]) > 0:
            st.markdown("##### ‚ö†Ô∏è Faux N√©gatifs (Anomalies manqu√©es)")
            sample_fn = error_analysis["false_negatives"][:min(n_samples_viz, len(error_analysis["false_negatives"]))]
            
            cols_fn = st.columns(len(sample_fn))
            for i, idx in enumerate(sample_fn):
                with cols_fn[i]:
                    if len(X_test.shape) > 3:
                        img = X_test[idx]
                        img_display = (img - img.min()) / (img.max() - img.min()) if img.max() > img.min() else img
                        st.image(img_display, caption=f"FN #{idx}", use_column_width=True)
                    st.caption(f"Confiance: {y_pred_proba[idx]:.3f}")


# TAB 3: MOD√àLE VS R√âALIT√â
with tabs[2]:
    st.markdown("### üéØ Visualisation Mod√®le vs R√©alit√©")
    st.markdown("""
    <div style="background: #f0f9ff; padding: 1rem; border-radius: 8px; margin-bottom: 1rem;">
        <p style="margin: 0; color: #0369a1;">
            <strong>üîç Cette section compare ce que le mod√®le voit avec la r√©alit√©:</strong><br>
            ‚Ä¢ <strong>Pr√©diction du mod√®le:</strong> Ce que le mod√®le a d√©tect√©<br>
            ‚Ä¢ <strong>Label r√©el:</strong> La v√©rit√© terrain<br>
            ‚Ä¢ <strong>Heatmaps:</strong> O√π le mod√®le localise les anomalies<br>
            ‚Ä¢ <strong>Type d'erreur:</strong> Classification des types de d√©fauts (si disponible)
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # R√©cup√©ration des heatmaps depuis prediction_results
    heatmaps = prediction_results.get("heatmaps")
    error_maps = prediction_results.get("error_maps")
    binary_masks = prediction_results.get("binary_masks")
    
    has_localization = heatmaps is not None and error_maps is not None
    
    if has_localization:
        st.success(f"‚úÖ Localisation disponible: {len(heatmaps)} images avec heatmaps")
    else:
        st.warning("‚ö†Ô∏è Heatmaps non disponibles pour ce mod√®le")
    
    # S√©lection d'√©chantillons √† visualiser
    st.markdown("---")
    st.markdown("#### üñºÔ∏è √âchantillons D√©taill√©s")
    
    # Cr√©er des cat√©gories d'√©chantillons
    tp_indices = error_analysis["true_positives"]
    tn_indices = error_analysis["true_negatives"]
    fp_indices = error_analysis["false_positives"]
    fn_indices = error_analysis["false_negatives"]
    
    sample_categories = {
        "‚úÖ Vrais Positifs (Anomalies d√©tect√©es correctement)": tp_indices,
        "‚úÖ Vrais N√©gatifs (Normales d√©tect√©es correctement)": tn_indices,
        "‚ùå Faux Positifs (Normales class√©es comme anomalies)": fp_indices,
        "‚ö†Ô∏è Faux N√©gatifs (Anomalies manqu√©es)": fn_indices
    }
    
    for category_name, indices in sample_categories.items():
        if len(indices) == 0:
            continue
        
        st.markdown(f"##### {category_name} ({len(indices)} √©chantillons)")
        
        # S√©lectionner jusqu'√† n_samples_viz √©chantillons
        sample_indices = indices[:n_samples_viz]
        
        # Cr√©er une grille de visualisations
        for idx in sample_indices:
            col_viz1, col_viz2 = st.columns([1, 1])
            
            with col_viz1:
                st.markdown("**üì∑ Image Originale**")
                
                # Affichage image originale
                img = X_test[idx]
                
                # Normalisation pour affichage
                if img.dtype != np.uint8:
                    if img.max() > 1:
                        img_display = img / 255.0
                    else:
                        img_display = img
                    img_display = np.clip(img_display, 0, 1)
                else:
                    img_display = img / 255.0 if img.max() > 1 else img
                
                # Gestion format (channels_first vs channels_last)
                if len(img_display.shape) == 3 and img_display.shape[0] in [1, 3]:
                    # channels_first -> channels_last
                    img_display = np.transpose(img_display, (1, 2, 0))
                elif len(img_display.shape) == 2:
                    img_display = np.stack([img_display, img_display, img_display], axis=-1)
                
                # Conversion grayscale -> RGB si n√©cessaire
                if img_display.shape[-1] == 1:
                    img_display = np.repeat(img_display, 3, axis=-1)
                
                st.image(img_display, use_container_width=True)
                
                # Informations labels
                label_real = y_test[idx]
                pred_real = y_pred_binary[idx]
                proba = y_pred_proba[idx]
                
                st.markdown(f"""
                <div style="background: #f9fafb; padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;">
                    <div style="font-size: 0.9rem;">
                        <strong>Label r√©el:</strong> 
                        <span style="color: {'#10b981' if label_real == 0 else '#ef4444'}; font-weight: 700;">
                            {'‚úÖ Normal' if label_real == 0 else '‚ùå Anomalie'}
                        </span><br>
                        <strong>Pr√©diction mod√®le:</strong> 
                        <span style="color: {'#10b981' if pred_real == 0 else '#ef4444'}; font-weight: 700;">
                            {'‚úÖ Normal' if pred_real == 0 else '‚ùå Anomalie'}
                        </span><br>
                        <strong>Confiance:</strong> {proba:.3f}
                    </div>
                </div>
                """, unsafe_allow_html=True)
            
            with col_viz2:
                st.markdown("**üî• Heatmap de Localisation**")
                
                if has_localization and idx < len(heatmaps):
                    try:
                        # R√©cup√©rer heatmap pour cet index
                        heatmap = heatmaps[idx]
                        error_map = error_maps[idx]
                        
                        # S'assurer que la heatmap a les bonnes dimensions
                        if len(heatmap.shape) == 2:
                            # Aligner heatmap √† l'image si n√©cessaire
                            img_h, img_w = img_display.shape[:2]
                            if heatmap.shape != (img_h, img_w):
                                zoom_factors = (img_h / heatmap.shape[0], img_w / heatmap.shape[1])
                                heatmap = zoom(heatmap, zoom_factors, order=1)
                                error_map = zoom(error_map, zoom_factors, order=1)
                            
                            # Cr√©er visualisation avec Plotly
                            fig_heatmap = go.Figure()
                            
                            # Image de base
                            img_for_plot = (img_display * 255).astype(np.uint8)
                            fig_heatmap.add_trace(go.Image(z=img_for_plot))
                            
                            # Heatmap superpos√©e
                            fig_heatmap.add_trace(go.Heatmap(
                                z=heatmap,
                                colorscale="Jet",
                                opacity=0.6,
                                showscale=True,
                                colorbar=dict(title="Score anomalie")
                            ))
                            
                            fig_heatmap.update_layout(
                                title=f"Localisation Anomalie (Index {idx})",
                                xaxis=dict(visible=False),
                                yaxis=dict(visible=False),
                                height=400,
                                margin=dict(l=0, r=0, t=40, b=0)
                            )
                            
                            st.plotly_chart(fig_heatmap, use_container_width=True)
                            
                            # Informations heatmap
                            max_error = float(error_map.max())
                            mean_error = float(error_map.mean())
                            
                            st.markdown(f"""
                            <div style="background: #f9fafb; padding: 0.75rem; border-radius: 6px; margin-top: 0.5rem;">
                                <div style="font-size: 0.9rem;">
                                    <strong>Erreur max:</strong> {max_error:.4f}<br>
                                    <strong>Erreur moyenne:</strong> {mean_error:.4f}
                                </div>
                            </div>
                            """, unsafe_allow_html=True)
                            
                            # Mask binaire si disponible
                            if binary_masks is not None and idx < len(binary_masks):
                                binary_mask = binary_masks[idx]
                                
                                # Aligner mask si n√©cessaire
                                if binary_mask.shape != (img_h, img_w):
                                    zoom_factors = (img_h / binary_mask.shape[0], img_w / binary_mask.shape[1])
                                    binary_mask = zoom(binary_mask, zoom_factors, order=0)
                                
                                # Afficher le mask binaire
                                st.markdown("**üéØ Masque Binaire (R√©gion d√©tect√©e)**")
                                mask_for_display = (binary_mask * 255).astype(np.uint8)
                                st.image(mask_for_display, use_container_width=True, clamp=True)
                        
                        else:
                            st.warning("Format de heatmap non support√©")
                    
                    except Exception as e:
                        logger.error(f"Erreur g√©n√©ration heatmap pour index {idx}: {e}", exc_info=True)
                        st.warning(f"Impossible de g√©n√©rer heatmap: {str(e)}")
                else:
                    st.info("Heatmap non disponible pour cet √©chantillon")
            
            st.markdown("---")
    
    # R√©sum√© statistique
    st.markdown("---")
    st.markdown("#### üìä R√©sum√© Statistique")
    
    col_stat1, col_stat2, col_stat3, col_stat4 = st.columns(4)
    
    with col_stat1:
        st.metric("Vrais Positifs", len(tp_indices), 
                 f"{len(tp_indices)/max(len(y_test), 1)*100:.1f}%")
    
    with col_stat2:
        st.metric("Vrais N√©gatifs", len(tn_indices),
                 f"{len(tn_indices)/max(len(y_test), 1)*100:.1f}%")
    
    with col_stat3:
        st.metric("Faux Positifs", len(fp_indices),
                 f"{len(fp_indices)/max(len(y_test), 1)*100:.1f}%")
    
    with col_stat4:
        st.metric("Faux N√©gatifs", len(fn_indices),
                 f"{len(fn_indices)/max(len(y_test), 1)*100:.1f}%")


# TAB 4: RECOMMANDATIONS
with tabs[2]:
    st.markdown("### üí° Recommandations Intelligentes")
    
    if show_recommendations and recommendations:
        # Grouper par priorit√©
        high_recs = [r for r in recommendations if r["priority"] == "high"]
        medium_recs = [r for r in recommendations if r["priority"] == "medium"]
        low_recs = [r for r in recommendations if r["priority"] == "low"]
        
        if high_recs:
            st.markdown("#### üî¥ Actions Critiques")
            for rec in high_recs:
                st.markdown(f'''
                <div class="recommendation-card rec-priority-high">
                    <div class="rec-title">
                        <span>{rec['icon']}</span>
                        <span>{rec['action']}</span>
                    </div>
                    <p style="margin: 0.5rem 0 0 0; color: #6b7280;">{rec['message']}</p>
                </div>
                ''', unsafe_allow_html=True)
        
        if medium_recs:
            st.markdown("#### üü° Am√©liorations Recommand√©es")
            for rec in medium_recs:
                st.markdown(f'''
                <div class="recommendation-card rec-priority-medium">
                    <div class="rec-title">
                        <span>{rec['icon']}</span>
                        <span>{rec['action']}</span>
                    </div>
                    <p style="margin: 0.5rem 0 0 0; color: #6b7280;">{rec['message']}</p>
                </div>
                ''', unsafe_allow_html=True)
        
        if low_recs:
            st.markdown("#### üîµ Optimisations")
            for rec in low_recs:
                st.markdown(f'''
                <div class="recommendation-card">
                    <div class="rec-title">
                        <span>{rec['icon']}</span>
                        <span>{rec['action']}</span>
                    </div>
                    <p style="margin: 0.5rem 0 0 0; color: #6b7280;">{rec['message']}</p>
                </div>
                ''', unsafe_allow_html=True)
    
    # Points forts et faibles
    st.markdown("---")
    st.markdown("### üìà Analyse SWOT")
    
    col_swot1, col_swot2 = st.columns(2)
    
    with col_swot1:
        st.markdown("#### ‚úÖ Points Forts")
        strengths = performance_summary.get("strengths", [])
        if strengths:
            for strength in strengths:
                st.success(f"‚úì {strength}")
        else:
            st.info("Analyse en cours...")
    
    with col_swot2:
        st.markdown("#### ‚ö†Ô∏è Points d'Am√©lioration")
        weaknesses = performance_summary.get("weaknesses", [])
        if weaknesses:
            for weakness in weaknesses:
                st.warning(f"‚Üí {weakness}")
        else:
            st.success("Aucun point faible d√©tect√©!")


# TAB 5: VISUALISATIONS
with tabs[4]:
    st.markdown("### üé® Visualisations Avanc√©es")
    
    # Courbes ROC et PR
    st.markdown("#### üìà Courbes de Performance")
    
    col_viz1, col_viz2 = st.columns(2)
    
    with col_viz1:
        # Courbe ROC
        try:
            fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
            
            fig_roc = go.Figure()
            fig_roc.add_trace(go.Scatter(
                x=fpr, y=tpr,
                mode='lines',
                name=f'ROC (AUC={metrics.get("auc_roc", 0):.3f})',
                line=dict(color='#6366f1', width=3)
            ))
            fig_roc.add_trace(go.Scatter(
                x=[0, 1], y=[0, 1],
                mode='lines',
                name='Al√©atoire',
                line=dict(color='gray', dash='dash')
            ))
            
            fig_roc.update_layout(
                title="Courbe ROC",
                xaxis_title="Taux Faux Positifs",
                yaxis_title="Taux Vrais Positifs",
                height=400
            )
            
            st.plotly_chart(fig_roc, use_container_width=True)
        except Exception as e:
            st.warning(f"Impossible de g√©n√©rer courbe ROC: {e}")
    
    with col_viz2:
        # Courbe Precision-Recall
        try:
            precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba)
            
            fig_pr = go.Figure()
            fig_pr.add_trace(go.Scatter(
                x=recall_curve, y=precision_curve,
                mode='lines',
                name='Precision-Recall',
                line=dict(color='#10b981', width=3),
                fill='tonexty'
            ))
            
            fig_pr.update_layout(
                title="Courbe Precision-Recall",
                xaxis_title="Recall",
                yaxis_title="Precision",
                height=400
            )
            
            st.plotly_chart(fig_pr, use_container_width=True)
        except Exception as e:
            st.warning(f"Impossible de g√©n√©rer courbe PR: {e}")
    
    # Distribution des scores
    st.markdown("---")
    st.markdown("#### üìä Distribution des Scores de Confiance")
    
    fig_hist = go.Figure()
    
    fig_hist.add_trace(go.Histogram(
        x=y_pred_proba[y_test == 0],
        name='Normal',
        marker_color='#3b82f6',
        opacity=0.7,
        nbinsx=30
    ))
    
    fig_hist.add_trace(go.Histogram(
        x=y_pred_proba[y_test == 1],
        name='Anomalie',
        marker_color='#ef4444',
        opacity=0.7,
        nbinsx=30
    ))
    
    fig_hist.add_vline(
        x=threshold,
        line_dash="dash",
        line_color="green",
        annotation_text=f"Seuil: {threshold}",
        annotation_position="top"
    )
    
    fig_hist.update_layout(
        title="Distribution des Scores par Classe",
        xaxis_title="Score de Confiance",
        yaxis_title="Fr√©quence",
        barmode='overlay',
        height=400
    )
    
    st.plotly_chart(fig_hist, use_container_width=True)


# TAB 6: RAPPORT
with tabs[5]:
    st.markdown("### üìã Rapport d'√âvaluation")
    
    # R√©sum√© ex√©cutif
    st.markdown("#### üìÑ R√©sum√© Ex√©cutif")
    
    st.markdown(f'''
    <div class="panel-card">
        <p><strong>Date:</strong> {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}</p>
        <p><strong>Type de Mod√®le:</strong> {model_type}</p>
        <p><strong>√âchantillons Test√©s:</strong> {len(X_test):,}</p>
        <p><strong>Score Global:</strong> {performance_summary["overall_score"]:.1%}</p>
        <p><strong>Statut Production:</strong> {"‚úÖ Pr√™t" if performance_summary["production_ready"] else "‚ö†Ô∏è Optimisation requise"}</p>
        <p><strong>Niveau de Risque:</strong> {performance_summary["risk_level"].title()}</p>
    </div>
    ''', unsafe_allow_html=True)
    
    # Tableau r√©capitulatif
    st.markdown("#### üìä Tableau R√©capitulatif")
    
    summary_data = {
        "M√©trique": list(metrics.keys()),
        "Valeur": [f"{v:.3f}" if isinstance(v, float) else str(v) for v in metrics.values()]
    }
    
    summary_df = pd.DataFrame(summary_data)
    st.dataframe(summary_df, use_container_width=True)
    
    # Export
    st.markdown("---")
    st.markdown("#### üíæ Export du Rapport")
    
    col_export1, col_export2, col_export3 = st.columns(3)
    
    with col_export1:
        if st.button("üì• JSON", use_container_width=True):
            report_data = {
                "timestamp": datetime.now().isoformat(),
                "model_type": model_type,
                "threshold": threshold,
                "metrics": {k: float(v) if isinstance(v, (int, float, np.number)) else str(v) 
                           for k, v in metrics.items()},
                "performance_summary": performance_summary,
                "error_analysis": {k: int(v) if isinstance(v, (int, np.integer)) else float(v) if isinstance(v, (float, np.floating)) else v
                                  for k, v in error_analysis.items() if k not in ['false_positives', 'false_negatives', 'true_positives', 'true_negatives']}
            }
            
            json_str = json.dumps(report_data, indent=2, default=str)
            st.download_button(
                "‚¨áÔ∏è T√©l√©charger JSON",
                json_str,
                "evaluation_report.json",
                "application/json",
                use_container_width=True
            )
    
    with col_export2:
        if st.button("üì• CSV", use_container_width=True):
            csv_data = summary_df.to_csv(index=False)
            st.download_button(
                "‚¨áÔ∏è T√©l√©charger CSV",
                csv_data,
                "evaluation_metrics.csv",
                "text/csv",
                use_container_width=True
            )
    
    with col_export3:
        if st.button("üì• Markdown", use_container_width=True):
            md_content = f"""# Rapport d'√âvaluation
            
## R√©sum√©
- **Date**: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}
- **Score Global**: {performance_summary["overall_score"]:.1%}
- **Production Ready**: {"‚úÖ Oui" if performance_summary["production_ready"] else "‚ö†Ô∏è Non"}

## M√©triques
{summary_df.to_markdown(index=False)}

## Recommandations
"""
            for rec in recommendations[:5]:
                md_content += f"\n- **{rec['action']}**: {rec['message']}"
            
            st.download_button(
                "‚¨áÔ∏è T√©l√©charger MD",
                md_content,
                "evaluation_report.md",
                "text/markdown",
                use_container_width=True
            )


# ============================================================================
# FOOTER & NAVIGATION
# ============================================================================

st.markdown("---")

col_nav1, col_nav2, col_nav3, col_nav4 = st.columns(4)

with col_nav1:
    if st.button("üè† Dashboard", use_container_width=True):
        st.switch_page("pages/1_dashboard.py")

with col_nav2:
    if st.button("üîô Entra√Ænement", use_container_width=True):
        st.switch_page("pages/4_training_computer.py")

with col_nav3:
    if st.button("üîÑ Nouvelle √âvaluation", use_container_width=True):
        st.rerun()

with col_nav4:
    if st.button("üíæ Sauvegarder Session", type="primary", use_container_width=True):
        try:
            session_data = {
                "metrics": metrics,
                "error_analysis": error_analysis,
                "performance_summary": performance_summary,
                "timestamp": datetime.now().isoformat()
            }
            
            session_file = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(session_file, 'w') as f:
                json.dump(session_data, f, indent=2, default=str)
            
            st.success(f"‚úÖ Session sauvegard√©e: {session_file}")
        except Exception as e:
            st.error(f"‚ùå Erreur: {str(e)}")

# Footer info
st.markdown("---")
st.caption(f"üïí G√©n√©r√© le {datetime.now().strftime('%d/%m/%Y √† %H:%M:%S')} | DataLab Pro v2.0 Premium")