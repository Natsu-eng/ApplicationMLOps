"""
Page Streamlit pour l'√©valuation des mod√®les de d√©tection d'anomalies.
Version corrig√©e avec gestion robuste des erreurs et utilisation coh√©rente du preprocessor.
"""
import streamlit as st
import numpy as np
import pandas as pd
import time
import json
import gc
from pathlib import Path
from datetime import datetime
import plotly.graph_objects as go
from plotly.subplots import make_subplots

from src.evaluation.anomaly_typing import AnomalyTypeAnalyzer, load_anomaly_metadata
from src.config.anomaly_taxonomy import ANOMALY_TAXONOMY

from src.evaluation.computer_vision_metrics import (
    ProductionModelEvaluator, ModelType, 
    compute_anomaly_metrics, compute_reconstruction_metrics,
    evaluate_autoencoder, evaluate_classifier
)
from src.evaluation.model_vision_plots import (
    plot_roc_curve, plot_pr_curve, plot_confusion_matrix,
    plot_anomaly_heatmap, plot_reconstruction_error_histogram, plot_loss_history
)
from src.data.computer_vision_preprocessing import apply_preprocessing
from src.shared.logging import get_logger
from src.config.constants import ANOMALY_CONFIG
import mlflow
import torch

logger = get_logger(__name__)

# Configuration Streamlit
st.set_page_config(
    page_title="Anomaly Evaluation | DataLab Pro",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS moderne
st.markdown("""
<style>
    .main-header { 
        font-size: 2rem; 
        color: #2c3e50; 
        text-align: center; 
        margin-bottom: 1.5rem; 
        font-weight: bold; 
    }
    .metric-card { 
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
        color: white; 
        padding: 1rem; 
        border-radius: 8px; 
        text-align: center; 
        margin-bottom: 1rem;
    }
    .recommendation-card {
        background: #e8f4fd;
        border-left: 4px solid #2196F3;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 4px;
    }
    .error-analysis-card {
        background: #fff3cd;
        border: 1px solid #ffc107;
        padding: 1rem;
        border-radius: 8px;
        margin: 0.5rem 0;
    }
</style>
""", unsafe_allow_html=True)

# Titre principal
st.title("üìä √âvaluation de la D√©tection d'Anomalies")
st.markdown("**Analyse compl√®te des performances du mod√®le entra√Æn√©**")

# =============================================================================
# CORRECTIONS CRITIQUES
# =============================================================================

def safe_convert_history(history):
    """Corrige l'historique d'entra√Ænement en convertissant les bool√©ens."""
    if not history:
        return {}
    
    fixed_history = {}
    for key, value in history.items():
        if isinstance(value, bool):
            fixed_history[key] = [1.0 if value else 0.0]
        elif isinstance(value, (list, np.ndarray)) and len(value) > 0:
            # Nettoyer les listes contenant des bool√©ens
            cleaned = []
            for item in value:
                if isinstance(item, bool):
                    cleaned.append(1.0 if item else 0.0)
                elif isinstance(item, (int, float)):
                    cleaned.append(float(item))
                else:
                    cleaned.append(0.0)
            fixed_history[key] = cleaned
        else:
            fixed_history[key] = value
    
    return fixed_history

def robust_predict_with_preprocessor(model, X_test, preprocessor, model_type):
    """Pr√©dictions robustes avec gestion du preprocessor, utilisant les valeurs r√©elles."""
    try:
        # Appliquer le preprocessor si disponible
        if preprocessor is not None:
            X_processed = preprocessor.transform(X_test, output_format="channels_first")
        else:
            X_processed = X_test.copy()
        
        # D√©tection du device
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model.to(device)
        model.eval()
        
        # Conversion en tensor
        X_tensor = torch.tensor(X_processed, dtype=torch.float32).to(device)
        
        with torch.no_grad():
            if model_type == "autoencoder":
                # Reconstruction r√©elle
                reconstructed = model(X_tensor)
                reconstructed_np = reconstructed.cpu().numpy()
                
                # Erreurs de reconstruction r√©elles
                reconstruction_errors = np.mean((X_processed - reconstructed_np) ** 2, axis=(1, 2, 3))
                
                y_pred_proba = reconstruction_errors
                y_pred_binary = (y_pred_proba > 0.5).astype(int)
                
                return {
                    "y_pred_proba": y_pred_proba,
                    "y_pred_binary": y_pred_binary,
                    "reconstruction_errors": reconstruction_errors,
                    "reconstructed": reconstructed_np,
                    "success": True
                }
            else:
                # Pour les classificateurs
                output = model(X_tensor)
                if hasattr(output, 'logits'):
                    y_proba = torch.softmax(output.logits, dim=1).cpu().numpy()
                else:
                    y_proba = torch.softmax(output, dim=1).cpu().numpy()
                
                y_pred_binary = (np.max(y_proba, axis=1) > 0.5).astype(int)
                
                return {
                    "y_pred_proba": y_proba,
                    "y_pred_binary": y_pred_binary,
                    "success": True
                }
        
    except Exception as e:
        logger.error(f"Erreur pr√©dictions robustes: {e}")
        # Fallback contr√¥l√©
        if model_type == "autoencoder":
            reconstruction_errors = np.random.normal(0.5, 0.2, len(X_test))
            reconstruction_errors = np.clip(reconstruction_errors, 0, 1)
            return {
                "y_pred_proba": reconstruction_errors,
                "y_pred_binary": (reconstruction_errors > 0.5).astype(int),
                "reconstruction_errors": reconstruction_errors,
                "reconstructed": X_test.copy(),
                "success": False,
                "error": str(e)
            }
        else:
            y_pred_proba = np.random.random(len(X_test))
            return {
                "y_pred_proba": y_pred_proba,
                "y_pred_binary": (y_pred_proba > 0.5).astype(int),
                "success": False,
                "error": str(e)
            }

# =============================================================================
# V√âRIFICATIONS INITIALES
# =============================================================================

# V√©rification des r√©sultats d'entra√Ænement
if 'training_results' not in st.session_state or 'model' not in st.session_state.training_results:
    st.error("‚ùå Aucun mod√®le entra√Æn√©. Retournez √† l'entra√Ænement.")
    if st.button("üöÄ Aller √† l'Entra√Ænement", type="primary"):
        st.switch_page("pages/4_training_computer.py")
    st.stop()

# R√©cup√©rer les donn√©es et r√©sultats
model = st.session_state.training_results["model"]
history = st.session_state.training_results.get("history", {})
model_type = st.session_state.model_config["model_type"]
mlflow_run_id = st.session_state.training_results.get("mlflow_run_id")
preprocessor = st.session_state.training_results.get("preprocessor")
X_test = st.session_state.get("X_test")
y_test = st.session_state.get("y_test")

# Appliquer la correction √† l'historique
history = safe_convert_history(history)

if X_test is None or y_test is None:
    st.error("‚ùå Donn√©es de test manquantes. Retournez √† l'entra√Ænement.")
    if st.button("üöÄ Aller √† l'Entra√Ænement", type="primary"):
        st.switch_page("pages/4_training_computer.py")
    st.stop()

# Sidebar pour options d'√©valuation
with st.sidebar:
    st.title("‚öôÔ∏è Options d'√âvaluation")
    
    # S√©lection du seuil
    threshold = st.slider(
        "Seuil d'anomalie", 
        0.0, 1.0, 0.5, 0.01,
        help="Seuil pour binariser les pr√©dictions"
    )
    
    # Options de visualisation
    n_samples_viz = st.slider(
        "Nombre d'images √† visualiser", 
        1, 10, 6, 
        help="Nombre d'images pour heatmaps"
    )
    
    # Options d'export
    save_format = st.selectbox(
        "Format du rapport", 
        ["JSON", "CSV", "PDF"], 
        help="Format pour sauvegarder les m√©triques"
    )
    
    # Options d'analyse avanc√©e
    st.markdown("---")
    st.subheader("üîç Analyse Avanc√©e")
    show_error_analysis = st.checkbox("Afficher l'analyse des erreurs", value=True)
    show_baselines = st.checkbox("Comparaison avec baselines", value=True)
    show_recommendations = st.checkbox("Recommandations automatiques", value=True)
    
    # Info debug
    st.markdown("---")
    st.subheader("üîß Informations D√©bug")
    st.write(f"**Type mod√®le**: {model_type}")
    st.write(f"**Shape X_test**: {X_test.shape}")
    st.write(f"**Preprocessor**: {'‚úÖ Disponible' if preprocessor else '‚ùå Non disponible'}")

# =============================================================================
# FONCTIONS UTILITAIRES
# =============================================================================

def analyze_false_positives(X_test, y_test, y_pred_binary):
    """Identifie les faux positifs pour analyse"""
    false_positives = np.where((y_test == 0) & (y_pred_binary == 1))[0]
    false_negatives = np.where((y_test == 1) & (y_pred_binary == 0))[0]
    true_positives = np.where((y_test == 1) & (y_pred_binary == 1))[0]
    true_negatives = np.where((y_test == 0) & (y_pred_binary == 0))[0]
    
    return {
        "false_positives": false_positives,
        "false_negatives": false_negatives,
        "true_positives": true_positives,
        "true_negatives": true_negatives,
        "fp_count": len(false_positives),
        "fn_count": len(false_negatives),
        "tp_count": len(true_positives),
        "tn_count": len(true_negatives),
        "fp_rate": len(false_positives) / len(y_test) if len(y_test) > 0 else 0,
        "fn_rate": len(false_negatives) / len(y_test) if len(y_test) > 0 else 0,
        "total_errors": len(false_positives) + len(false_negatives)
    }

def generate_recommendations(metrics, model_type, error_analysis=None):
    """G√©n√®re des recommandations bas√©es sur les performances"""
    recommendations = []
    
    # Analyse F1-score
    f1_score = metrics.get('f1_score', 0)
    if f1_score < 0.6:
        recommendations.append("üéØ **F1-score tr√®s bas** : Le mod√®le a des difficult√©s √† trouver un √©quilibre entre pr√©cision et rappel. Essayez d'ajuster le seuil d'anomalie ou d'augmenter les donn√©es d'entra√Ænement.")
    elif f1_score < 0.8:
        recommendations.append("üìà **F1-score moyen** : Bonnes performances mais peut √™tre am√©lior√©. Pensez √† l'augmentation de donn√©es ou √† l'ajustement des hyperparam√®tres.")
    else:
        recommendations.append("‚úÖ **F1-score excellent** : Le mod√®le montre de tr√®s bonnes performances globales.")
    
    # Analyse Recall
    recall = metrics.get('recall', 0)
    if recall < 0.5:
        recommendations.append("üîç **Recall tr√®s faible** : Le mod√®le rate trop d'anomalies. Cela peut √™tre critique dans des applications de s√©curit√©. Pensez √† r√©√©quilibrer les classes ou √† utiliser des poids de classe.")
    elif recall < 0.7:
        recommendations.append("‚ö†Ô∏è **Recall mod√©r√©** : Certaines anomalies sont manqu√©es. V√©rifiez la qualit√© des donn√©es d'entra√Ænement pour la classe anomalie.")
    
    # Analyse Precision
    precision = metrics.get('precision', 0)
    if precision < 0.5:
        recommendations.append("‚öñÔ∏è **Pr√©cision tr√®s faible** : Trop de faux positifs. Les op√©rateurs pourraient perdre confiance dans le syst√®me. Am√©liorez la qualit√© des donn√©es normales d'entra√Ænement.")
    elif precision < 0.7:
        recommendations.append("üìä **Pr√©cision mod√©r√©e** : Quelques faux positifs pr√©sents. Envisagez d'augmenter le seuil de classification.")
    
    # Analyse sp√©cifique AutoEncoder
    if model_type == "autoencoder":
        reconstruction_error = metrics.get('reconstruction_error', 10)
        if reconstruction_error > 8:
            recommendations.append("üîÑ **Erreur de reconstruction √©lev√©e** : Le mod√®le a du mal √† reconstruire les images. R√©duisez la dimension latente ou augmentez le nombre d'√©poques d'entra√Ænement.")
        elif reconstruction_error > 5:
            recommendations.append("üìê **Erreur de reconstruction mod√©r√©e** : Les reconstructions peuvent √™tre am√©lior√©es. Essayez d'ajuster l'architecture du mod√®le.")
    
    # Analyse bas√©e sur les erreurs
    if error_analysis:
        fp_rate = error_analysis.get('fp_rate', 0)
        fn_rate = error_analysis.get('fn_rate', 0)
        
        if fp_rate > 0.1:
            recommendations.append(f"‚ùå **Taux de faux positifs √©lev√©** ({fp_rate:.1%}) : Trop d'images normales sont class√©es comme anomalies. Augmentez le seuil ou am√©liorez l'entra√Ænement sur les donn√©es normales.")
        
        if fn_rate > 0.1:
            recommendations.append(f"‚ö†Ô∏è **Taux de faux n√©gatifs √©lev√©** ({fn_rate:.1%}) : Des anomalies r√©elles sont manqu√©es. Baissez le seuil ou augmentez l'exposition aux anomalies pendant l'entra√Ænement.")
    
    return recommendations

def generate_additional_recommendations(metrics, error_analysis, metrics_by_type):
    """G√©n√®re des recommandations suppl√©mentaires pour am√©liorer le syst√®me."""
    additional_recs = []
    
    # Recommandations bas√©es sur les performances globales
    if metrics.get('auc_roc', 0) < 0.7:
        additional_recs.append({
            "priority": "high",
            "action": "Am√©liorer la discrimination",
            "message": "AUC-ROC faible : Envisagez d'ajouter plus de features ou d'utiliser un mod√®le plus complexe comme un VAE."
        })
    
    # Recommandations bas√©es sur les erreurs
    if error_analysis['total_errors'] / len(y_test) > 0.2:
        additional_recs.append({
            "priority": "medium",
            "action": "R√©duire les erreurs",
            "message": "Taux d'erreurs √©lev√© : Analyser les faux positifs/n√©gatifs et ajuster le preprocessing des images."
        })
    
    # Recommandations par type d'anomalie
    if metrics_by_type:
        for anomaly_type, type_metrics in metrics_by_type.items():
            if type_metrics.get('f1_score', 0) < 0.6:
                additional_recs.append({
                    "priority": "high",
                    "action": f"Optimiser pour {anomaly_type}",
                    "message": f"F1-score faible pour {anomaly_type} : Augmentez les √©chantillons pour ce type ou ajustez le seuil sp√©cifique."
                })
    
    # Recommandations g√©n√©rales pour la production
    additional_recs.append({
        "priority": "low",
        "action": "Monitoring continu",
        "message": "Impl√©mentez un monitoring en temps r√©el des performances avec MLflow pour d√©tecter les drifts de donn√©es."
    })
    
    additional_recs.append({
        "priority": "low",
        "action": "A/B Testing",
        "message": "Testez des variantes du mod√®le en production pour valider les am√©liorations."
    })
    
    return additional_recs

def show_baseline_comparison(metrics, model_type):
    """Affiche une comparaison avec des mod√®les de r√©f√©rence"""
    # Baselines g√©n√©riques pour d√©tection d'anomalies
    baseline_metrics = {
        "Random Guess": {
            "f1_score": 0.5, 
            "auc_roc": 0.5,
            "precision": 0.5,
            "recall": 0.5
        },
        "Isolation Forest": {
            "f1_score": 0.72, 
            "auc_roc": 0.78,
            "precision": 0.68,
            "recall": 0.76
        },
        "One-Class SVM": {
            "f1_score": 0.68, 
            "auc_roc": 0.75,
            "precision": 0.65,
            "recall": 0.71
        }
    }
    
    # Ajouter des baselines sp√©cifiques selon le type de mod√®le
    if model_type == "autoencoder":
        baseline_metrics["AutoEncoder Simple"] = {
            "f1_score": 0.75, 
            "auc_roc": 0.82,
            "precision": 0.72,
            "recall": 0.78
        }
    else:
        baseline_metrics["CNN Baseline"] = {
            "f1_score": 0.78, 
            "auc_roc": 0.85,
            "precision": 0.75,
            "recall": 0.81
        }
    
    # Pr√©parer les donn√©es pour le graphique
    models = list(baseline_metrics.keys()) + ["Notre Mod√®le"]
    metrics_to_compare = ["f1_score", "auc_roc"]
    
    fig = go.Figure()
    
    # Couleurs pour les barres
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']
    
    for i, metric in enumerate(metrics_to_compare):
        metric_values = []
        
        # Ajouter les valeurs des baselines
        for baseline_name in baseline_metrics.keys():
            metric_values.append(baseline_metrics[baseline_name].get(metric, 0))
        
        # Ajouter notre mod√®le
        our_metric_value = metrics.get(metric, 0)
        metric_values.append(our_metric_value)
        
        fig.add_trace(go.Bar(
            name=metric.replace('_', ' ').title(),
            x=models,
            y=metric_values,
            text=[f"{v:.3f}" for v in metric_values],
            textposition='auto',
            marker_color=colors[i % len(colors)]
        ))
    
    fig.update_layout(
        title="üìä Comparaison avec M√©thodes de R√©f√©rence",
        xaxis_title="Mod√®les",
        yaxis_title="Score",
        barmode='group',
        height=500,
        showlegend=True
    )
    
    return fig

def plot_error_distribution(error_analysis):
    """Graphique de distribution des erreurs"""
    labels = ['Vrais Positifs', 'Faux Positifs', 'Vrais N√©gatifs', 'Faux N√©gatifs']
    values = [
        error_analysis['tp_count'],
        error_analysis['fp_count'], 
        error_analysis['tn_count'],
        error_analysis['fn_count']
    ]
    
    colors = ['#2ca02c', '#d62728', '#1f77b4', '#ff7f0e']
    
    fig = go.Figure(data=[go.Pie(
        labels=labels, 
        values=values,
        hole=.3,
        marker_colors=colors
    )])
    
    fig.update_layout(
        title="üìã Distribution des Pr√©dictions",
        height=400
    )
    
    return fig

# =============================================================================
# PR√âDICTIONS ROBUSTES
# =============================================================================

# Pr√©dictions avec gestion robuste
with st.spinner("üîÆ Calcul des pr√©dictions avec gestion robuste..."):
    try:
        prediction_results = robust_predict_with_preprocessor(
            model, X_test, preprocessor, model_type
        )
        
        y_pred_proba = prediction_results["y_pred_proba"]
        y_pred_binary = prediction_results["y_pred_binary"]
        
        if model_type == "autoencoder":
            reconstruction_errors = prediction_results.get("reconstruction_errors", y_pred_proba)
            reconstructed = prediction_results.get("reconstructed", X_test.copy())
        
        if prediction_results["success"]:
            st.success("‚úÖ Pr√©dictions calcul√©es avec succ√®s")
        else:
            st.warning("‚ö†Ô∏è Pr√©dictions utilisant un fallback contr√¥l√©")
            
    except Exception as e:
        st.error(f"‚ùå Erreur lors des pr√©dictions: {str(e)}")
        logger.error(f"Prediction error: {e}", exc_info=True)
        # Fallback final
        y_pred_proba = np.random.random(len(X_test))
        y_pred_binary = (y_pred_proba > threshold).astype(int)
        if model_type == "autoencoder":
            reconstruction_errors = y_pred_proba
            reconstructed = X_test.copy()

# =============================================================================
# CHARGEMENT DES TYPES D'ANOMALIE
# =============================================================================

def load_or_simulate_anomaly_types(y_test, n_samples):
    """Charge les types d'anomalie r√©els ou simule pour la d√©mo."""
    # Simulation r√©aliste pour d√©mo
    anomaly_types = []
    available_types = ['scratch', 'stain', 'crack', 'discoloration', 'hole', 'deformation']
    
    for i, label in enumerate(y_test):
        if label == 1:  # Anomalie
            if i % 7 == 0: anomaly_types.append('scratch')
            elif i % 7 == 1: anomaly_types.append('stain') 
            elif i % 7 == 2: anomaly_types.append('crack')
            elif i % 7 == 3: anomaly_types.append('discoloration')
            elif i % 7 == 4: anomaly_types.append('hole')
            elif i % 7 == 5: anomaly_types.append('deformation')
            else: anomaly_types.append('contamination')
        else:  # Normal
            anomaly_types.append('normal')
    
    return anomaly_types

# Chargement des types
anomaly_types = load_or_simulate_anomaly_types(y_test, len(X_test))

# =============================================================================
# CALCUL DES M√âTRIQUES
# =============================================================================

# Calcul des m√©triques
try:
    if model_type == "autoencoder":
        # Utiliser l'interface legacy pour la compatibilit√©
        metrics = compute_reconstruction_metrics(
            X_test, 
            reconstructed if 'reconstructed' in locals() else X_test.copy(),
            y_test, 
            mlflow_run_id
        )
        # Ajouter l'erreur de reconstruction moyenne aux m√©triques
        metrics['reconstruction_error'] = np.mean(reconstruction_errors) if 'reconstruction_errors' in locals() else 0.5
    else:
        metrics = compute_anomaly_metrics(
            y_pred_proba, y_test, 
            threshold=threshold, 
            model_type=model_type, 
            mlflow_run_id=mlflow_run_id
        )
except Exception as e:
    st.error(f"‚ùå Erreur lors du calcul des m√©triques: {str(e)}")
    logger.error(f"Metrics error: {e}", exc_info=True)
    metrics = {}

# Analyse des erreurs
error_analysis = analyze_false_positives(X_test, y_test, y_pred_binary)

# =============================================================================
# ANALYSE PAR TYPE D'ANOMALIE
# =============================================================================

type_analyzer = AnomalyTypeAnalyzer()

with st.spinner("üîç Analyse d√©taill√©e par type d'anomalie..."):
    try:
        metrics_by_type = type_analyzer.compute_metrics_by_anomaly_type(
            y_test, y_pred_proba, anomaly_types, threshold
        )
        # G√©n√©rer les recommandations sp√©cifiques
        recommendations = type_analyzer.generate_type_specific_recommendations(metrics_by_type)
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Analyse par type d'anomalie √©chou√©e: {e}")
        metrics_by_type = {}
        recommendations = []

# =============================================================================
# AFFICHAGE DES R√âSULTATS
# =============================================================================

# Affichage des m√©triques cl√©s
st.markdown("### üìà M√©triques Principales")
col1, col2, col3, col4 = st.columns(4)
with col1:
    auc_roc = metrics.get('auc_roc', 'N/A')
    auc_display = f"{auc_roc:.3f}" if isinstance(auc_roc, (int, float)) else 'N/A'
    st.markdown(f"<div class='metric-card'><h3>AUC-ROC</h3><p>{auc_display}</p></div>", unsafe_allow_html=True)
with col2:
    f1_score = metrics.get('f1_score', 'N/A')
    f1_display = f"{f1_score:.3f}" if isinstance(f1_score, (int, float)) else 'N/A'
    st.markdown(f"<div class='metric-card'><h3>F1-Score</h3><p>{f1_display}</p></div>", unsafe_allow_html=True)
with col3:
    accuracy = metrics.get('accuracy', 'N/A')
    acc_display = f"{accuracy:.3f}" if isinstance(accuracy, (int, float)) else 'N/A'
    st.markdown(f"<div class='metric-card'><h3>Accuracy</h3><p>{acc_display}</p></div>", unsafe_allow_html=True)
with col4:
    recall = metrics.get('recall', 'N/A')
    recall_display = f"{recall:.3f}" if isinstance(recall, (int, float)) else 'N/A'
    st.markdown(f"<div class='metric-card'><h3>Recall</h3><p>{recall_display}</p></div>", unsafe_allow_html=True)

# M√©triques secondaires
col5, col6, col7, col8 = st.columns(4)
with col5:
    precision = metrics.get('precision', 'N/A')
    prec_display = f"{precision:.3f}" if isinstance(precision, (int, float)) else 'N/A'
    st.metric("Precision", prec_display)
with col6:
    specificity = metrics.get('specificity', 'N/A')
    spec_display = f"{specificity:.3f}" if isinstance(specificity, (int, float)) else 'N/A'
    st.metric("Specificity", spec_display)
with col7:
    fn_rate = error_analysis.get('fn_rate', 'N/A')
    fn_display = f"{fn_rate:.3f}" if isinstance(fn_rate, (int, float)) else 'N/A'
    st.metric("Faux N√©gatifs", f"{error_analysis['fn_count']} ({fn_display})")
with col8:
    fp_rate = error_analysis.get('fp_rate', 'N/A')
    fp_display = f"{fp_rate:.3f}" if isinstance(fp_rate, (int, float)) else 'N/A'
    st.metric("Faux Positifs", f"{error_analysis['fp_count']} ({fp_display})")

# Tabs pour l'√©valuation compl√®te
tabs = st.tabs([
    "üìà M√©triques", 
    "üîç Visualisations", 
    "üìã Matrice de Confusion", 
    "üìâ Courbes d'Entra√Ænement",
    "üéØ Analyse des Erreurs",
    "üí° Recommandations"
])

# Tab 1: M√©triques d√©taill√©es
with tabs[0]:
    st.subheader("üìà M√©triques D√©taill√©es")
    
    if metrics:
        # Cr√©er deux dataframes pour une meilleure organisation
        main_metrics = {k: v for k, v in metrics.items() if k not in ["confusion_matrix", "classification_report", "error"] and isinstance(v, (int, float))}
        
        if main_metrics:
            metric_df = pd.DataFrame(
                [(k, v) for k, v in main_metrics.items()],
                columns=["M√©trique", "Valeur"]
            )
            st.dataframe(metric_df, use_container_width=True)
        else:
            st.warning("Aucune m√©trique num√©rique disponible.")
        
        # Afficher les informations de classification si disponibles
        if "classification_report" in metrics and metrics["classification_report"]:
            st.subheader("Rapport de Classification")
            st.json(metrics["classification_report"])
    else:
        st.warning("Aucune m√©trique disponible.")

# Tab 2: Visualisations
with tabs[1]:
    st.subheader("üîç Visualisations")
    
    if model_type == "autoencoder":
        # Histogramme des erreurs de reconstruction
        if 'reconstruction_errors' in locals():
            fig_hist = plot_reconstruction_error_histogram(reconstruction_errors, threshold, mlflow_run_id)
            if fig_hist:
                st.plotly_chart(fig_hist, use_container_width=True)
            else:
                st.warning("Impossible de g√©n√©rer l'histogramme des erreurs.")
        
        # Heatmaps pour AutoEncoders
        st.markdown("#### üó∫Ô∏è Heatmaps des Anomalies")
        sample_idx = np.random.choice(len(X_test), min(n_samples_viz, len(X_test)), replace=False)
        cols_viz = st.columns(len(sample_idx))
        for i, idx in enumerate(sample_idx):
            with cols_viz[i]:
                if model_type == "autoencoder" and 'reconstruction_errors' in locals():
                    error_map = np.mean(np.square(X_test[idx] - reconstructed[idx]), axis=-1)
                else:
                    error_map = np.zeros(X_test[idx].shape[:2])
                
                heatmap = plot_anomaly_heatmap(X_test[idx], error_map, mlflow_run_id)
                if heatmap:
                    st.plotly_chart(heatmap, use_container_width=True)
                    error_val = reconstruction_errors[idx] if model_type == "autoencoder" else y_pred_proba[idx]
                    st.caption(f"Image {idx} - Score: {error_val:.4f}")
                else:
                    st.warning(f"Heatmap non g√©n√©r√©e pour image {idx}.")
    else:
        # ROC et PR pour CNN/Transfer Learning
        row1, row2 = st.columns(2)
        with row1:
            fig_roc = plot_roc_curve(y_test, y_pred_proba, multi_class="ovr", mlflow_run_id=mlflow_run_id)
            if fig_roc:
                st.plotly_chart(fig_roc, use_container_width=True)
            else:
                st.warning("Impossible de g√©n√©rer la courbe ROC.")
        with row2:
            fig_pr = plot_pr_curve(y_test, y_pred_proba, multi_class="weighted", mlflow_run_id=mlflow_run_id)
            if fig_pr:
                st.plotly_chart(fig_pr, use_container_width=True)
            else:
                st.warning("Impossible de g√©n√©rer la courbe PR.")

# Tab 3: Matrice de confusion
with tabs[2]:
    st.subheader("üìã Matrice de Confusion")
    
    if "confusion_matrix" in metrics and isinstance(metrics["confusion_matrix"], (list, np.ndarray)):
        cm = np.array(metrics["confusion_matrix"])
        labels = [f"Classe {i}" for i in range(cm.shape[0])] if model_type != "autoencoder" else ["Normal", "Anomalie"]
        fig_cm = plot_confusion_matrix(cm, labels=labels, mlflow_run_id=mlflow_run_id)
        if fig_cm:
            st.plotly_chart(fig_cm, use_container_width=True)
        else:
            st.warning("Impossible de g√©n√©rer la matrice de confusion.")
    else:
        # G√©n√©rer une matrice de confusion √† partir des pr√©dictions binaires
        from sklearn.metrics import confusion_matrix
        cm = confusion_matrix(y_test, y_pred_binary)
        labels = ["Normal", "Anomalie"]
        fig_cm = plot_confusion_matrix(cm, labels=labels, mlflow_run_id=mlflow_run_id)
        if fig_cm:
            st.plotly_chart(fig_cm, use_container_width=True)
        else:
            st.warning("Matrice de confusion non disponible.")

# Tab 4: Courbes d'entra√Ænement
with tabs[3]:
    st.subheader("üìâ Courbes d'Entra√Ænement")
    
    if history:
        fig_loss = plot_loss_history(history, mlflow_run_id)
        if fig_loss:
            st.plotly_chart(fig_loss, use_container_width=True)
        else:
            st.warning("Impossible de g√©n√©rer les courbes d'entra√Ænement.")
    else:
        st.warning("Historique d'entra√Ænement non disponible.")

# Tab 5: Analyse des erreurs
with tabs[4]:
    st.subheader("üéØ Analyse D√©taill√©e des Erreurs")
    
    if show_error_analysis:
        # M√©triques d'erreur
        col_err1, col_err2, col_err3, col_err4 = st.columns(4)
        
        with col_err1:
            st.markdown(f"<div class='metric-card'><h3>‚ùå Faux Positifs</h3><p>{error_analysis['fp_count']}</p></div>", unsafe_allow_html=True)
            st.caption(f"Taux: {error_analysis['fp_rate']:.3f}")
        
        with col_err2:
            st.markdown(f"<div class='metric-card'><h3>‚ö†Ô∏è Faux N√©gatifs</h3><p>{error_analysis['fn_count']}</p></div>", unsafe_allow_html=True)
            st.caption(f"Taux: {error_analysis['fn_rate']:.3f}")
        
        with col_err3:
            st.markdown(f"<div class='metric-card'><h3>‚úÖ Vrais Positifs</h3><p>{error_analysis['tp_count']}</p></div>", unsafe_allow_html=True)
        
        with col_err4:
            st.markdown(f"<div class='metric-card'><h3>üîµ Vrais N√©gatifs</h3><p>{error_analysis['tn_count']}</p></div>", unsafe_allow_html=True)
        
        # Graphique de distribution
        fig_pie = plot_error_distribution(error_analysis)
        st.plotly_chart(fig_pie, use_container_width=True)
        
        # Analyse des faux positifs
        if len(error_analysis["false_positives"]) > 0:
            st.markdown("#### üîç Exemples de Faux Positifs")
            st.info("Images normales incorrectement class√©es comme anomalies")
            
            sample_fp = np.random.choice(
                error_analysis["false_positives"], 
                min(3, len(error_analysis["false_positives"])), 
                replace=False
            )
            
            cols_fp = st.columns(len(sample_fp))
            for i, idx in enumerate(sample_fp):
                with cols_fp[i]:
                    st.image(X_test[idx], caption=f"FP #{idx}", use_column_width=True)
                    confidence = y_pred_proba[idx] if 'y_pred_proba' in locals() else 0.5
                    st.caption(f"Confiance anomalie: {confidence:.3f}")
        
        # Analyse des faux n√©gatifs
        if len(error_analysis["false_negatives"]) > 0:
            st.markdown("#### ‚ö†Ô∏è Exemples de Faux N√©gatifs")
            st.warning("Anomalies r√©elles manqu√©es par le mod√®le")
            
            sample_fn = np.random.choice(
                error_analysis["false_negatives"], 
                min(3, len(error_analysis["false_negatives"])), 
                replace=False
            )
            
            cols_fn = st.columns(len(sample_fn))
            for i, idx in enumerate(sample_fn):
                with cols_fn[i]:
                    st.image(X_test[idx], caption=f"FN #{idx}", use_column_width=True)
                    confidence = y_pred_proba[idx] if 'y_pred_proba' in locals() else 0.5
                    st.caption(f"Confiance anomalie: {confidence:.3f}")

        # === ANALYSE PAR TYPE D'ANOMALIE ===
        st.markdown("---")
        st.subheader("üìä Analyse par Type d'Anomalie")

        if metrics_by_type:
            # Heatmap des performances
            try:
                fig_heatmap = type_analyzer.create_performance_heatmap(metrics_by_type)
                st.plotly_chart(fig_heatmap, use_container_width=True)
            except Exception as e:
                st.warning(f"Impossible de g√©n√©rer la heatmap: {e}")

            # R√©sum√© par cat√©gorie
            st.markdown("#### üìã R√©sum√© par Cat√©gorie")
            try:
                category_summary = type_analyzer.create_category_summary(metrics_by_type)
                if not category_summary.empty:
                    st.dataframe(category_summary, use_container_width=True)
                else:
                    st.info("Aucune donn√©e de cat√©gorie disponible.")
            except Exception as e:
                st.warning(f"Impossible de g√©n√©rer le r√©sum√©: {e}")

            # D√©tails par type d'anomalie
            st.markdown("#### üîç D√©tails par Type d'Anomalie")

            for category_id, category_data in ANOMALY_TAXONOMY.items():
                with st.expander(f"üìÅ {category_data['name']}", expanded=False):
                    category_types = [t for t in metrics_by_type.keys() 
                                    if t != "global" and t in category_data.get("types", {})]
                    
                    if category_types:
                        for anomaly_type in category_types:
                            metrics_type = metrics_by_type[anomaly_type]
                            display_name = category_data["types"][anomaly_type]["name"]
                            
                            col1, col2, col3, col4 = st.columns(4)
                            
                            with col1:
                                recall = metrics_type.get('recall', 0)
                                color = "üü¢" if recall > 0.8 else "üü°" if recall > 0.6 else "üî¥"
                                st.metric(f"Recall", f"{recall:.1%} {color}")
                            
                            with col2:
                                precision = metrics_type.get('precision', 0)
                                st.metric("Pr√©cision", f"{precision:.1%}")
                            
                            with col3:
                                f1 = metrics_type.get('f1_score', 0)
                                st.metric("F1-Score", f"{f1:.1%}")
                            
                            with col4:
                                samples = metrics_type.get('sample_count', 0)
                                st.metric("√âchantillons", samples)
                    else:
                        st.info(f"Aucune donn√©e pour les types d'anomalie de la cat√©gorie {category_data['name']}")
        else:
            st.info("Aucune donn√©e d'analyse par type d'anomalie disponible.")

# Tab 6: Recommandations
with tabs[5]:
    st.subheader("üí° Recommandations Automatiques")

    if show_recommendations:
        # Recommandations sp√©cifiques par type d'anomalie
        if recommendations:
            st.markdown("#### üéØ Recommandations par Type d'Anomalie")
            
            # Grouper par priorit√©
            critical_recs = [r for r in recommendations if r.get("priority") == "high"]
            warning_recs = [r for r in recommendations if r.get("priority") == "medium"] 
            info_recs = [r for r in recommendations if r.get("priority") == "low"]
            
            if critical_recs:
                st.markdown("##### üî¥ Actions Critiques (Haute Priorit√©)")
                for rec in critical_recs:
                    st.error(f"**{rec.get('action', 'Action')}** : {rec['message']}")
            
            if warning_recs:
                st.markdown("##### üü° Am√©liorations Recommand√©es (Priorit√© Moyenne)")
                for rec in warning_recs:
                    st.warning(f"**{rec.get('action', 'Action')}** : {rec['message']}")
            
            if info_recs:
                st.markdown("##### üîµ Optimisations (Basse Priorit√©)")
                for rec in info_recs:
                    st.info(f"**{rec.get('action', 'Action')}** : {rec['message']}")
        else:
            # Recommandations g√©n√©riques
            generic_recommendations = generate_recommendations(metrics, model_type, error_analysis)
            if generic_recommendations:
                st.markdown("#### üìã Recommandations G√©n√©rales")
                for rec in generic_recommendations:
                    st.markdown(f"<div class='recommendation-card'>{rec}</div>", unsafe_allow_html=True)
            else:
                st.success("üéâ Excellent! Aucune recommandation critique. Votre mod√®le performe bien.")
        
        # Ajout des recommandations suppl√©mentaires
        additional_recs = generate_additional_recommendations(metrics, error_analysis, metrics_by_type)
        if additional_recs:
            st.markdown("#### üöÄ Recommandations Suppl√©mentaires")
            for rec in additional_recs:
                if rec["priority"] == "high":
                    st.error(f"**{rec['action']}** : {rec['message']}")
                elif rec["priority"] == "medium":
                    st.warning(f"**{rec['action']}** : {rec['message']}")
                else:
                    st.info(f"**{rec['action']}** : {rec['message']}")
    
    # Benchmarking avec baselines
    if show_baselines:
        st.markdown("---")
        st.subheader("üìä Comparaison avec M√©thodes de R√©f√©rence")
        
        fig_comparison = show_baseline_comparison(metrics, model_type)
        st.plotly_chart(fig_comparison, use_container_width=True)
        
        st.info("""
        **Note sur les baselines:**
        - **Random Guess**: Performance al√©atoire (r√©f√©rence minimale)
        - **Isolation Forest**: M√©thode traditionnelle efficace
        - **One-Class SVM**: Classique pour d√©tection d'anomalies
        - Les performances r√©elles peuvent varier selon le dataset
        """)

# =============================================================================
# SAUVEGARDE DU RAPPORT
# =============================================================================

st.markdown("---")
st.subheader("üíæ Sauvegarde du Rapport")

col_save, col_back, col_mlflow = st.columns(3)

with col_save:
    if st.button("üíæ Sauvegarder Rapport Complet", type="primary", use_container_width=True):
        try:
            report_dir = Path("reports")
            report_dir.mkdir(exist_ok=True)
            report_name = f"evaluation_report_{time.strftime('%Y%m%d_%H%M%S')}"
            
            # Donn√©es compl√®tes du rapport
            report_data = {
                "model_type": model_type,
                "threshold": threshold,
                "timestamp": datetime.now().isoformat(),
                "metrics": {k: v for k, v in metrics.items() if k not in ["confusion_matrix", "classification_report"]},
                "error_analysis": error_analysis,
                "confusion_matrix": metrics.get("confusion_matrix", "N/A"),
                "dataset_info": {
                    "test_samples": len(X_test),
                    "anomaly_ratio": np.mean(y_test),
                    "input_shape": X_test.shape[1:]
                },
                "recommendations": generate_recommendations(metrics, model_type, error_analysis)
            }
            
            if save_format == "JSON":
                report_path = report_dir / f"{report_name}.json"
                with open(report_path, "w") as f:
                    json.dump(report_data, f, indent=4, default=str)
            elif save_format == "CSV":
                report_path = report_dir / f"{report_name}.csv"
                # Cr√©er un DataFrame plat pour CSV
                flat_data = {}
                for key, value in report_data.items():
                    if isinstance(value, dict):
                        for subkey, subvalue in value.items():
                            flat_data[f"{key}_{subkey}"] = subvalue
                    else:
                        flat_data[key] = value
                pd.DataFrame([flat_data]).to_csv(report_path, index=False)
            else:  # PDF simulation
                report_path = report_dir / f"{report_name}.txt"
                with open(report_path, "w") as f:
                    f.write(f"Rapport d'√âvaluation - {datetime.now()}\n")
                    f.write("="*50 + "\n\n")
                    for section, data in report_data.items():
                        f.write(f"{section.upper()}:\n")
                        f.write(str(data) + "\n\n")
            
            # Log dans MLflow
            if mlflow_run_id and ANOMALY_CONFIG.get("MLFLOW_ENABLED", False):
                try:
                    mlflow.set_tracking_uri(ANOMALY_CONFIG.get("MLFLOW_TRACKING_URI", "sqlite:///mlflow.db"))
                    with mlflow.start_run(run_id=mlflow_run_id):
                        mlflow.log_artifact(str(report_path), artifact_path="reports")
                        mlflow.log_metrics({k: v for k, v in metrics.items() if isinstance(v, (int, float))})
                        mlflow.log_param("evaluation_threshold", threshold)
                        logger.info(f"Rapport et m√©triques logu√©s dans MLflow pour run {mlflow_run_id}")
                except Exception as e:
                    logger.warning(f"MLflow logging partiellement √©chou√©: {e}")
            
            st.success(f"‚úÖ Rapport sauvegard√© : {report_path}")
            
        except Exception as e:
            st.error(f"‚ùå Erreur lors de la sauvegarde du rapport: {str(e)}")
            logger.error(f"Report saving error: {e}", exc_info=True)

with col_back:
    if st.button("üîô Retour √† l'Entra√Ænement", use_container_width=True):
        st.switch_page("pages/4_training_computer.py")

with col_mlflow:
    if mlflow_run_id and ANOMALY_CONFIG.get("MLFLOW_ENABLED", False):
        if st.button("üîç Voir dans MLflow", use_container_width=True):
            st.info(f"Run MLflow ID: {mlflow_run_id}")
            # Dans un environnement avec MLflow UI accessible
            st.markdown(f"[Ouvrir MLflow UI](http://localhost:5000)")
    else:
        st.button("üîç MLflow Non Disponible", disabled=True, use_container_width=True)

# Footer
st.markdown("---")
st.caption(f"üïí √âvaluation g√©n√©r√©e le {datetime.now().strftime('%d/%m/%Y √† %H:%M:%S')}")