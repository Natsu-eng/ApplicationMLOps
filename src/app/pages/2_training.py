"""
Page de configuration et d'entra√Ænement ML.
"""
import logging
import mlflow
import streamlit as st
import pandas as pd
import time
import os
import gc
from typing import Dict
import concurrent.futures

# Imports des modules ML
from src.models.catalog import MODEL_CATALOG
from src.data.data_analysis import detect_imbalance, auto_detect_column_types
from src.models.training import is_mlflow_available
from src.shared.logging import get_logger
from src.config.constants import (
    VALIDATION_CONSTANTS, PREPROCESSING_CONSTANTS, TRAINING_CONSTANTS,
    MLFLOW_CONSTANTS, VISUALIZATION_CONSTANTS
)

# Nouveaux imports pour la structure r√©organis√©e
from helpers.data_validators import DataValidator
from monitoring.state_managers import MLStateManager
from helpers.training_helpers import TrainingHelpers
from helpers.task_detection import safe_get_task_type
from utils.errors_handlers import safe_train_models
from utils.system_utils import get_system_metrics, check_system_resources

# Configuration
logger = get_logger(__name__)
st.set_page_config(page_title="Configuration ML", page_icon="‚öôÔ∏è", layout="wide")

# Initialisation centralis√©e de session_state
session_state_defaults = {
    'warnings': [],
    'current_step': 1,
    'previous_task_type': None,
    'task_type': 'classification',
    'target_column_for_ml_config': None,
    'feature_list_for_ml_config': [],
    'selected_models_for_training': [],
    'test_split_for_ml_config': 20,
    'optimize_hp_for_ml_config': False,
    'preprocessing_choices': {
        'numeric_imputation': PREPROCESSING_CONSTANTS["NUMERIC_IMPUTATION_DEFAULT"],
        'categorical_imputation': PREPROCESSING_CONSTANTS["CATEGORICAL_IMPUTATION_DEFAULT"],
        'remove_constant_cols': True,
        'remove_identifier_cols': True,
        'scale_features': True,
        'pca_preprocessing': False,
        'use_smote': False,
        'smote_k_neighbors': 5,
        'smote_sampling_strategy': 'auto'
    },
    'ml_training_in_progress': False,
    'ml_last_training_time': None,
    'ml_error_count': 0,
    'ml_config_setup_done': False,
    'ml_results': None,
    'mlflow_runs': [],
    'model_performance_history': []
}
for key, value in session_state_defaults.items():
    if key not in st.session_state:
        st.session_state[key] = value

def log_structured(level: str, message: str, extra: Dict = None):
    """Fonction de journalisation structur√©e avec format texte clair."""
    try:
        log_message = f"{message}"
        if extra:
            extra_str = " ".join([f"[{key}: {value}]" for key, value in extra.items()])
            log_message = f"{log_message} {extra_str}"
        logger.log(getattr(logging, level.upper()), log_message)
    except Exception as e:
        logger.error(f"Erreur lors de la journalisation structur√©e: {str(e)[:100]}")

def setup_ml_config_environment():
    """Configuration robuste pour l'environnement de production ML"""
    if not st.session_state.ml_config_setup_done:
        st.session_state.ml_config_setup_done = True
        
        if is_mlflow_available():
            try:
                mlflow.set_tracking_uri(MLFLOW_CONSTANTS["TRACKING_URI"])
                try:
                    experiments = mlflow.search_experiments()
                    log_structured("INFO", f"MLflow connect√© - {len(experiments)} exp√©riences")
                except Exception as conn_error:
                    log_structured("WARNING", f"MLflow connect√© mais erreur recherche: {str(conn_error)[:100]}")
                
                experiment = mlflow.get_experiment_by_name(MLFLOW_CONSTANTS["EXPERIMENT_NAME"])
                if experiment is None:
                    try:
                        mlflow.create_experiment(MLFLOW_CONSTANTS["EXPERIMENT_NAME"])
                        log_structured("INFO", f"Exp√©rience cr√©√©e: {MLFLOW_CONSTANTS['EXPERIMENT_NAME']}")
                    except Exception as create_error:
                        log_structured("ERROR", f"√âchec cr√©ation exp√©rience: {str(create_error)[:100]}")
                else:
                    log_structured("INFO", f"Exp√©rience existante: {experiment.name}")
            except Exception as e:
                log_structured("ERROR", f"√âchec configuration MLflow: {str(e)[:100]}")

# Interface principale
st.title("‚öôÔ∏è Configuration de l'Exp√©rimentation ML")
st.markdown("Configurez votre analyse en 4 √©tapes simples et lancez l'entra√Ænement des mod√®les.")

# V√©rification des donn√©es
if 'df' not in st.session_state or st.session_state.df is None:
    st.error("üìä Aucun dataset charg√©")
    st.info("Chargez un dataset depuis la page d'accueil.")
    if st.button("üè† Retour √† l'accueil"):
        st.switch_page("app.py")
    st.stop()

df = st.session_state.df

# Validation DataFrame
validation_result = DataValidator.validate_dataframe_for_ml(df)
if not validation_result["is_valid"]:
    st.error("‚ùå Dataset non compatible avec l'analyse ML")
    with st.expander("üîç D√©tails des probl√®mes", expanded=True):
        for issue in validation_result["issues"]:
            st.error(f"‚Ä¢ {issue}")
        st.info(f"**Crit√®res requis**:\n- Minimum {VALIDATION_CONSTANTS['MIN_ROWS_REQUIRED']} lignes\n- Minimum {VALIDATION_CONSTANTS['MIN_COLS_REQUIRED']} colonnes\n- Moins de {VALIDATION_CONSTANTS['MAX_MISSING_RATIO']*100:.0f}% de valeurs manquantes")
    if st.button("üîÑ Rev√©rifier"):
        st.rerun()
    st.stop()

if validation_result["warnings"]:
    with st.expander("‚ö†Ô∏è Avertissements qualit√© donn√©es", expanded=False):
        for warning in validation_result["warnings"]:
            st.warning(f"‚Ä¢ {warning}")
    st.session_state.warnings.extend(validation_result["warnings"])

# Initialisation √©tat
MLStateManager.initialize_ml_config()

# M√©triques dataset
st.markdown("### üìä Aper√ßu du Dataset")
col1, col2, col3, col4, col5 = st.columns(5)
with col1:
    st.metric("üìè Lignes", f"{validation_result['stats']['n_rows']:,}")
with col2:
    st.metric("üìã Colonnes", validation_result["stats"]["n_cols"])
with col3:
    memory_mb = validation_result["stats"].get("memory_mb", 0)
    st.metric("üíæ M√©moire", f"{memory_mb:.1f} MB" if memory_mb > 0 else "N/A")
with col4:
    missing_pct = df.isnull().mean().mean() * 100
    st.metric("üï≥Ô∏è Manquant", f"{missing_pct:.1f}%")
with col5:
    sys_metrics = get_system_metrics()
    color = "üü¢" if sys_metrics["memory_percent"] < 70 else "üü°" if sys_metrics["memory_percent"] < TRAINING_CONSTANTS["HIGH_MEMORY_THRESHOLD"] else "üî¥"
    st.metric(f"{color} RAM Sys", f"{sys_metrics['memory_percent']:.0f}%")

st.markdown("---")

# Navigation par √©tapes
steps = ["üéØ Cible", "üîß Pr√©process", "ü§ñ Mod√®les", "üöÄ Lancement"]
st.radio("√âtapes", steps, index=st.session_state.current_step - 1, horizontal=True, key="step_selector")
st.session_state.current_step = steps.index(st.session_state.get('step_selector', steps[0])) + 1

# √âtape 1: Configuration cible
if st.session_state.current_step == 1:
    st.header("üéØ Configuration de la T√¢che et Cible")
    
    task_options = ["Classification Supervis√©e", "R√©gression Supervis√©e", "Clustering Non Supervis√©"]
    task_descriptions = {
        "Classification Supervis√©e": "Pr√©dire des cat√©gories (ex: spam/non-spam)",
        "R√©gression Supervis√©e": "Pr√©dire des valeurs num√©riques (ex: prix, score)", 
        "Clustering Non Supervis√©": "D√©couvrir des groupes naturels dans les donn√©es"
    }
    
    current_task_idx = {'classification': 0, 'regression': 1, 'clustering': 2}.get(st.session_state.task_type, 0)
    task_selection = st.selectbox(
        "Type de probl√®me",
        options=task_options,
        index=current_task_idx,
        key="ml_task_selection",
        help="S√©lectionnez le type d'apprentissage adapt√© √† vos donn√©es"
    )
    st.info(f"**{task_selection}** - {task_descriptions[task_selection]}")
    
    selected_task_type = {
        "Classification Supervis√©e": "classification",
        "R√©gression Supervis√©e": "regression", 
        "Clustering Non Supervis√©": "clustering"
    }[task_selection]
    
    if st.session_state.previous_task_type != selected_task_type:
        st.session_state.target_column_for_ml_config = None
        st.session_state.feature_list_for_ml_config = []
        st.session_state.preprocessing_choices['use_smote'] = False
        st.session_state.previous_task_type = selected_task_type
        st.session_state.task_type = selected_task_type
        st.session_state.warnings = []
        log_structured("INFO", f"Changement de type de t√¢che: {selected_task_type}")
        st.rerun()
    else:
        st.session_state.task_type = selected_task_type
    
    if selected_task_type in ['classification', 'regression']:
        st.subheader("üéØ Variable Cible (Y)")
        available_targets = (
            [col for col in df.columns if df[col].nunique() <= TRAINING_CONSTANTS["MAX_CLASSES"] or not pd.api.types.is_numeric_dtype(df[col])]
            if selected_task_type == 'classification' else
            [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and df[col].nunique() > VALIDATION_CONSTANTS["MIN_UNIQUE_VALUES"]]
        )
        
        if not available_targets:
            st.error("‚ùå Aucune variable cible appropri√©e trouv√©e")
            st.info(f"Classification: ‚â§{TRAINING_CONSTANTS['MAX_CLASSES']} valeurs uniques\nR√©gression: num√©rique, >{VALIDATION_CONSTANTS['MIN_UNIQUE_VALUES']} valeurs uniques")
            st.session_state.warnings.append("Aucune variable cible appropri√©e")
        else:
            available_targets = [None] + available_targets
            target_idx = available_targets.index(st.session_state.target_column_for_ml_config) if st.session_state.target_column_for_ml_config in available_targets else 0
            target_column = st.selectbox(
                "Variable √† pr√©dire",
                options=available_targets,
                index=target_idx,
                key="ml_target_selector",
                help="Variable que le mod√®le apprendra √† pr√©dire"
            )
            
            if target_column != st.session_state.target_column_for_ml_config:
                st.session_state.target_column_for_ml_config = target_column
                st.session_state.feature_list_for_ml_config = []
            
            if target_column:
                task_info = safe_get_task_type(df, target_column)
                if task_info["error"]:
                    st.error(f"‚ùå Erreur: {task_info['error']}")
                    st.info("Action: S√©lectionnez une autre colonne ou v√©rifiez les donn√©es.")
                    st.session_state.warnings.append(task_info["error"])
                else:
                    if selected_task_type == "classification":
                        st.success(f"‚úÖ **Classification** ({task_info['n_classes']} classes)")
                        class_dist = df[target_column].value_counts()
                        if len(class_dist) <= 10:
                            st.bar_chart(class_dist, height=300, color=VISUALIZATION_CONSTANTS["BAR_CHART_COLOR"])
                            st.caption(f"Distribution des classes")
                        imbalance_info = detect_imbalance(df, target_column)
                        if imbalance_info.get("is_imbalanced", False):
                            st.warning(f"‚ö†Ô∏è D√©s√©quilibre (ratio: {imbalance_info.get('imbalance_ratio', 'N/A'):.2f})")
                            st.session_state.warnings.append(f"D√©s√©quilibre classes (ratio: {imbalance_info['imbalance_ratio']:.2f})")
                        if task_info["warnings"]:
                            st.session_state.warnings.extend(task_info["warnings"])
                    else:
                        st.success("‚úÖ **R√©gression**")
                        target_stats = df[target_column].describe()
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("Moyenne", f"{target_stats['mean']:.3f}")
                        with col2:
                            st.metric("M√©diane", f"{target_stats['50%']:.3f}")
                        with col3:
                            st.metric("√âcart-type", f"{target_stats['std']:.3f}")
                        with col4:
                            st.metric("Plage", f"{target_stats['max'] - target_stats['min']:.3f}")
        
        st.subheader("üìä Variables Explicatives (X)")
        all_features = [col for col in df.columns if col != target_column] if target_column else list(df.columns)
        
        if all_features:
            recommend_features = st.checkbox(
                "S√©lection automatique des features",
                value=len(st.session_state.feature_list_for_ml_config) == 0,
                help="S√©lectionne automatiquement les variables pertinentes"
            )
            
            if recommend_features and target_column:
                with st.spinner("ü§ñ Analyse des features..."):
                    column_types = auto_detect_column_types(df)
                    recommended_features = column_types.get('numeric', []) + [
                        col for col in column_types.get('categorical', []) if df[col].nunique() <= VALIDATION_CONSTANTS["MAX_CATEGORICAL_UNIQUE"]
                    ]
                    recommended_features = [col for col in recommended_features if col != target_column and col in all_features][:TRAINING_CONSTANTS["MAX_FEATURES"]]
                    st.session_state.feature_list_for_ml_config = recommended_features
                    st.success(f"‚úÖ {len(recommended_features)} features s√©lectionn√©es")
                    log_structured("INFO", "Features auto-s√©lectionn√©es", {"n_features": len(recommended_features)})
            else:
                selected_features = st.multiselect(
                    "Variables d'entr√©e",
                    options=all_features,
                    default=st.session_state.feature_list_for_ml_config,
                    key="ml_features_selector",
                    help="Variables utilis√©es pour la pr√©diction"
                )
                st.session_state.feature_list_for_ml_config = selected_features
            
            if st.session_state.feature_list_for_ml_config:
                st.success(f"‚úÖ {len(st.session_state.feature_list_for_ml_config)} features s√©lectionn√©es")
                st.caption(f"üìã {', '.join(st.session_state.feature_list_for_ml_config[:10])}{' ...' if len(st.session_state.feature_list_for_ml_config) > 10 else ''}")
                if len(st.session_state.feature_list_for_ml_config) > TRAINING_CONSTANTS["MAX_FEATURES"]:
                    st.warning("‚ö†Ô∏è Nombre √©lev√© de features - risque de surapprentissage")
                    st.session_state.warnings.append("Nombre √©lev√© de features")
                    st.info("Action: R√©duisez le nombre de features ou activez PCA.")
            else:
                st.warning("‚ö†Ô∏è Aucune feature s√©lectionn√©e")
                st.info("Action: S√©lectionnez au moins une variable.")
                st.session_state.warnings.append("Aucune feature s√©lectionn√©e")
        else:
            st.error("‚ùå Aucune feature disponible")
            st.info("Action: V√©rifiez votre dataset.")
            st.session_state.warnings.append("Aucune feature disponible")
    
    else:  # Clustering
        st.session_state.target_column_for_ml_config = None
        st.success("‚úÖ **Clustering Non Supervis√©**")
        st.info("üîç Le mod√®le identifiera des groupes naturels dans les donn√©es.")
        
        all_numeric_features = df.select_dtypes(include=['number']).columns.tolist()
        if not all_numeric_features:
            st.error("‚ùå Aucune variable num√©rique pour le clustering")
            st.info("Action: Ajoutez des variables num√©riques au dataset.")
            st.session_state.warnings.append("Aucune variable num√©rique pour clustering")
        else:
            st.subheader("üìä Variables pour le Clustering")
            auto_cluster_features = st.checkbox(
                "S√©lection automatique",
                value=len(st.session_state.feature_list_for_ml_config) == 0,
                help="S√©lectionne les variables num√©riques adapt√©es"
            )
            
            if auto_cluster_features:
                validation_result = DataValidator.validate_clustering_features(df, all_numeric_features)
                st.session_state.feature_list_for_ml_config = validation_result["valid_features"]
                if validation_result["suggested_features"]:
                    st.info(f"üí° Suggestion: {', '.join(validation_result['suggested_features'][:5])}")
                if validation_result["warnings"]:
                    with st.expander("‚ö†Ô∏è Avertissements", expanded=True):
                        for warning in validation_result["warnings"]:
                            st.warning(f"‚Ä¢ {warning}")
                    st.session_state.warnings.extend(validation_result["warnings"])
                st.success(f"‚úÖ {len(st.session_state.feature_list_for_ml_config)} variables s√©lectionn√©es")
                log_structured("INFO", "Variables clustering auto-s√©lectionn√©es", {"n_features": len(st.session_state.feature_list_for_ml_config)})
            else:
                clustering_features = st.multiselect(
                    "Variables pour clustering",
                    options=all_numeric_features,
                    default=st.session_state.feature_list_for_ml_config or all_numeric_features[:10],
                    key="clustering_features_selector",
                    help="Variables num√©riques pour identifier les clusters"
                )
                validation_result = DataValidator.validate_clustering_features(df, clustering_features)
                st.session_state.feature_list_for_ml_config = validation_result["valid_features"]
                if validation_result["suggested_features"]:
                    st.info(f"üí° Suggestion: {', '.join(validation_result['suggested_features'][:5])}")
                if validation_result["warnings"]:
                    with st.expander("‚ö†Ô∏è Avertissements", expanded=True):
                        for warning in validation_result["warnings"]:
                            st.warning(f"‚Ä¢ {warning}")
                    st.session_state.warnings.extend(validation_result["warnings"])
            
            if st.session_state.feature_list_for_ml_config:
                st.success(f"‚úÖ {len(st.session_state.feature_list_for_ml_config)} variables s√©lectionn√©es")
                if len(st.session_state.feature_list_for_ml_config) < VALIDATION_CONSTANTS["MIN_COLS_REQUIRED"]:
                    st.warning(f"‚ö†Ô∏è Minimum {VALIDATION_CONSTANTS['MIN_COLS_REQUIRED']} variables pour clustering")
                    st.session_state.warnings.append(f"Moins de {VALIDATION_CONSTANTS['MIN_COLS_REQUIRED']} variables pour clustering")
                elif len(st.session_state.feature_list_for_ml_config) > TRAINING_CONSTANTS["MAX_FEATURES"]:
                    st.warning("‚ö†Ô∏è Nombre √©lev√© de variables - risque de mal√©diction dimensionnelle")
                    st.session_state.warnings.append("Nombre √©lev√© de variables pour clustering")
                    st.info("Action: Activez PCA ou r√©duisez les variables.")
                with st.expander("üìà Aper√ßu statistiques", expanded=False):
                    st.dataframe(df[st.session_state.feature_list_for_ml_config].describe().style.format("{:.3f}"), use_container_width=True)
            else:
                st.warning("‚ö†Ô∏è Aucune variable s√©lectionn√©e")
                st.info("Action: S√©lectionnez des variables num√©riques.")
                st.session_state.warnings.append("Aucune variable s√©lectionn√©e pour clustering")

# √âtape 2: Pr√©traitement
elif st.session_state.current_step == 2:
    st.header("üîß Configuration du Pr√©traitement")
    task_type = st.session_state.get('task_type', 'classification')
    
    st.info(f"**Pipeline pour {task_type.upper()}**: Transformations appliqu√©es s√©par√©ment sur train/validation pour √©viter le data leakage.")
    
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("üß© Valeurs Manquantes")
        st.session_state.preprocessing_choices['numeric_imputation'] = st.selectbox(
            "Variables num√©riques",
            options=['mean', 'median', 'constant', 'knn'],
            index=['mean', 'median', 'constant', 'knn'].index(st.session_state.preprocessing_choices.get('numeric_imputation', PREPROCESSING_CONSTANTS["NUMERIC_IMPUTATION_DEFAULT"])),
            key='numeric_imputation_selector',
            help="mean=moyenne, median=m√©diane, constant=0, knn=k-voisins"
        )
        st.session_state.preprocessing_choices['categorical_imputation'] = st.selectbox(
            "Variables cat√©gorielles",
            options=['most_frequent', 'constant'],
            index=['most_frequent', 'constant'].index(st.session_state.preprocessing_choices.get('categorical_imputation', PREPROCESSING_CONSTANTS["CATEGORICAL_IMPUTATION_DEFAULT"])),
            key='categorical_imputation_selector',
            help="most_frequent=mode, constant='missing'"
        )
        
        st.subheader("üßπ Nettoyage")
        st.session_state.preprocessing_choices['remove_constant_cols'] = st.checkbox(
            "Supprimer colonnes constantes",
            value=st.session_state.preprocessing_choices.get('remove_constant_cols', True),
            key="remove_constant_checkbox",
            help="√âlimine variables sans variance"
        )
        st.session_state.preprocessing_choices['remove_identifier_cols'] = st.checkbox(
            "Supprimer colonnes identifiantes",
            value=st.session_state.preprocessing_choices.get('remove_identifier_cols', True),
            key="remove_id_checkbox",
            help="√âlimine variables avec valeurs uniques (ID)"
        )

        if st.session_state.preprocessing_choices['remove_constant_cols'] or st.session_state.preprocessing_choices['remove_identifier_cols']:
            with st.spinner("Analyse des colonnes..."):
                column_types = auto_detect_column_types(df)
                numeric_cols = df.select_dtypes(include='number').columns
                constant_cols = [col for col in numeric_cols if df[col].std() == 0] if len(numeric_cols) > 0 else []
                identifier_cols = [col for col in df.columns if df[col].nunique() == len(df)]
                if constant_cols or identifier_cols:
                    st.info(f"üßπ Nettoyage: {len(constant_cols)} colonnes constantes, {len(identifier_cols)} colonnes identifiantes d√©tect√©es")
                    log_structured("INFO", "Colonnes √† nettoyer d√©tect√©es", {
                        "n_constant": len(constant_cols),
                        "n_identifier": len(identifier_cols)
                    })
                else:
                    st.info("üßπ Aucune colonne constante ou identifiant d√©tect√©e.")
    
    with col2:
        st.subheader("üìè Normalisation")
        scale_help = {
            'classification': "Recommand√© pour SVM, KNN, r√©seaux de neurones",
            'regression': "Recommand√© pour la plupart des algorithmes", 
            'clustering': "ESSENTIEL pour le clustering (KMeans, DBSCAN)"
        }
        st.session_state.preprocessing_choices['scale_features'] = st.checkbox(
            "Normaliser les features",
            value=st.session_state.preprocessing_choices.get('scale_features', True),
            key="scale_features_checkbox",
            help=scale_help.get(task_type, "Recommand√©")
        )

        if task_type in ['classification', 'regression']:
            st.subheader("üîç R√©duction Dimensionnelle")
            st.session_state.preprocessing_choices['pca_preprocessing'] = st.checkbox(
                "R√©duction dimension (PCA)",
                value=st.session_state.preprocessing_choices.get('pca_preprocessing', False),
                key="pca_preprocessing_checkbox_supervised",
                help="R√©duit le bruit pour donn√©es haute dimension"
            )
            if len(st.session_state.feature_list_for_ml_config) > TRAINING_CONSTANTS["MAX_FEATURES"]:
                st.info("üí° PCA recommand√© pour r√©duire le nombre de features.")
                st.session_state.warnings.append("PCA recommand√© pour nombre √©lev√© de features")

        if task_type == 'clustering' and not st.session_state.preprocessing_choices.get('scale_features', True):
            st.error("‚ùå Normalisation critique pour le clustering!")
            st.info("Action: Activez la normalisation pour de meilleurs r√©sultats.")
            st.session_state.warnings.append("Normalisation non activ√©e pour clustering")
        
        if task_type == 'classification':
            st.subheader("‚öñÔ∏è D√©s√©quilibre")
            if st.session_state.target_column_for_ml_config:
                imbalance_info = detect_imbalance(df, st.session_state.target_column_for_ml_config)
                min_class_count = min(df[st.session_state.target_column_for_ml_config].value_counts())
                if imbalance_info.get("is_imbalanced", False):
                    st.warning(f"üìâ D√©s√©quilibre d√©tect√© (ratio: {imbalance_info.get('imbalance_ratio', 'N/A'):.2f})")
                    st.session_state.preprocessing_choices['use_smote'] = st.checkbox(
                        "Activer SMOTE",
                        value=st.session_state.preprocessing_choices.get('use_smote', True),
                        key="smote_checkbox",
                        help="G√©n√®re des donn√©es synth√©tiques pour √©quilibrer les classes minoritaires"
                    )
                    if st.session_state.preprocessing_choices['use_smote']:
                        with st.expander("‚öôÔ∏è Param√®tres SMOTE", expanded=False):
                            st.session_state.preprocessing_choices['smote_k_neighbors'] = st.number_input(
                                "Nombre de voisins (k)",
                                min_value=1,
                                max_value=min(20, min_class_count-1 if min_class_count > 1 else 1),
                                value=min(st.session_state.preprocessing_choices.get('smote_k_neighbors', 5), min_class_count-1 if min_class_count > 1 else 1),
                                step=1,
                                key="smote_k_neighbors_input",
                                help="Nombre de voisins utilis√©s pour g√©n√©rer les samples synth√©tiques"
                            )
                            st.session_state.preprocessing_choices['smote_sampling_strategy'] = st.selectbox(
                                "Strat√©gie d'√©chantillonnage",
                                options=['auto', 'minority', 'not minority', 'not majority', 'all'],
                                index=['auto', 'minority', 'not minority', 'not majority', 'all'].index(
                                    st.session_state.preprocessing_choices.get('smote_sampling_strategy', 'auto')
                                ),
                                key="smote_sampling_strategy_select",
                                help="D√©termine quelles classes r√©√©quilibrer (auto = classe minoritaire)"
                            )
                            if min_class_count < st.session_state.preprocessing_choices['smote_k_neighbors']:
                                st.warning(f"‚ö†Ô∏è Classe minoritaire trop petite ({min_class_count} samples) pour k={st.session_state.preprocessing_choices['smote_k_neighbors']}.")
                                st.session_state.warnings.append(f"Classe minoritaire trop petite pour SMOTE k={st.session_state.preprocessing_choices['smote_k_neighbors']}")
                else:
                    st.success("‚úÖ Classes √©quilibr√©es")
                    st.session_state.preprocessing_choices['use_smote'] = st.checkbox(
                        "SMOTE (optionnel)",
                        value=st.session_state.preprocessing_choices.get('use_smote', False),
                        key="smote_optional_checkbox",
                        help="G√©n√®re des donn√©es synth√©tiques m√™me si les classes sont √©quilibr√©es"
                    )
                    if st.session_state.preprocessing_choices['use_smote']:
                        with st.expander("‚öôÔ∏è Param√®tres SMOTE", expanded=False):
                            st.session_state.preprocessing_choices['smote_k_neighbors'] = st.number_input(
                                "Nombre de voisins (k)",
                                min_value=1,
                                max_value=min(20, min_class_count-1 if min_class_count > 1 else 1),
                                value=min(st.session_state.preprocessing_choices.get('smote_k_neighbors', 5), min_class_count-1 if min_class_count > 1 else 1),
                                step=1,
                                key="smote_k_neighbors_input_optional",
                                help="Nombre de voisins utilis√©s pour g√©n√©rer les samples synth√©tiques"
                            )
                            st.session_state.preprocessing_choices['smote_sampling_strategy'] = st.selectbox(
                                "Strat√©gie d'√©chantillonnage",
                                options=['auto', 'minority', 'not minority', 'not majority', 'all'],
                                index=['auto', 'minority', 'not minority', 'not majority', 'all'].index(
                                    st.session_state.preprocessing_choices.get('smote_sampling_strategy', 'auto')
                                ),
                                key="smote_sampling_strategy_select_optional",
                                help="D√©termine quelles classes r√©√©quilibrer (auto = classe minoritaire)"
                            )
                            if min_class_count < st.session_state.preprocessing_choices['smote_k_neighbors']:
                                st.warning(f"‚ö†Ô∏è Classe minoritaire trop petite ({min_class_count} samples) pour k={st.session_state.preprocessing_choices['smote_k_neighbors']}.")
                                st.session_state.warnings.append(f"Classe minoritaire trop petite pour SMOTE k={st.session_state.preprocessing_choices['smote_k_neighbors']}")
            else:
                st.info("üîí Variable cible requise pour activer SMOTE")
                st.session_state.preprocessing_choices['use_smote'] = False
                st.session_state.warnings.append("SMOTE d√©sactiv√©: pas de cible")
        elif task_type == 'clustering':
            st.subheader("üîç Clustering")
            st.session_state.preprocessing_choices['pca_preprocessing'] = st.checkbox(
                "R√©duction dimension (PCA)",
                value=st.session_state.preprocessing_choices.get('pca_preprocessing', False),
                key="pca_preprocessing_checkbox",
                help="R√©duit le bruit pour donn√©es haute dimension"
            )
            if st.session_state.preprocessing_choices['pca_preprocessing']:
                for model in st.session_state.selected_models_for_training:
                    if model in ['DBSCAN']:
                        st.warning(f"‚ö†Ô∏è PCA peut √™tre incompatible avec {model}")
                        st.session_state.warnings.append(f"PCA potentiellement incompatible avec {model}")

# √âtape 3: S√©lection des Mod√®les
elif st.session_state.current_step == 3:
    st.header("ü§ñ S√©lection des Mod√®les")
    task_type = st.session_state.get('task_type', 'classification')
    available_models = TrainingHelpers.get_task_specific_models(task_type)
    
    if not available_models:
        st.error(f"‚ùå Aucun mod√®le disponible pour '{task_type}'")
        st.info("Action: V√©rifiez le catalogue de mod√®les.")
        st.session_state.warnings.append(f"Aucun mod√®le pour {task_type}")
        st.stop()
    
    col1, col2 = st.columns([2, 1])
    with col1:
        st.subheader("üéØ Mod√®les")
        if not st.session_state.selected_models_for_training:
            st.session_state.selected_models_for_training = TrainingHelpers.get_default_models_for_task(task_type)
        
        selected_models = st.multiselect(
            f"Mod√®les {task_type}",
            options=available_models,
            default=st.session_state.selected_models_for_training,
            key="models_multiselect",
            help="Mod√®les √† entra√Æner et comparer"
        )
        st.session_state.selected_models_for_training = selected_models
        
        if selected_models:
            if len(selected_models) > TRAINING_CONSTANTS["MAX_MODELS"]:
                st.warning(f"‚ö†Ô∏è Maximum {TRAINING_CONSTANTS['MAX_MODELS']} mod√®les recommand√©s")
                st.session_state.warnings.append(f"Trop de mod√®les s√©lectionn√©s ({len(selected_models)})")
                st.session_state.selected_models_for_training = selected_models[:TRAINING_CONSTANTS["MAX_MODELS"]]
            st.success(f"‚úÖ {len(st.session_state.selected_models_for_training)} mod√®les s√©lectionn√©s")
            with st.expander("üìã D√©tails des mod√®les", expanded=False):
                for model_name in selected_models:
                    model_config = MODEL_CATALOG[task_type].get(model_name, {})
                    st.write(f"**{model_name}**")
                    st.caption(f"‚Ä¢ {model_config.get('description', 'Description non disponible')}")
        else:
            st.warning("‚ö†Ô∏è Aucun mod√®le s√©lectionn√©")
            st.info("Action: S√©lectionnez au moins un mod√®le.")
            st.session_state.warnings.append("Aucun mod√®le s√©lectionn√©")
            
    with col2:
        st.subheader("‚öôÔ∏è Configuration")
        if task_type != 'clustering':
            test_split = st.slider(
                "Jeu de test (%)",
                min_value=10,
                max_value=40,
                value=st.session_state.get('test_split_for_ml_config', 20),
                step=5,
                key="test_split_slider",
                help="Donn√©es r√©serv√©es pour l'√©valuation"
            )
            st.session_state.test_split_for_ml_config = test_split
            st.caption(f"üìä {test_split}% test, {100-test_split}% entra√Ænement")
        else:
            st.info("üîç Clustering: 100% des donn√©es utilis√©es")
            st.session_state.test_split_for_ml_config = 0
        
        optimize_hp = st.checkbox(
            "Optimisation hyperparam√®tres",
            value=st.session_state.get('optimize_hp_for_ml_config', False),
            key="optimize_hp_checkbox",
            help="Recherche des meilleurs param√®tres (plus long)"
        )
        st.session_state.optimize_hp_for_ml_config = optimize_hp
        
        if optimize_hp:
            st.warning("‚è∞ Temps d'entra√Ænement +3-5x")
            st.session_state.preprocessing_choices['optimization_method'] = st.selectbox(
                "M√©thode",
                options=['Silhouette Score', 'Davies-Bouldin'] if task_type == 'clustering' else ['GridSearch', 'RandomSearch'],
                index=0,
                key="optimization_method_selector",
                help="Silhouette=qualit√© clusters, Davies-Bouldin=compacit√©, GridSearch=exhaustif, RandomSearch=rapide"
            )
        
        n_features = len(st.session_state.feature_list_for_ml_config)
        estimated_seconds = TrainingHelpers.estimate_training_time(df, len(selected_models), task_type, optimize_hp, n_features, st.session_state.preprocessing_choices.get('use_smote', False))
        st.info(f"‚è±Ô∏è Temps estim√©: {max(1, estimated_seconds // 60)} minute(s)")
        
        if selected_models:
            resource_check = check_system_resources(df, len(selected_models))
            if not resource_check["has_enough_resources"]:
                st.error("‚ùå Ressources insuffisantes")
                for issue in resource_check["issues"]:
                    st.error(f"‚Ä¢ {issue}")
                st.session_state.warnings.extend(resource_check["issues"])
            elif resource_check["warnings"]:
                st.warning("‚ö†Ô∏è Ressources limites")
                for warning in resource_check["warnings"]:
                    st.warning(f"‚Ä¢ {warning}")
                st.session_state.warnings.extend(resource_check["warnings"])

# √âtape 4: Lancement
elif st.session_state.current_step == 4:
    st.header("üöÄ Lancement de l'Exp√©rimentation")
    task_type = st.session_state.get('task_type', 'classification')
    
    config_issues = []
    if task_type in ['classification', 'regression'] and not st.session_state.target_column_for_ml_config:
        config_issues.append("Variable cible non d√©finie")
    if not st.session_state.feature_list_for_ml_config:
        config_issues.append("Aucune variable explicative s√©lectionn√©e")
    elif len(st.session_state.feature_list_for_ml_config) < VALIDATION_CONSTANTS["MIN_COLS_REQUIRED"] and task_type == 'clustering':
        config_issues.append(f"Minimum {VALIDATION_CONSTANTS['MIN_COLS_REQUIRED']} variables pour clustering")
    if not st.session_state.selected_models_for_training:
        config_issues.append("Aucun mod√®le s√©lectionn√©")
    
    if task_type == 'clustering' and not st.session_state.preprocessing_choices.get('scale_features', True):
        config_issues.append("Normalisation requise pour clustering")
    if len(st.session_state.feature_list_for_ml_config) > TRAINING_CONSTANTS["MAX_FEATURES"]:
        config_issues.append("Trop de features - risque de surapprentissage")
    
    resource_check = check_system_resources(df, len(st.session_state.selected_models_for_training))
    config_issues.extend(resource_check["issues"])
    
    with st.expander("üìã R√©capitulatif", expanded=True):
        if config_issues:
            st.error("‚ùå Configuration incompl√®te:")
            for issue in config_issues:
                st.error(f"‚Ä¢ {issue}")
                st.info("Action: Revenez aux √©tapes pr√©c√©dentes pour corriger.")
            st.session_state.warnings.extend(config_issues)
        else:
            st.success("‚úÖ Configuration valide")
        
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("**üìä Donn√©es**")
            st.write(f"‚Ä¢ Type: {task_type.upper()}")
            if task_type != 'clustering':
                st.write(f"‚Ä¢ Cible: `{st.session_state.target_column_for_ml_config or 'Non d√©fini'}`")
            st.write(f"‚Ä¢ Features: {len(st.session_state.feature_list_for_ml_config)}")
            if task_type != 'clustering':
                st.write(f"‚Ä¢ Test: {st.session_state.test_split_for_ml_config}%")
            else:
                st.write("‚Ä¢ Test: 0% (clustering)")
        with col2:
            st.markdown("**ü§ñ Mod√®les**")
            st.write(f"‚Ä¢ Mod√®les: {len(st.session_state.selected_models_for_training)}")
            st.write(f"‚Ä¢ Optimisation: {'‚úÖ' if st.session_state.optimize_hp_for_ml_config else '‚ùå'}")
            if task_type == 'classification':
                st.write(f"‚Ä¢ SMOTE: {'‚úÖ' if st.session_state.preprocessing_choices.get('use_smote') else '‚ùå'}")
            st.write(f"‚Ä¢ Normalisation: {'‚úÖ' if st.session_state.preprocessing_choices.get('scale_features') else '‚ùå'}")
    
    col_launch, col_reset = st.columns([2, 1])
    with col_launch:
        launch_disabled = len(config_issues) > 0 or st.session_state.get('ml_training_in_progress', False)
        if st.button("üöÄ Lancer", type="primary", use_container_width=True, disabled=launch_disabled):
            st.session_state.ml_training_in_progress = True
            st.session_state.ml_last_training_time = time.time()
            
            training_config = {
                'df': df,
                'target_column': st.session_state.target_column_for_ml_config,
                'model_names': st.session_state.selected_models_for_training,
                'task_type': task_type,
                'test_size': st.session_state.test_split_for_ml_config / 100 if task_type != 'clustering' else 0.0,
                'optimize': st.session_state.optimize_hp_for_ml_config,
                'feature_list': st.session_state.feature_list_for_ml_config,
                'use_smote': st.session_state.preprocessing_choices.get('use_smote', False),
                'preprocessing_choices': st.session_state.preprocessing_choices,
            }
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            results_container = st.empty()
            
            try:
                status_text.text("üìä Pr√©paration des donn√©es...")
                progress_bar.progress(10)
                
                n_models = len(st.session_state.selected_models_for_training)
                results = []
                
                def train_single_model(model_name, config):
                    """Entra√Æne un seul mod√®le sans duplication"""
                    try:
                        model_config = config.copy()
                        model_config['model_names'] = [model_name]
                        
                        model_results = safe_train_models(**model_config)
                        
                        if model_results:
                            for result in model_results:
                                if 'task_type' not in result or not result['task_type']:
                                    result['task_type'] = config.get('task_type', 'unknown')
                                    log_structured("WARNING", f"task_type manquant, ajout√©: {result['task_type']}")
                                if 'feature_names' not in result:
                                    result['feature_names'] = config.get('feature_list', [])
                        
                        return model_results
                        
                    except Exception as e:
                        log_structured("ERROR", f"Erreur entra√Ænement {model_name}: {str(e)[:100]}")
                        return [{
                            "model_name": model_name,
                            "task_type": config.get('task_type', 'unknown'),
                            "metrics": {"error": str(e)[:100]},
                            "success": False,
                            "training_time": 0,
                            "X_sample": None,
                            "y_test": None,
                            "labels": None,
                            "feature_names": config.get('feature_list', []),
                            "warnings": []
                        }]

                if TRAINING_CONSTANTS["N_JOBS"] == -1:
                    with concurrent.futures.ThreadPoolExecutor(max_workers=len(st.session_state.selected_models_for_training)) as executor:
                        future_to_model = {
                            executor.submit(train_single_model, model_name, training_config): model_name 
                            for model_name in st.session_state.selected_models_for_training
                        }
                        for i, future in enumerate(concurrent.futures.as_completed(future_to_model)):
                            model_name = future_to_model[future]
                            status_text.text(f"üîß Entra√Ænement {i+1}/{n_models}: {model_name}")
                            progress_bar.progress(10 + int((i / n_models) * 80))
                            try:
                                model_result = future.result()
                                if model_result:
                                    results.extend(model_result)
                            except Exception as e:
                                log_structured("ERROR", f"√âchec entra√Ænement parall√®le {model_name}: {str(e)[:100]}")
                                results.append({
                                    "model_name": model_name,
                                    "metrics": {"error": str(e)[:100]},
                                    "success": False,
                                    "training_time": 0,
                                    "X_sample": None,
                                    "y_test": None,
                                    "labels": None,
                                    "feature_names": training_config['feature_list'],
                                    "warnings": []
                                })
                else:
                    for i, model_name in enumerate(st.session_state.selected_models_for_training):
                        status_text.text(f"üîß Entra√Ænement {i+1}/{n_models}: {model_name}")
                        progress_bar.progress(10 + int((i / n_models) * 80))
                        model_result = train_single_model(model_name, training_config)
                        if model_result:
                            results.extend(model_result)
                
                status_text.text("‚úÖ Finalisation...")
                progress_bar.progress(95)
                
                elapsed_time = time.time() - st.session_state.ml_last_training_time
                status_text.text(f"‚úÖ Termin√© en {elapsed_time:.1f}s")
                progress_bar.progress(100)
                
                st.session_state.ml_results = results
                st.session_state.ml_training_in_progress = False
                st.session_state.ml_error_count = 0
                
                successful_models = [r for r in results if r.get('success', False) and not r.get('metrics', {}).get('error')]
                results_analysis = TrainingHelpers.process_training_results(results, task_type)

                with results_container.container():
                    if successful_models:
                        st.success(f"‚úÖ {len(successful_models)}/{len(results)} mod√®les entra√Æn√©s avec succ√®s")
                        
                        if results_analysis["best_model"]:
                            best_model = results_analysis["best_model"]
                            best_score = best_model['metrics'].get(
                                'silhouette_score' if task_type == 'clustering' else 'r2' if task_type == 'regression' else 'accuracy', 
                                0
                            )
                            st.info(f"üèÜ **Meilleur mod√®le**: {best_model['model_name']} (Score: {best_score:.3f})")
                        
                        with st.expander("üìä R√©sultats d√©taill√©s", expanded=False):
                            for model in successful_models:
                                col1, col2, col3 = st.columns([2, 2, 1])
                                with col1:
                                    st.write(f"**{model['model_name']}**")
                                with col2:
                                    primary_metric = (
                                        'silhouette_score' if task_type == 'clustering' 
                                        else 'r2' if task_type == 'regression' 
                                        else 'accuracy'
                                    )
                                    score = model['metrics'].get(primary_metric, 'N/A')
                                    if isinstance(score, (int, float)):
                                        st.write(f"`{primary_metric}: {score:.3f}`")
                                with col3:
                                    st.write(f"`{model['training_time']:.1f}s`")
                        
                        if results_analysis["recommendations"]:
                            with st.expander("üí° Recommandations", expanded=False):
                                for rec in results_analysis["recommendations"]:
                                    st.info(rec)
                    else:
                        st.error("‚ùå Aucun mod√®le n'a pu √™tre entra√Æn√©")
                        if results_analysis["failed_models"]:
                            with st.expander("üîç D√©tails des erreurs", expanded=True):
                                for model in results_analysis["failed_models"]:
                                    st.error(f"**{model['model_name']}**: {model.get('metrics', {}).get('error', 'Erreur inconnue')}")

                if successful_models:
                    if st.button("üìà Voir l'analyse d√©taill√©e des r√©sultats", type="primary", use_container_width=True):
                        st.session_state.ml_results = results
                        st.session_state.results_analysis = results_analysis
                        st.switch_page("pages/3_evaluation.py")
                
                gc.collect()
                
            except Exception as e:
                st.session_state.ml_training_in_progress = False
                st.session_state.ml_error_count = st.session_state.get('ml_error_count', 0) + 1
                status_text.text("‚ùå √âchec")
                progress_bar.progress(0)
                st.error(f"‚ùå Erreur: {str(e)[:100]}")
                st.info("Action: V√©rifiez la configuration ou contactez le support.")
                log_structured("ERROR", f"Training √©chou√©: {str(e)[:100]}", {"error": str(e)[:100]})
                st.session_state.warnings.append(f"√âchec entra√Ænement: {str(e)[:100]}")
    
    with col_reset:
        if st.button("üîÑ Reset", use_container_width=True):
            ml_keys_to_reset = [
                'target_column_for_ml_config', 'feature_list_for_ml_config',
                'selected_models_for_training', 'ml_results', 'task_type', 
                'previous_task_type', 'test_split_for_ml_config', 'optimize_hp_for_ml_config',
                'preprocessing_choices', 'ml_training_in_progress', 'ml_last_training_time', 
                'ml_error_count', 'mlflow_runs', 'model_performance_history', 'warnings'
            ]
            for key in ml_keys_to_reset:
                if key in st.session_state:
                    del st.session_state[key]
            # R√©initialiser les cl√©s par d√©faut
            for key, value in session_state_defaults.items():
                st.session_state[key] = value
            log_structured("INFO", "Configuration r√©initialis√©e")
            st.success("Configuration r√©initialis√©e")
            st.rerun()

# Affichage global des avertissements
if st.session_state.warnings:
    with st.expander("‚ö†Ô∏è Avertissements cumul√©s", expanded=False):
        for warning in st.session_state.warnings:
            st.warning(f"‚Ä¢ {warning}")

# Footer
st.markdown("---")
col1, col2, col3 = st.columns(3)
with col1:
    progress = (st.session_state.current_step / len(steps)) * 100
    st.caption(f"üìä √âtape {st.session_state.current_step}/{len(steps)} ({progress:.0f}%)")
with col2:
    st.caption(f"üéØ {st.session_state.get('task_type', 'Non d√©fini').upper()}")
with col3:
    st.caption(f"‚è∞ {time.strftime('%H:%M:%S')}")

# Navigation
col_prev, col_next = st.columns(2)
with col_prev:
    if st.session_state.current_step > 1:
        if st.button("‚óÄÔ∏è Pr√©c√©dent", use_container_width=True):
            st.session_state.current_step -= 1
            st.rerun()
with col_next:
    if st.session_state.current_step < 4:
        if st.button("Suivant ‚ñ∂Ô∏è", use_container_width=True, type="primary"):
            if st.session_state.current_step == 1:
                if st.session_state.task_type in ['classification', 'regression'] and not st.session_state.target_column_for_ml_config:
                    st.error("Veuillez s√©lectionner une variable cible")
                    st.session_state.warnings.append("Variable cible non s√©lectionn√©e")
                elif not st.session_state.feature_list_for_ml_config:
                    st.error("Veuillez s√©lectionner au moins une variable")
                    st.session_state.warnings.append("Aucune variable explicative s√©lectionn√©e")
                else:
                    st.session_state.current_step += 1
                    st.rerun()
            else:
                st.session_state.current_step += 1
                st.rerun()

# Debug
if os.getenv("DEBUG_MODE", "false").lower() == "true":
    with st.expander("üîç Debug", expanded=False):
        st.json({
            "current_step": st.session_state.current_step,
            "task_type": st.session_state.get('task_type'),
            "previous_task_type": st.session_state.get('previous_task_type'),
            "target_column": st.session_state.get('target_column_for_ml_config'),
            "num_features": len(st.session_state.get('feature_list_for_ml_config', [])),
            "num_models": len(st.session_state.get('selected_models_for_training', [])),
            "test_split": st.session_state.get('test_split_for_ml_config'),
            "training_in_progress": st.session_state.get('ml_training_in_progress', False),
            "error_count": st.session_state.get('ml_error_count', 0),
            "warnings": st.session_state.get('warnings', [])
        })